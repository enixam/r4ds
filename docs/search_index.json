[
["index.html", "R for data science: tidyverse and beyond 前言", " R for data science: tidyverse and beyond Maxine 2020-05-11 前言 关于 R for Data Science (Wickham and Grolemund 2016) 的个人笔记，随缘更新。任何建议：https://github.com/enixam/rfordatascience/issues 或 565702994@qq.com tidyverse 已经在每个章节之前加载好： library(tidyverse) "],
["dplyr-data-transformation.html", "1 dplyr: Data transformation", " 1 dplyr: Data transformation dplyr 承担了 tidyverse 中最基本也最重要的数据处理、转换、分析功能(to my mind)。它的设计思想是发展处一套简洁、统一的数据操作语法(a grammar of data manipulation)，用英语中常见的动词命名操作数据的函数，并充分利用管道符 %&gt;% 和更“现代”的数据框格式 tibble 增加代码可读性。 为了介绍 dplyr 中的基本数据操作。可以使用 nycflights::flights，这个数据集包含了 2013 年从纽约市处罚的所有 336776 次航班的信息。该数据来自于美国交通统计局。 library(nycflights13) flights #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 819 #&gt; 2 2013 1 1 533 529 4 850 830 #&gt; 3 2013 1 1 542 540 2 923 850 #&gt; 4 2013 1 1 544 545 -1 1004 1022 #&gt; 5 2013 1 1 554 600 -6 812 837 #&gt; 6 2013 1 1 554 558 -4 740 728 #&gt; # ... with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 专门为 tibble 数据格式编写的函数 glimpse()，它的功能和 str() 类似，但输出更为整洁，显示的数据也更多一些: glimpse(flights) #&gt; Rows: 336,776 #&gt; Columns: 19 #&gt; $ year &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013... #&gt; $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... #&gt; $ day &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... #&gt; $ dep_time &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 55... #&gt; $ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 60... #&gt; $ dep_delay &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2,... #&gt; $ arr_time &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 8... #&gt; $ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 8... #&gt; $ arr_delay &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7,... #&gt; $ carrier &lt;chr&gt; &quot;UA&quot;, &quot;UA&quot;, &quot;AA&quot;, &quot;B6&quot;, &quot;DL&quot;, &quot;UA&quot;, &quot;B6&quot;, &quot;EV&quot;, &quot;B6&quot;... #&gt; $ flight &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301... #&gt; $ tailnum &lt;chr&gt; &quot;N14228&quot;, &quot;N24211&quot;, &quot;N619AA&quot;, &quot;N804JB&quot;, &quot;N668DN&quot;, &quot;N... #&gt; $ origin &lt;chr&gt; &quot;EWR&quot;, &quot;LGA&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;LGA&quot;, &quot;EWR&quot;, &quot;EWR&quot;, &quot;LG... #&gt; $ dest &lt;chr&gt; &quot;IAH&quot;, &quot;IAH&quot;, &quot;MIA&quot;, &quot;BQN&quot;, &quot;ATL&quot;, &quot;ORD&quot;, &quot;FLL&quot;, &quot;IA... #&gt; $ air_time &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149... #&gt; $ distance &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 73... #&gt; $ hour &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6... #&gt; $ minute &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59... #&gt; $ time_hour &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-0... str(flights) #&gt; tibble [336,776 x 19] (S3: tbl_df/tbl/data.frame) #&gt; $ year : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... #&gt; $ month : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ day : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ... #&gt; $ dep_time : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ... #&gt; $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ... #&gt; $ dep_delay : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ... #&gt; $ arr_time : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ... #&gt; $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ... #&gt; $ arr_delay : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ... #&gt; $ carrier : chr [1:336776] &quot;UA&quot; &quot;UA&quot; &quot;AA&quot; &quot;B6&quot; ... #&gt; $ flight : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ... #&gt; $ tailnum : chr [1:336776] &quot;N14228&quot; &quot;N24211&quot; &quot;N619AA&quot; &quot;N804JB&quot; ... #&gt; $ origin : chr [1:336776] &quot;EWR&quot; &quot;LGA&quot; &quot;JFK&quot; &quot;JFK&quot; ... #&gt; $ dest : chr [1:336776] &quot;IAH&quot; &quot;IAH&quot; &quot;MIA&quot; &quot;BQN&quot; ... #&gt; $ air_time : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ... #&gt; $ distance : num [1:336776] 1400 1416 1089 1576 762 ... #&gt; $ hour : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ... #&gt; $ minute : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ... #&gt; $ time_hour : POSIXct[1:336776], format: &quot;2013-01-01 05:00:00&quot; &quot;2013-01-01 05:00:00&quot; ... 五个 dplyr 核心函数能解决数据转换中的绝大多数问题： 使用 filter() 筛选行 使用 arrange() 排列行 使用 select 选取列 用现有的变量创建新变量 mutate() 聚合并计算摘要统计量 summarize() 上面的所有函数都可以和 group_by() 函数联合起来使用，group_by() 函数可以改变以上每个函数的作用范围，让其从在整个数据集上操作变为在每个变量的水平上分别操作。这 6 个函数构成了数据处理的基本工具。 这些函数有完全相同的参数结构和工作方式： 第一个参数是数据集，表明我们想对什么数据进行处理 随后的参数是变量名称（不带引号）描述了在数据上进行什么处理，不同的变量之间用逗号分隔 它们不会改变原数据，而是生成一个新的数据框 "],
["filter.html", "1.1 filter()", " 1.1 filter() filter() 可以基于观测值筛选行，符合条件的行留下，不符合条件的被剔除，最终得到一个观测子集。第一个参数是数据集的名称，第二个参数以及随后的参数是用来筛选行的条件。例如，我们可以使用以下代码筛选出一月一日的所有航班（条件：月 = 1 且 日 = 1） flights %&gt;% filter(month == 1, day == 1) #&gt; # A tibble: 842 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 819 #&gt; 2 2013 1 1 533 529 4 850 830 #&gt; 3 2013 1 1 542 540 2 923 850 #&gt; 4 2013 1 1 544 545 -1 1004 1022 #&gt; 5 2013 1 1 554 600 -6 812 837 #&gt; 6 2013 1 1 554 558 -4 740 728 #&gt; # ... with 836 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 思考筛选的方式： 以行为单位，如果该行满足所指定的条件，则被筛选出 ； 若不满足，则被剔除。使用filter()时，总应该从每一行的角度来思考问题。 1.1.1 Operators 为了有效地进行筛选，R 提供了一套标准的运算符，包括比较运算符和逻辑运算符。 比较运算符： ==、！= 、 &gt; 、 &gt;= 、 &lt; 、 &lt;= 当开始编写条件时，最容易犯的错误就是用=而不是==来测试是否相等。R 对于这种错误会提供一条启发性的错误信息： flights %&gt;% filter(month = 1) #&gt; Error: `month` (`month = 1`) must not be named, do you need `==`? 在判断是否相等时，还有另一个常见问题：浮点数。例如，下面的结果可能出人意料： sqrt(2)^2 == 2 #&gt; [1] FALSE 1 /49 * 49 == 1 #&gt; [1] FALSE 计算机使用的是有限位运算，不能存储无限位的数。因此我们看到的每个数都是一个近似值。比较浮点数是否相等时，不能用==,而应该用near(),它用于比较两个数值向量是否相等，且带有一定容忍度(tolerence): near(sqrt(2) ^ 2, 2) #&gt; [1] TRUE near(1 /49 * 49, 1) #&gt; [1] TRUE **逻辑运算符* filter()中的多个参数是“与”的关系，如 data %&gt;% filter(condition_1,condition_2,···,condition_n) 表示的是“我希望同时筛选出满足这n个条件的行。如果要实现其他类型的组合，需要使用逻辑（布尔）运算符。&amp;表示与，|表示或，!表示非。下图给出了布尔运算的完整集合： 例如，想要找出11月 或 12月出发的所有航班： flights %&gt;% filter(month == 11 | month == 12) #&gt; # A tibble: 55,403 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 11 1 5 2359 6 352 345 #&gt; 2 2013 11 1 35 2250 105 123 2356 #&gt; 3 2013 11 1 455 500 -5 641 651 #&gt; 4 2013 11 1 539 545 -6 856 827 #&gt; 5 2013 11 1 542 545 -3 831 855 #&gt; 6 2013 11 1 549 600 -11 912 923 #&gt; # ... with 55,397 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 这种问题有一个有用的简写形式：x %in% y，这个表达式在x被包含于y的时候返回TRUE，我们可以这样改写上面的代码： flights %&gt;% filter(month %in% c(11, 12)) ## 找出所有月份值包含在该向量里的行 #&gt; # A tibble: 55,403 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 11 1 5 2359 6 352 345 #&gt; 2 2013 11 1 35 2250 105 123 2356 #&gt; 3 2013 11 1 455 500 -5 641 651 #&gt; 4 2013 11 1 539 545 -6 856 827 #&gt; 5 2013 11 1 542 545 -3 831 855 #&gt; 6 2013 11 1 549 600 -11 912 923 #&gt; # ... with 55,397 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 有时可以使用德摩根律来将筛选条件简化：!(x &amp; y)等价于!x | !y，而!(x | y)等价于!x &amp; !y。例如，如果想要找出延误时间（到达和出发)都不多于两个小时的航班，以下两种方式均可： flights %&gt;% filter(arr_delay &lt;= 120 &amp; dep_dealy &lt;= 120 ) flights %&gt;% filter(!(arr_delay &gt; 120| dep_delay &gt; 120 )) dplyr中另外一个对筛选有帮助的函数是between(x, left, right)，它用于判断x是否落在left和right两个值确定的闭区间里。 例如找出所有在11月和12月出发的航班也可以这样表达： flights %&gt;% filter(between(month, 11, 12)) #&gt; # A tibble: 55,403 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 11 1 5 2359 6 352 345 #&gt; 2 2013 11 1 35 2250 105 123 2356 #&gt; 3 2013 11 1 455 500 -5 641 651 #&gt; 4 2013 11 1 539 545 -6 856 827 #&gt; 5 2013 11 1 542 545 -3 831 855 #&gt; 6 2013 11 1 549 600 -11 912 923 #&gt; # ... with 55,397 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 有些时候，filter() 函数中用来筛选的条件可能比较复杂，需要书写令人费解的逻辑表达式，这时候可以考虑创建一个新变量代表逻辑判断的结果。这样检查代码会容易很多。我们很快就会介绍如何创建新变量。 &amp; and &amp;&amp; indicate logical AND and | and || indicate logical OR. The shorter form performs elementwise comparisons in much the same way as arithmetic operators. The longer form evaluates left to right examining only the first element of each vector. Evaluation proceeds only until the result is determined. The longer form is appropriate for programming control-flow and typically preferred in if clauses. 1.1.2 Missing values NA (not available)表示未知的值、缺失值，缺失值一个很重要的特点是它是“可传染的”。如果运算中包含了缺失值，那么运算结果一般来说也会是缺失值。 NA &gt; 5 #&gt; [1] NA NA == 10 #&gt; [1] NA NA + 2 #&gt; [1] NA NA / 2 #&gt; [1] NA 以上的表达式的结果都是NA，这很好理解，如果R不知道表达式其中的一个量究竟是什么值，自然表达式的结果也就不可知。 还要注意一件事： NA == NA #&gt; [1] NA 这样理解： 令 x 为Mary的年龄，我们不知道她有多大：x &lt;- NA 令 y 为John的年龄，我们同样不知道他又多大：x &lt;- NA Mary和John的年龄相同吗？： x == y 不知道！ 鉴于此，使用NA == x来判断x是否是缺失值不可行。我们用函数is.na()进行判断： x = NA is.na(x) #&gt; [1] TRUE 前面说过，filter() 实际上是在提问：某行的某个 \\ 某些变量满足给定的条件吗？如果为 TRUE，则筛选出该行。如果该行在涉及变量上的取值是 NA，那么逻辑表达式也会返回 NA，这些行将被返回结果为 FALSE 的行一并被排除。如果想保留缺失值，同样可以利用逻辑表达式指出： df &lt;- tibble(x = c(1, NA, 3)) df %&gt;% filter(is.na(x) | x &gt; 1) #&gt; # A tibble: 2 x 1 #&gt; x #&gt; &lt;dbl&gt; #&gt; 1 NA #&gt; 2 3 For more topics on missing values, see section 6.6 and Chapter 17. 1.1.3 Exercises Exercise 1.1 找出满足以下条件的所有航班: 到达时间延误两小时或更多的航班 flights %&gt;% filter(arr_delay &gt;= 120) #&gt; # A tibble: 10,200 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 811 630 101 1047 830 #&gt; 2 2013 1 1 848 1835 853 1001 1950 #&gt; 3 2013 1 1 957 733 144 1056 853 #&gt; 4 2013 1 1 1114 900 134 1447 1222 #&gt; 5 2013 1 1 1505 1310 115 1638 1431 #&gt; 6 2013 1 1 1525 1340 105 1831 1626 #&gt; # ... with 10,194 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 飞往休斯顿（IAH机场或者HOU机场）的航班 flights %&gt;% filter(dest == &quot;IAH&quot; | dest == &quot;HOU&quot;) #&gt; # A tibble: 9,313 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 819 #&gt; 2 2013 1 1 533 529 4 850 830 #&gt; 3 2013 1 1 623 627 -4 933 932 #&gt; 4 2013 1 1 728 732 -4 1041 1038 #&gt; 5 2013 1 1 739 739 0 1104 1038 #&gt; 6 2013 1 1 908 908 0 1228 1219 #&gt; # ... with 9,307 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 也可以写成filter(flights, dest %in% c(\"HOU\",\"IAH\")) 由联合航空（United）、美利坚航空（American）或者三角洲航空（Delat）运营的航班 carrier列代表了航空公司，但是用两个字母缩写表示： flights[&quot;carrier&quot;] #&gt; # A tibble: 336,776 x 1 #&gt; carrier #&gt; &lt;chr&gt; #&gt; 1 UA #&gt; 2 UA #&gt; 3 AA #&gt; 4 B6 #&gt; 5 DL #&gt; 6 UA #&gt; # ... with 336,770 more rows 我们可以在airlines数据集中找到这些缩写的含义： airlines #&gt; # A tibble: 16 x 2 #&gt; carrier name #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 9E Endeavor Air Inc. #&gt; 2 AA American Airlines Inc. #&gt; 3 AS Alaska Airlines Inc. #&gt; 4 B6 JetBlue Airways #&gt; 5 DL Delta Air Lines Inc. #&gt; 6 EV ExpressJet Airlines Inc. #&gt; # ... with 10 more rows 三角洲航空对应 “DL”，“UA” 代表联合航空，“AA”代表美利坚航空 flights %&gt;% filter(carrier %in% c(&quot;DL&quot;, &quot;UA&quot;, &quot;AA&quot;)) #&gt; # A tibble: 139,504 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 819 #&gt; 2 2013 1 1 533 529 4 850 830 #&gt; 3 2013 1 1 542 540 2 923 850 #&gt; 4 2013 1 1 554 600 -6 812 837 #&gt; 5 2013 1 1 554 558 -4 740 728 #&gt; 6 2013 1 1 558 600 -2 753 745 #&gt; # ... with 139,498 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 夏季(7月、8月、9月)出发的航班 flights %&gt;% filter(month %in% c(7, 8, 9)) #&gt; # A tibble: 86,326 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 7 1 1 2029 212 236 2359 #&gt; 2 2013 7 1 2 2359 3 344 344 #&gt; 3 2013 7 1 29 2245 104 151 1 #&gt; 4 2013 7 1 43 2130 193 322 14 #&gt; 5 2013 7 1 44 2150 174 300 100 #&gt; 6 2013 7 1 46 2051 235 304 2358 #&gt; # ... with 86,320 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 另一种写法：filter(flgihts,month &gt;= 7, month &lt;= 9) 用between()函数的写法： flights %&gt;% filter(between(month,7,9)) #&gt; # A tibble: 86,326 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 7 1 1 2029 212 236 2359 #&gt; 2 2013 7 1 2 2359 3 344 344 #&gt; 3 2013 7 1 29 2245 104 151 1 #&gt; 4 2013 7 1 43 2130 193 322 14 #&gt; 5 2013 7 1 44 2150 174 300 100 #&gt; 6 2013 7 1 46 2051 235 304 2358 #&gt; # ... with 86,320 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 到达时间延误超过两小时，但出发时间没有延误的航班 flights %&gt;% filter(dep_delay &gt; 120 , arr_delay &lt;= 120) #&gt; # A tibble: 1,262 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 1540 1338 122 2020 1825 #&gt; 2 2013 1 2 2334 2129 125 33 2242 #&gt; 3 2013 1 3 1321 1115 126 1450 1257 #&gt; 4 2013 1 3 1758 1550 128 2240 2050 #&gt; 5 2013 1 3 1933 1730 123 2131 1953 #&gt; 6 2013 1 4 1602 1359 123 1715 1517 #&gt; # ... with 1,256 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 延误至少一小时，但飞行过程弥补回30分钟的航班 flights %&gt;% filter(dep_delay - arr_delay &gt; 30) #&gt; # A tibble: 17,950 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 701 700 1 1123 1154 #&gt; 2 2013 1 1 820 820 0 1249 1329 #&gt; 3 2013 1 1 840 845 -5 1311 1350 #&gt; 4 2013 1 1 857 851 6 1157 1222 #&gt; 5 2013 1 1 909 810 59 1331 1315 #&gt; 6 2013 1 1 1025 951 34 1258 1302 #&gt; # ... with 17,944 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 出发时间在午夜和早上6点之间（包括0点和6点）的航班 在变量dep_time中，0 点用数值 2400 代表： summary(flights$dep_time) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s #&gt; 1 907 1401 1349 1744 2400 8255 出于这点，不能简单写成dep_time &lt;= 600，而是如下： flights %&gt;% filter(dep_time &lt;= 600 | dep_time == 2400) #&gt; # A tibble: 9,373 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 819 #&gt; 2 2013 1 1 533 529 4 850 830 #&gt; 3 2013 1 1 542 540 2 923 850 #&gt; 4 2013 1 1 544 545 -1 1004 1022 #&gt; 5 2013 1 1 554 600 -6 812 837 #&gt; 6 2013 1 1 554 558 -4 740 728 #&gt; # ... with 9,367 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Exercise 1.2 dep_time`有缺失值的航班有多少？其他变量的缺失值情况如何？ sum(is.na(flights$dep_time)) #&gt; [1] 8255 flights %&gt;% filter(is.na(dep_time)) #&gt; # A tibble: 8,255 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 NA 1630 NA NA 1815 #&gt; 2 2013 1 1 NA 1935 NA NA 2240 #&gt; 3 2013 1 1 NA 1500 NA NA 1825 #&gt; 4 2013 1 1 NA 600 NA NA 901 #&gt; 5 2013 1 2 NA 1540 NA NA 1747 #&gt; 6 2013 1 2 NA 1620 NA NA 1746 #&gt; # ... with 8,249 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 注意到，所有dep_time为 NA 的航班在有关实际到达、出发情况的变量上取值皆为 NA，这些很可能是被取消的航班。 # 其他变量中的缺失值 flights %&gt;% summarize_all( ~ sum(is.na(.))) #&gt; # A tibble: 1 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 0 0 0 8255 0 8255 8713 0 #&gt; # ... with 11 more variables: arr_delay &lt;int&gt;, carrier &lt;int&gt;, flight &lt;int&gt;, #&gt; # tailnum &lt;int&gt;, origin &lt;int&gt;, dest &lt;int&gt;, air_time &lt;int&gt;, distance &lt;int&gt;, #&gt; # hour &lt;int&gt;, minute &lt;int&gt;, time_hour &lt;int&gt; Exercise 1.3 为什么NA ^ 0的值不是NA，而NA * 0的值是NA ？为什么NA | TRUE 的值不是NA？为什么FALSE &amp; NA的值不是NA，能找出一般规律吗？ 只要表达式的值被NA背后的未知量所决定，就返回NA 对于所有 x 的取值，都有\\(x ^ 0 = 1\\)，NA ^ 0不取决于 NA 到底可能是什么值。 但对于\\(x * 0\\)，如果x趋近于正负无穷（ R 用-inf 和 inf代表），则R会返回 NaN 错误（not a number），只要知道 NA 究竟是什么，才能知道该表达式的结果 NA ^ 0 #&gt; [1] 1 NA * 0 #&gt; [1] NA c(Inf,-Inf) * 0 #&gt; [1] NaN NaN 同样，对于 NA | TRUE，不论 NA 是什么，该表达式总为真 ； 对于NA &amp; FALSE，不论 NA 是什么，该表达式总为假。 NA | TRUE #&gt; [1] TRUE NA &amp; FALSE #&gt; [1] FALSE 而 NA &amp; TRUE和NA | FALSE则会返回 NA： NA &amp; TRUE #&gt; [1] NA NA | FALSE #&gt; [1] NA 1.1.4 slice() slice() 及相关帮助函数同样可以用来筛选行， mtcars %&gt;% slice(1L) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 # Similar to tail(mtcars, 1): mtcars %&gt;% slice(n()) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; 1 21.4 4 121 109 4.11 2.78 18.6 1 1 4 2 mtcars %&gt;% slice(5:n()) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; 1 18.7 8 360.0 175 3.15 3.44 17.0 0 0 3 2 #&gt; 2 18.1 6 225.0 105 2.76 3.46 20.2 1 0 3 1 #&gt; 3 14.3 8 360.0 245 3.21 3.57 15.8 0 0 3 4 #&gt; 4 24.4 4 146.7 62 3.69 3.19 20.0 1 0 4 2 #&gt; 5 22.8 4 140.8 95 3.92 3.15 22.9 1 0 4 2 #&gt; 6 19.2 6 167.6 123 3.92 3.44 18.3 1 0 4 4 #&gt; 7 17.8 6 167.6 123 3.92 3.44 18.9 1 0 4 4 #&gt; 8 16.4 8 275.8 180 3.07 4.07 17.4 0 0 3 3 #&gt; 9 17.3 8 275.8 180 3.07 3.73 17.6 0 0 3 3 #&gt; 10 15.2 8 275.8 180 3.07 3.78 18.0 0 0 3 3 #&gt; 11 10.4 8 472.0 205 2.93 5.25 18.0 0 0 3 4 #&gt; 12 10.4 8 460.0 215 3.00 5.42 17.8 0 0 3 4 #&gt; 13 14.7 8 440.0 230 3.23 5.34 17.4 0 0 3 4 #&gt; 14 32.4 4 78.7 66 4.08 2.20 19.5 1 1 4 1 #&gt; 15 30.4 4 75.7 52 4.93 1.61 18.5 1 1 4 2 #&gt; 16 33.9 4 71.1 65 4.22 1.83 19.9 1 1 4 1 #&gt; 17 21.5 4 120.1 97 3.70 2.46 20.0 1 0 3 1 #&gt; 18 15.5 8 318.0 150 2.76 3.52 16.9 0 0 3 2 #&gt; 19 15.2 8 304.0 150 3.15 3.44 17.3 0 0 3 2 #&gt; 20 13.3 8 350.0 245 3.73 3.84 15.4 0 0 3 4 #&gt; 21 19.2 8 400.0 175 3.08 3.85 17.1 0 0 3 2 #&gt; 22 27.3 4 79.0 66 4.08 1.94 18.9 1 1 4 1 #&gt; 23 26.0 4 120.3 91 4.43 2.14 16.7 0 1 5 2 #&gt; 24 30.4 4 95.1 113 3.77 1.51 16.9 1 1 5 2 #&gt; 25 15.8 8 351.0 264 4.22 3.17 14.5 0 1 5 4 #&gt; 26 19.7 6 145.0 175 3.62 2.77 15.5 0 1 5 6 #&gt; 27 15.0 8 301.0 335 3.54 3.57 14.6 0 1 5 8 #&gt; 28 21.4 4 121.0 109 4.11 2.78 18.6 1 1 4 2 # Rows can be dropped with negative indices: slice(mtcars, -(1:4)) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; 1 18.7 8 360.0 175 3.15 3.44 17.0 0 0 3 2 #&gt; 2 18.1 6 225.0 105 2.76 3.46 20.2 1 0 3 1 #&gt; 3 14.3 8 360.0 245 3.21 3.57 15.8 0 0 3 4 #&gt; 4 24.4 4 146.7 62 3.69 3.19 20.0 1 0 4 2 #&gt; 5 22.8 4 140.8 95 3.92 3.15 22.9 1 0 4 2 #&gt; 6 19.2 6 167.6 123 3.92 3.44 18.3 1 0 4 4 #&gt; 7 17.8 6 167.6 123 3.92 3.44 18.9 1 0 4 4 #&gt; 8 16.4 8 275.8 180 3.07 4.07 17.4 0 0 3 3 #&gt; 9 17.3 8 275.8 180 3.07 3.73 17.6 0 0 3 3 #&gt; 10 15.2 8 275.8 180 3.07 3.78 18.0 0 0 3 3 #&gt; 11 10.4 8 472.0 205 2.93 5.25 18.0 0 0 3 4 #&gt; 12 10.4 8 460.0 215 3.00 5.42 17.8 0 0 3 4 #&gt; 13 14.7 8 440.0 230 3.23 5.34 17.4 0 0 3 4 #&gt; 14 32.4 4 78.7 66 4.08 2.20 19.5 1 1 4 1 #&gt; 15 30.4 4 75.7 52 4.93 1.61 18.5 1 1 4 2 #&gt; 16 33.9 4 71.1 65 4.22 1.83 19.9 1 1 4 1 #&gt; 17 21.5 4 120.1 97 3.70 2.46 20.0 1 0 3 1 #&gt; 18 15.5 8 318.0 150 2.76 3.52 16.9 0 0 3 2 #&gt; 19 15.2 8 304.0 150 3.15 3.44 17.3 0 0 3 2 #&gt; 20 13.3 8 350.0 245 3.73 3.84 15.4 0 0 3 4 #&gt; 21 19.2 8 400.0 175 3.08 3.85 17.1 0 0 3 2 #&gt; 22 27.3 4 79.0 66 4.08 1.94 18.9 1 1 4 1 #&gt; 23 26.0 4 120.3 91 4.43 2.14 16.7 0 1 5 2 #&gt; 24 30.4 4 95.1 113 3.77 1.51 16.9 1 1 5 2 #&gt; 25 15.8 8 351.0 264 4.22 3.17 14.5 0 1 5 4 #&gt; 26 19.7 6 145.0 175 3.62 2.77 15.5 0 1 5 6 #&gt; 27 15.0 8 301.0 335 3.54 3.57 14.6 0 1 5 8 #&gt; 28 21.4 4 121.0 109 4.11 2.78 18.6 1 1 4 2 当要求依据某种排序选出前 / 后 n个观测时，slice_max() 和 slice_min() 可以很好地完成任务 # top 5 cars with largest mpg mtcars %&gt;% slice_max(order_by = mpg, n = 5) # top 10% cars with least mpg, with ties not kept together mtcars %&gt;% slice_min(order_by = mpg, prop = 0.05, with_ties = FALSE) slice_head() and slice_tail() select the first or last rows. slice_sample() randomly selects rows. # First and last rows based on existing order mtcars %&gt;% slice_head(n = 5) mtcars %&gt;% slice_tail(n = 5) # slice_sample() allows you to random select with or without replacement mtcars %&gt;% slice_sample(n = 5) mtcars %&gt;% slice_sample(n = 5, replace = TRUE) "],
["arrange.html", "1.2 arrange()", " 1.2 arrange() arrange() 的工作方式与 filter() 非常相似，但它不是筛选行，而是改变行的顺序。它接受一个数据集和一组作为排序依据的列名（或者更复杂的表达式）作为参数，如果列名不止一个，那么就是用后面的列在前面排序的基础上继续排序： # 依次按照 year -&gt; month -&gt; day 三个变量按升序排序 flights %&gt;% arrange(year, month, day) #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 819 #&gt; 2 2013 1 1 533 529 4 850 830 #&gt; 3 2013 1 1 542 540 2 923 850 #&gt; 4 2013 1 1 544 545 -1 1004 1022 #&gt; 5 2013 1 1 554 600 -6 812 837 #&gt; 6 2013 1 1 554 558 -4 740 728 #&gt; # ... with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 如果想要按某列降序排列行，可以对该列名使用函数desc()： flights %&gt;% arrange(desc(arr_delay)) ## 按照到达延误时间从大到小排序 #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 9 641 900 1301 1242 1530 #&gt; 2 2013 6 15 1432 1935 1137 1607 2120 #&gt; 3 2013 1 10 1121 1635 1126 1239 1810 #&gt; 4 2013 9 20 1139 1845 1014 1457 2210 #&gt; 5 2013 7 22 845 1600 1005 1044 1815 #&gt; 6 2013 4 10 1100 1900 960 1342 2211 #&gt; # ... with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 缺失值总是排在最后： df &lt;- tibble(x = c(5, 2, NA)) df %&gt;% arrange(x) #&gt; # A tibble: 3 x 1 #&gt; x #&gt; &lt;dbl&gt; #&gt; 1 2 #&gt; 2 5 #&gt; 3 NA 如果参数是一个关于某些列的表达式 expression(var)，它的意思是告诉 arrange()：“按照各行在这个表达式上的取值的从小到大的顺序排列观测 # dep_delay的绝对值小的行被排在前面 flights %&gt;% arrange((dep_delay ** 2)) #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 559 559 0 702 706 #&gt; 2 2013 1 1 600 600 0 851 858 #&gt; 3 2013 1 1 600 600 0 837 825 #&gt; 4 2013 1 1 607 607 0 858 915 #&gt; 5 2013 1 1 615 615 0 1039 1100 #&gt; 6 2013 1 1 615 615 0 833 842 #&gt; # ... with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 例如，我们希望找到总延误时间(dep_delay + arr_delay)最长的航班 ## 延误时间最长的航班 flights %&gt;% arrange(desc(arr_delay + dep_delay)) #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 9 641 900 1301 1242 1530 #&gt; 2 2013 6 15 1432 1935 1137 1607 2120 #&gt; 3 2013 1 10 1121 1635 1126 1239 1810 #&gt; 4 2013 9 20 1139 1845 1014 1457 2210 #&gt; 5 2013 7 22 845 1600 1005 1044 1815 #&gt; 6 2013 4 10 1100 1900 960 1342 2211 #&gt; # ... with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 如果A行在两个量上的取值是(2,5)，B行是(1,10)，B会出现在A后面，因为arrange()按照这个表达式来进行排列。 1.2.1 Exercises Exercise 1.4 如何使用 arrange() 将缺失值排在最前面？（提示：使用is.na()) 例：把 flights 数据集中dep_time上的 NA 值排在最前面( to sort the data frame by departure time (dep_time) in ascending order but NA values first) 首先is.na(dep_time)将把所有 NA 变为 TRUE (1)，其他数值变成 FALSE (0)，所以 desc(is.na(dep_time)) 是一个排序依据,它告诉 arrange()，把那些经过运算is.na(dep_time)后值大的行排在前面，即原先的 NA ； 之后，我们仍希望在那些不含 NA 的观测中按照 dep_time 排序，所以添加一个参数 dep_time： flights %&gt;% arrange(desc(is.na(dep_time)), dep_time) #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 NA 1630 NA NA 1815 #&gt; 2 2013 1 1 NA 1935 NA NA 2240 #&gt; 3 2013 1 1 NA 1500 NA NA 1825 #&gt; 4 2013 1 1 NA 600 NA NA 901 #&gt; 5 2013 1 2 NA 1540 NA NA 1747 #&gt; 6 2013 1 2 NA 1620 NA NA 1746 #&gt; # ... with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Exercise 1.5 对 flights 排序以找出延误时间最长的航班。找出出发时间最早的航班。 # 延误时间最长的航班 flights %&gt;% arrange(desc(dep_delay)) #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 9 641 900 1301 1242 1530 #&gt; 2 2013 6 15 1432 1935 1137 1607 2120 #&gt; 3 2013 1 10 1121 1635 1126 1239 1810 #&gt; 4 2013 9 20 1139 1845 1014 1457 2210 #&gt; 5 2013 7 22 845 1600 1005 1044 1815 #&gt; 6 2013 4 10 1100 1900 960 1342 2211 #&gt; # ... with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # 出发时间最早的航班 flights %&gt;% arrange(dep_time) #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 13 1 2249 72 108 2357 #&gt; 2 2013 1 31 1 2100 181 124 2225 #&gt; 3 2013 11 13 1 2359 2 442 440 #&gt; 4 2013 12 16 1 2359 2 447 437 #&gt; 5 2013 12 20 1 2359 2 430 440 #&gt; 6 2013 12 26 1 2359 2 437 440 #&gt; # ... with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Exercise 1.6 对 flights 排序以找出速度最快的航班 这个排序条件需要用到表达式，速度 = distance / air_time flights %&gt;% arrange(distance / air_time) #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 28 1917 1825 52 2118 1935 #&gt; 2 2013 6 29 755 800 -5 1035 909 #&gt; 3 2013 8 28 932 940 -8 1116 1051 #&gt; 4 2013 1 30 1037 955 42 1221 1100 #&gt; 5 2013 11 27 556 600 -4 727 658 #&gt; 6 2013 5 21 558 600 -2 721 657 #&gt; # ... with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; "],
["select.html", "1.3 select()", " 1.3 select() 如今，数据集有几百个甚至几千个变量已经司空见惯。这种情况下，如何找出真正感兴趣的变量经常是一个挑战。通过基于变量名的操作，select()函数可以让我们快速生成一个有用的变量子集。 glimpse(flights) #&gt; Rows: 336,776 #&gt; Columns: 19 #&gt; $ year &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013... #&gt; $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... #&gt; $ day &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... #&gt; $ dep_time &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 55... #&gt; $ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 60... #&gt; $ dep_delay &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2,... #&gt; $ arr_time &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 8... #&gt; $ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 8... #&gt; $ arr_delay &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7,... #&gt; $ carrier &lt;chr&gt; &quot;UA&quot;, &quot;UA&quot;, &quot;AA&quot;, &quot;B6&quot;, &quot;DL&quot;, &quot;UA&quot;, &quot;B6&quot;, &quot;EV&quot;, &quot;B6&quot;... #&gt; $ flight &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301... #&gt; $ tailnum &lt;chr&gt; &quot;N14228&quot;, &quot;N24211&quot;, &quot;N619AA&quot;, &quot;N804JB&quot;, &quot;N668DN&quot;, &quot;N... #&gt; $ origin &lt;chr&gt; &quot;EWR&quot;, &quot;LGA&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;LGA&quot;, &quot;EWR&quot;, &quot;EWR&quot;, &quot;LG... #&gt; $ dest &lt;chr&gt; &quot;IAH&quot;, &quot;IAH&quot;, &quot;MIA&quot;, &quot;BQN&quot;, &quot;ATL&quot;, &quot;ORD&quot;, &quot;FLL&quot;, &quot;IA... #&gt; $ air_time &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149... #&gt; $ distance &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 73... #&gt; $ hour &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6... #&gt; $ minute &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59... #&gt; $ time_hour &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-0... # 选择 year,month,day 三个变量 flights %&gt;% select(year, month, day) #&gt; # A tibble: 336,776 x 3 #&gt; year month day #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 #&gt; 2 2013 1 1 #&gt; 3 2013 1 1 #&gt; 4 2013 1 1 #&gt; 5 2013 1 1 #&gt; 6 2013 1 1 #&gt; # ... with 336,770 more rows 顺便说一句，如果把变量名变成字符串或者它在所有变量中的顺序也可以正常工作,如 flights %&gt;% select(\"year\", \"month\", \"day\")和flights %&gt;% select(1, 2, 3)和上面代码会返回一样结果，但是这两种方法都不值得推荐。 关于 Non-standard evalution {{ var }} !!enquo(var) 或者 .data[[var]] 将会使 select()按照我们希望的那样工作。 my_select &lt;- function(df, var) { select(df, var) } df &lt;- tibble(x = 1:3, y = 4:6) # wrong way df %&gt;% my_select(x) #&gt; Error: Selections can&#39;t have missing values. # solution 1 my_select &lt;- function(df, var) { select(df, {{ var }}) } df %&gt;% my_select(x) #&gt; # A tibble: 3 x 1 #&gt; x #&gt; &lt;int&gt; #&gt; 1 1 #&gt; 2 2 #&gt; 3 3 # solution 2 my_select &lt;- function(df, var) { select(df, var) } df %&gt;% my_select(.data$x) #&gt; # A tibble: 3 x 0 # solution 3 my_select &lt;- function(df, var) { select(df, !!enquo(var)) } df %&gt;% my_select(x) #&gt; # A tibble: 3 x 1 #&gt; x #&gt; &lt;int&gt; #&gt; 1 1 #&gt; 2 2 #&gt; 3 3 这里的问题同样适用于 dplyr 中的其他采用惰性求值的函数。另外, all_of(var) 和 .env$var 可以处理相反的问题。 var &lt;- c(&quot;x&quot;, &quot;y&quot;) df %&gt;% select(all_of(var)) #&gt; # A tibble: 3 x 2 #&gt; x y #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 4 #&gt; 2 2 5 #&gt; 3 3 6 df %&gt;% select(.env$var) #&gt; # A tibble: 3 x 2 #&gt; x y #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 4 #&gt; 2 2 5 #&gt; 3 3 6 select() 还可以重命名变量，但应该避免这样使用它，因为这样会丢掉所有未明确提及的变量。我们应该使用 select() 函数的变体 rename() 函数来重命名变量，它会把未提及的那些变量按照原名字放到生成的数据框里： # 将 tail_num 重命名为 tailnum rename(flights, tail_num = tailnum) #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 819 #&gt; 2 2013 1 1 533 529 4 850 830 #&gt; 3 2013 1 1 542 540 2 923 850 #&gt; 4 2013 1 1 544 545 -1 1004 1022 #&gt; 5 2013 1 1 554 600 -6 812 837 #&gt; 6 2013 1 1 554 558 -4 740 728 #&gt; # ... with 336,770 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tail_num &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 其他常见的select()函数用法如下所示： ## 选择“year”和“day”之间的所有变量，冒号 flights %&gt;% select(year:day) #&gt; # A tibble: 336,776 x 3 #&gt; year month day #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 #&gt; 2 2013 1 1 #&gt; 3 2013 1 1 #&gt; 4 2013 1 1 #&gt; 5 2013 1 1 #&gt; 6 2013 1 1 #&gt; # ... with 336,770 more rows ## 选择不在“year”和“day”之间的所有列，减号 flights %&gt;% select(-(year:day)) #&gt; # A tibble: 336,776 x 16 #&gt; dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier #&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 517 515 2 830 819 11 UA #&gt; 2 533 529 4 850 830 20 UA #&gt; 3 542 540 2 923 850 33 AA #&gt; 4 544 545 -1 1004 1022 -18 B6 #&gt; 5 554 600 -6 812 837 -25 DL #&gt; 6 554 558 -4 740 728 12 UA #&gt; # ... with 336,770 more rows, and 9 more variables: flight &lt;int&gt;, #&gt; # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, #&gt; # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 还可以在select()函数中使用一些辅助函数： * starts_with(\"abc\")：匹配以名字以“abc”开头的列 flights %&gt;% select(starts_with(&quot;arr&quot;)) ## 所有以arr开头的列 #&gt; # A tibble: 336,776 x 2 #&gt; arr_time arr_delay #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 830 11 #&gt; 2 850 20 #&gt; 3 923 33 #&gt; 4 1004 -18 #&gt; 5 812 -25 #&gt; 6 740 12 #&gt; # ... with 336,770 more rows ends_with(\"xyz\"): 匹配名字以“xyz”结尾的列 ## 所有以&quot;delay&quot;结尾的列 flights %&gt;% select(ends_with(&quot;delay&quot;)) #&gt; # A tibble: 336,776 x 2 #&gt; dep_delay arr_delay #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2 11 #&gt; 2 4 20 #&gt; 3 2 33 #&gt; 4 -1 -18 #&gt; 5 -6 -25 #&gt; 6 -4 12 #&gt; # ... with 336,770 more rows contains(\"ijk\")，匹配名字包含“ijk”的列 ## 所有包含&quot;time&quot;的列 flights %&gt;% select(contains(&quot;time&quot;)) #&gt; # A tibble: 336,776 x 6 #&gt; dep_time sched_dep_time arr_time sched_arr_time air_time time_hour #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dttm&gt; #&gt; 1 517 515 830 819 227 2013-01-01 05:00:00 #&gt; 2 533 529 850 830 227 2013-01-01 05:00:00 #&gt; 3 542 540 923 850 160 2013-01-01 05:00:00 #&gt; 4 544 545 1004 1022 183 2013-01-01 05:00:00 #&gt; 5 554 600 812 837 116 2013-01-01 06:00:00 #&gt; 6 554 558 740 728 150 2013-01-01 05:00:00 #&gt; # ... with 336,770 more rows matches(\"(.)\\\\1\")：选择名字符合正则表达式要求的列，后面将具体讲述正则表达式 num_range(\"x\",c(1,2,3))，选择名字为“x1”、“x2”、“x3”的列 df &lt;- tibble(x1 = 1,x2 = 2,x3 = 3) df %&gt;% select(num_range(&quot;X&quot;, 1:3)) #&gt; # A tibble: 1 x 0 one_of(character_1,···,character_n):如果某个列的名字出现在序列里，则选出它 flights %&gt;% select(one_of(&quot;arr_delay&quot;, &quot;dep_delay&quot;, &quot;xyz&quot;)) #&gt; # A tibble: 336,776 x 2 #&gt; arr_delay dep_delay #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 11 2 #&gt; 2 20 4 #&gt; 3 33 2 #&gt; 4 -18 -1 #&gt; 5 -25 -6 #&gt; 6 12 -4 #&gt; # ... with 336,770 more rows everything()：匹配所有(剩余)变量，当想要将几个变量移到数据集开头时，这种方法很有用 ## 将time_hour和air_time两个变量移到flights数据的开头 flights %&gt;% select(time_hour,air_time, everything()) #&gt; # A tibble: 336,776 x 19 #&gt; time_hour air_time year month day dep_time sched_dep_time #&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013-01-01 05:00:00 227 2013 1 1 517 515 #&gt; 2 2013-01-01 05:00:00 227 2013 1 1 533 529 #&gt; 3 2013-01-01 05:00:00 160 2013 1 1 542 540 #&gt; 4 2013-01-01 05:00:00 183 2013 1 1 544 545 #&gt; 5 2013-01-01 06:00:00 116 2013 1 1 554 600 #&gt; 6 2013-01-01 05:00:00 150 2013 1 1 554 558 #&gt; # ... with 336,770 more rows, and 12 more variables: dep_delay &lt;dbl&gt;, #&gt; # arr_time &lt;int&gt;, sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, #&gt; # flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, distance &lt;dbl&gt;, #&gt; # hour &lt;dbl&gt;, minute &lt;dbl&gt; last_col(offset = n)：选择倒数第n列，不设置offset时，默认选择最后一列 flights %&gt;% select(last_col()) #&gt; # A tibble: 336,776 x 1 #&gt; time_hour #&gt; &lt;dttm&gt; #&gt; 1 2013-01-01 05:00:00 #&gt; 2 2013-01-01 05:00:00 #&gt; 3 2013-01-01 05:00:00 #&gt; 4 2013-01-01 05:00:00 #&gt; 5 2013-01-01 06:00:00 #&gt; 6 2013-01-01 05:00:00 #&gt; # ... with 336,770 more rows 利用这些帮助函数，我们可以为选择列设置任意数目的条件，select()中以逗号分隔的列表示“或” 关系，如： ## 找出以“arr”开头或者以“time”结尾的列 flights %&gt;% select(starts_with(&quot;arr&quot;), ends_with(&quot;time&quot;)) #&gt; # A tibble: 336,776 x 6 #&gt; arr_time arr_delay dep_time sched_dep_time sched_arr_time air_time #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 830 11 517 515 819 227 #&gt; 2 850 20 533 529 830 227 #&gt; 3 923 33 542 540 850 160 #&gt; 4 1004 -18 544 545 1022 183 #&gt; 5 812 -25 554 600 837 116 #&gt; 6 740 12 554 558 728 150 #&gt; # ... with 336,770 more rows 注意: 所有帮助函数都忽略大小写: flights %&gt;% select(ends_with(&quot;DELAY&quot;)) #&gt; # A tibble: 336,776 x 2 #&gt; dep_delay arr_delay #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2 11 #&gt; 2 4 20 #&gt; 3 2 33 #&gt; 4 -1 -18 #&gt; 5 -6 -25 #&gt; 6 -4 12 #&gt; # ... with 336,770 more rows 如果要区分大小写，可以设置任意帮助函数的参数ignore.case = FALSE flights %&gt;% select(ends_with(&quot;DELAY&quot;, ignore.case = F)) ## 将不会选择出任何列 #&gt; # A tibble: 336,776 x 0 1.3.1 练习 Exercise 1.7 从 flights 中选择 dep_time、dep_delay、arr_time、arr_delay，找出尽可能多的方法 先查看这些变量的位置： glimpse(flights) #&gt; Rows: 336,776 #&gt; Columns: 19 #&gt; $ year &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013... #&gt; $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... #&gt; $ day &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... #&gt; $ dep_time &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 55... #&gt; $ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 60... #&gt; $ dep_delay &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2,... #&gt; $ arr_time &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 8... #&gt; $ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 8... #&gt; $ arr_delay &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7,... #&gt; $ carrier &lt;chr&gt; &quot;UA&quot;, &quot;UA&quot;, &quot;AA&quot;, &quot;B6&quot;, &quot;DL&quot;, &quot;UA&quot;, &quot;B6&quot;, &quot;EV&quot;, &quot;B6&quot;... #&gt; $ flight &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301... #&gt; $ tailnum &lt;chr&gt; &quot;N14228&quot;, &quot;N24211&quot;, &quot;N619AA&quot;, &quot;N804JB&quot;, &quot;N668DN&quot;, &quot;N... #&gt; $ origin &lt;chr&gt; &quot;EWR&quot;, &quot;LGA&quot;, &quot;JFK&quot;, &quot;JFK&quot;, &quot;LGA&quot;, &quot;EWR&quot;, &quot;EWR&quot;, &quot;LG... #&gt; $ dest &lt;chr&gt; &quot;IAH&quot;, &quot;IAH&quot;, &quot;MIA&quot;, &quot;BQN&quot;, &quot;ATL&quot;, &quot;ORD&quot;, &quot;FLL&quot;, &quot;IA... #&gt; $ air_time &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149... #&gt; $ distance &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 73... #&gt; $ hour &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6... #&gt; $ minute &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59... #&gt; $ time_hour &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-0... # 常规方法 flights %&gt;% select(dep_time, dep_delay, arr_time, arr_delay) #&gt; # A tibble: 336,776 x 4 #&gt; dep_time dep_delay arr_time arr_delay #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 2 830 11 #&gt; 2 533 4 850 20 #&gt; 3 542 2 923 33 #&gt; 4 544 -1 1004 -18 #&gt; 5 554 -6 812 -25 #&gt; 6 554 -4 740 12 #&gt; # ... with 336,770 more rows # one_of() vars = c(&quot;dep_time&quot;, &quot;dep_delay&quot;, &quot;arr_time&quot;, &quot;arr_delay&quot;) flights %&gt;% select(one_of(vars)) #&gt; # A tibble: 336,776 x 4 #&gt; dep_time dep_delay arr_time arr_delay #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 517 2 830 11 #&gt; 2 533 4 850 20 #&gt; 3 542 2 923 33 #&gt; 4 544 -1 1004 -18 #&gt; 5 554 -6 812 -25 #&gt; 6 554 -4 740 12 #&gt; # ... with 336,770 more rows # 多个条件 flights %&gt;% select(starts_with(&quot;arr_&quot;), starts_with(&quot;dep_&quot;)) #&gt; # A tibble: 336,776 x 4 #&gt; arr_time arr_delay dep_time dep_delay #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 830 11 517 2 #&gt; 2 850 20 533 4 #&gt; 3 923 33 542 2 #&gt; 4 1004 -18 544 -1 #&gt; 5 812 -25 554 -6 #&gt; 6 740 12 554 -4 #&gt; # ... with 336,770 more rows Exercise 1.8 如果在select()中多次计入一个变量名，会发生什么情况？ select()函数将会忽略重复出现的变量，只选出一列，同时也不会报错： flights %&gt;% select(year, month, day, year, month, day) #&gt; # A tibble: 336,776 x 3 #&gt; year month day #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 #&gt; 2 2013 1 1 #&gt; 3 2013 1 1 #&gt; 4 2013 1 1 #&gt; 5 2013 1 1 #&gt; 6 2013 1 1 #&gt; # ... with 336,770 more rows ##mutate() 除了选择现有的列，经常还需要添加新列。新列是现有列的函数，这就 mutate() 函数的作用。 mutate() 总是将新列添加在最后,格式为 新列名= 表达式。为了便于观察它的效果，我们需要先创建一个更狭窄的数据集，以便能看到新变量。 例如，我们希望创建两个新列gain和hours，分别表示飞机在飞行过程中弥补的延误时间 (gain = arr_dealy - dep_delay)，然后把飞行时间换算成小时 hours = air_time / 60 (flights_gain &lt;- flights %&gt;% select(ends_with(&quot;delay&quot;),air_time) %&gt;% mutate(gain = arr_delay - dep_delay, hours = air_time / 60)) #&gt; # A tibble: 336,776 x 5 #&gt; dep_delay arr_delay air_time gain hours #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2 11 227 9 3.78 #&gt; 2 4 20 227 16 3.78 #&gt; 3 2 33 160 31 2.67 #&gt; 4 -1 -18 183 -17 3.05 #&gt; 5 -6 -25 116 -19 1.93 #&gt; 6 -4 12 150 16 2.5 #&gt; # ... with 336,770 more rows 一旦新列被创建，就可以立即使用。例如，可能想知道对gain做时间上的平均： flights_gain %&gt;% mutate(gain_per_hour = gain / hours) #&gt; # A tibble: 336,776 x 6 #&gt; dep_delay arr_delay air_time gain hours gain_per_hour #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2 11 227 9 3.78 2.38 #&gt; 2 4 20 227 16 3.78 4.23 #&gt; 3 2 33 160 31 2.67 11.6 #&gt; 4 -1 -18 183 -17 3.05 -5.57 #&gt; 5 -6 -25 116 -19 1.93 -9.83 #&gt; 6 -4 12 150 16 2.5 6.4 #&gt; # ... with 336,770 more rows 以上的数据转换也可以通过mutate()一次完成： flights %&gt;% select(ends_with(&quot;delay&quot;),air_time) %&gt;% mutate( gain = arr_delay - dep_delay, hours = air_time / 60, gain_per_hour = gain/hours) #&gt; # A tibble: 336,776 x 6 #&gt; dep_delay arr_delay air_time gain hours gain_per_hour #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2 11 227 9 3.78 2.38 #&gt; 2 4 20 227 16 3.78 4.23 #&gt; 3 2 33 160 31 2.67 11.6 #&gt; 4 -1 -18 183 -17 3.05 -5.57 #&gt; 5 -6 -25 116 -19 1.93 -9.83 #&gt; 6 -4 12 150 16 2.5 6.4 #&gt; # ... with 336,770 more rows 如果只想在保留新变量，可以使用transmute()： flights %&gt;% select(ends_with(&quot;delay&quot;),air_time) %&gt;% transmute( gain = arr_delay - dep_delay, hours = air_time / 60, gain_per_hour = gain/hours) #&gt; # A tibble: 336,776 x 3 #&gt; gain hours gain_per_hour #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 9 3.78 2.38 #&gt; 2 16 3.78 4.23 #&gt; 3 31 2.67 11.6 #&gt; 4 -17 3.05 -5.57 #&gt; 5 -19 1.93 -9.83 #&gt; 6 16 2.5 6.4 #&gt; # ... with 336,770 more rows 1.3.2 常用创建函数 有多种函数可以帮助mutate()创建新变量。比较重要的一点是，这些函数必须是向量化的：它能接受一个向量作为输入，并返回一个向量作为输出，而且输入和输出向量长度相等。下面介绍一些比较常用的函数。 **算术运算符 +、-、*、/、^** 它们都是向量化的，使用所谓的“循环法则(recycling rules)”。如果一个参数比另一个参数短，那么前者会自动扩展到相同的长度，但某个参数是单个数值时，这种方式是最有效的，如air_time * 60 或者 hours * 60 + minute等。 算术运算符的另一个用途是与我们后面将很快学到的聚集函数结合起来使用。例如,x / sum(x)可以计算出x的各个分量在总数中的比例，y - mean(y)计算出y的各个分量与均值之间的差异。 模运算符 %/% 和 % %/%（整除）和%%（求余）满足x == y * (x %/% y) + （x %% y, 这两个运算符在Python中分别是// 和 % 。 模运算非常好用，因为它可以拆分整数。例如，在flights数据集中，可以根据dep_time计算出 hour 和 minute： ## air_time中表示时间的方式是“xyz”表示x点yz分 flights %&gt;% transmute(hour = air_time %/% 100, minute = air_time %% 100) #&gt; # A tibble: 336,776 x 2 #&gt; hour minute #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2 27 #&gt; 2 2 27 #&gt; 3 1 60 #&gt; 4 1 83 #&gt; 5 1 16 #&gt; 6 1 50 #&gt; # ... with 336,770 more rows 对数函数 log()/log2()/log10() 在处理取值范围变化多个数量级的数据时，对数变换很有用。其他条件相同的情况下，更推荐使用log2()函数，因为它的解释很容易，对数变换后的变量每增加一个单位，意味着原始变量加倍 ； 减少一个单位，则原始数据变为原来的一半。 偏移函数 lead()和lag()函数分别将一个向量向前或向后移动指定的单位： x &lt;- 1:10 ## 将x向前移动2个单位 lead(x,n=2) #&gt; [1] 3 4 5 6 7 8 9 10 NA NA ## 将x向后移动一个单位（默认n=1） lag(x) #&gt; [1] NA 1 2 3 4 5 6 7 8 9 累加和滚动聚合 R的基础包提供了计算累加和、累加积、累加最小值和累加最大值的函数：cumsum()、cumprod()、cummax()、cummin()；dplyr包还提供了cummean()函数以计算累积平均值。 x &lt;- 1:10 cumsum(x) #&gt; [1] 1 3 6 10 15 21 28 36 45 55 cumprod(x) #&gt; [1] 1 2 6 24 120 720 5040 40320 362880 #&gt; [10] 3628800 cummax(x) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 cummin(x) #&gt; [1] 1 1 1 1 1 1 1 1 1 1 cummean(x) #&gt; [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 逻辑比较 &gt;、&lt;=、&gt;、&gt;=、==、!= 如果要进行一系列复杂的逻辑运算，最好将中间结果保存在新变量中，这样就可以检查每一步是否都符合预期。 排秩 排秩函数有很多，从min_rank()开始，它可以完成最常用的排秩任务。默认的排秩方式是，最小的值获得最前面的秩（升序），使用desc(x)可以让最大的值获得前面的名次,NA值对应的秩也是NA： x &lt;- c(1,2,2,NA,3,4) min_rank(x) #&gt; [1] 1 2 2 NA 4 5 min_rank(desc(x)) #&gt; [1] 5 3 3 NA 2 1 min_rank()函数把相同值赋予相同的秩，如果有n个值秩相同为x，则下一个值的秩会直接从x+n开始 如果min_rank()无法满足需要，可以看一下它的一些变体： row_number(),相同值不同秩 x &lt;- c(1,2,2,NA,3,4) min_rank(x) #&gt; [1] 1 2 2 NA 4 5 row_number(x) #&gt; [1] 1 2 3 NA 4 5 dense_rank：相同值的秩相同，但下一个值的秩不会跳转 x &lt;- c(1,2,2,NA,3,4) min_rank(x) #&gt; [1] 1 2 2 NA 4 5 row_number(x) #&gt; [1] 1 2 3 NA 4 5 dense_rank(x) #&gt; [1] 1 2 2 NA 3 4 percent_rank(): 将秩按照比例压缩为[0,1]的值 x &lt;- c(1,2,2,NA,3,4) percent_rank(x) #&gt; [1] 0.00 0.25 0.25 NA 0.75 1.00 ntile()：breaks the input vector into n buckets. x &lt;- c(1,3,4,5,6,8) ntile(x,n=2) #&gt; [1] 1 1 1 2 2 2 ntile(x,n=3) #&gt; [1] 1 1 2 2 3 3 1.3.3 Exercises Exercise 1.9 虽然现在的dep_time和sched_dep_time变量方便阅读，但不适合计算，因为它们实际上并不是连续型数值。将它们转换为一种更方便的表示形式，即从 0 点开始的分钟数 # 观察两个变量的存储方式 flights %&gt;% select(dep_time, sched_dep_time) #&gt; # A tibble: 336,776 x 2 #&gt; dep_time sched_dep_time #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 517 515 #&gt; 2 533 529 #&gt; 3 542 540 #&gt; 4 544 545 #&gt; 5 554 600 #&gt; 6 554 558 #&gt; # ... with 336,770 more rows xyz 表示 x 点 yz 分，则总分钟数为x %/% 100 * 60 + x %% 100 ; 但有一个问题是，由于 0 点是用2400代表的，经过这样的转换它变为 1440，我们希望它变为 0，所以在外层再套一个%% 1440 flights %&gt;% transmute( dep_time_mins = (dep_time %/% 100 * 60 + dep_time %% 100) %% 1440, sched_dep_time_mins = (sched_dep_time %/% 100 * 60 + sched_dep_time %% 100) %% 1440) #&gt; # A tibble: 336,776 x 2 #&gt; dep_time_mins sched_dep_time_mins #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 317 315 #&gt; 2 333 329 #&gt; 3 342 340 #&gt; 4 344 345 #&gt; 5 354 360 #&gt; 6 354 358 #&gt; # ... with 336,770 more rows 比较dep_time、sched_dep_time 和 dep_delay，这三者应该是何种关系？ flights_deptime &lt;- mutate(flights, dep_time_min = (dep_time %/% 100 * 60 + dep_time %% 100) %% 1440, sched_dep_time_min = (sched_dep_time %/% 100 * 60 + sched_dep_time %% 100) %% 1440, dep_delay_diff = dep_delay - dep_time_min + sched_dep_time_min ) filter(flights_deptime, dep_delay_diff != 0) %&gt;% select(dep_delay_diff) #&gt; # A tibble: 1,236 x 1 #&gt; dep_delay_diff #&gt; &lt;dbl&gt; #&gt; 1 1440 #&gt; 2 1440 #&gt; 3 1440 #&gt; 4 1440 #&gt; 5 1440 #&gt; 6 1440 #&gt; # ... with 1,230 more rows 如上所示，经过分钟的转换后，有1236行的dep_delay 不等于 dep_time - sched_dep_time. 有趣的是，这些差值全部等于1440。 &gt; the discrepancies could be because a flight was scheduled to depart before midnight, but was delayed after midnight. All of these discrepancies are exactly equal to 1440 (24 hours), and the flights with these discrepancies were scheduled to depart later in the day. 使用排秩函数找出10个出发延误时间最长的航班 ## 使min_rank默认小的值获得小的秩，arrange()默认降序排列，其中一个函数中要使用desc() flights %&gt;% mutate(delay_rank = min_rank(desc(dep_delay))) %&gt;% arrange(delay_rank) %&gt;% select(year,month,day,dep_delay,delay_rank) #&gt; # A tibble: 336,776 x 5 #&gt; year month day dep_delay delay_rank #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 2013 1 9 1301 1 #&gt; 2 2013 6 15 1137 2 #&gt; 3 2013 1 10 1126 3 #&gt; 4 2013 9 20 1014 4 #&gt; 5 2013 7 22 1005 5 #&gt; 6 2013 4 10 960 6 #&gt; # ... with 336,770 more rows 1:3 + 1:10会返回什么？为什么？ 1:3 + 1:10 #&gt; [1] 2 4 6 5 7 9 8 10 12 11 当一个向量中的值不够用时，这个向量会被循环使用 1:3 + 1:10等价于c(1 + 1, 2 + 2, 3 + 3, 1 + 4, 2 + 5, 3 + 6, 1 + 7, 2 + 8, 3 + 9, 1 + 10) c(1 + 1, 2 + 2, 3 + 3, 1 + 4, 2 + 5, 3 + 6, 1 + 7, 2 + 8, 3 + 9, 1 + 10) #&gt; [1] 2 4 6 5 7 9 8 10 12 11 "],
["summarize.html", "1.4 summarize()", " 1.4 summarize() 最后一个核心函数是summarize()，它用来计算摘要统计量，可以将数据框折叠成一行： ## 计算平均出发延误时间 summarize(flights,delay = mean(dep_delay,na.rm = TRUE)) #&gt; # A tibble: 1 x 1 #&gt; delay #&gt; &lt;dbl&gt; #&gt; 1 12.6 如果不和 group_by() 一起使用，那么 summarize() 也就没什么大用。group_by() 函数与 summarize() 联合使用的时候可以将分析单位从整个数据集更改为单个分组，接下来，在分组后的数据框上使用dplyr函数时，它们会自动应用到每个分组。更简单第说，你想从哪个层级上分析问题，就在group_by中对什么层级进行分组。group_by() + summarize()可以实现类似aggregate()函数的效果。 例如，我们想知道每一天的平均出发延误时间，可以先对(year,month,day)进行分组，然后再使用summarize()： flights %&gt;% group_by(year,month,day) %&gt;% summarize(delay = mean(dep_delay,na.rm = T)) #&gt; # A tibble: 365 x 4 #&gt; # Groups: year, month [12] #&gt; year month day delay #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 11.5 #&gt; 2 2013 1 2 13.9 #&gt; 3 2013 1 3 11.0 #&gt; 4 2013 1 4 8.95 #&gt; 5 2013 1 5 5.73 #&gt; 6 2013 1 6 7.15 #&gt; # ... with 359 more rows 注意，summarize()很不同的一点就是它会自动选择列，只在结果中显示之前在group_by中进行分类的变量和summarize()中算出的摘要统计量。 这个生成的数据框只有365行，因为flights数据集中的时间跨度只有一年，(year, month, day)的唯一组合只可能有365个，这就是summarize()中的摘要函数的折叠效果：接受一个向量，只返回一个值，然后再用分组变量的一个组合来标识这个摘要量的对象（哪个层级上的平均值、最大值？）。从这个角度看，summarize()和mutate()对函数的要求恰好相反。 用aggregate()函数的写法： aggregate(dep_delay~year+month+day, FUN = mean, data = flights) %&gt;% head(20) #&gt; year month day dep_delay #&gt; 1 2013 1 1 11.549 #&gt; 2 2013 2 1 10.853 #&gt; 3 2013 3 1 11.016 #&gt; 4 2013 4 1 12.421 #&gt; 5 2013 5 1 2.903 #&gt; 6 2013 6 1 2.778 #&gt; 7 2013 7 1 56.234 #&gt; 8 2013 8 1 34.574 #&gt; 9 2013 9 1 4.233 #&gt; 10 2013 10 1 -0.099 #&gt; 11 2013 11 1 13.273 #&gt; 12 2013 12 1 9.004 #&gt; 13 2013 1 2 13.859 #&gt; 14 2013 2 2 5.422 #&gt; 15 2013 3 2 8.027 #&gt; 16 2013 4 2 8.260 #&gt; 17 2013 5 2 6.389 #&gt; 18 2013 6 2 34.013 #&gt; 19 2013 7 2 19.285 #&gt; 20 2013 8 2 13.254 比较这两个结果，我们可以发现group_by()中越靠后的参数是越基本的单位，group_by(year,month,day)将按照 day, month, year 的顺序开始循环 ； 而 aggregate() 函数则正好相反 1.4.1 Missing values in summarize() 在按照日期计算平均出发延误时间的例子中，使用 mean() 时设置了参数na.rm = T，如果没有这样做，很多日期的平均延误时间将是缺失值： flights %&gt;% group_by(year,month,day) %&gt;% summarize(delay = mean(dep_delay)) #&gt; # A tibble: 365 x 4 #&gt; # Groups: year, month [12] #&gt; year month day delay #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 NA #&gt; 2 2013 1 2 NA #&gt; 3 2013 1 3 NA #&gt; 4 2013 1 4 NA #&gt; 5 2013 1 5 NA #&gt; 6 2013 1 6 NA #&gt; # ... with 359 more rows 这是因为聚合函数遵循缺失值的一般规则：如果输入中有缺失值，那么输出也是缺失值。好在所有聚合函数都有一个 na.rm 参数，可以在计算前出去缺失值。 在这个示例中，缺失值来源于取消的航班。我们也可以先取出取消的航班来解决却实质问题。保存去除缺失值的数据集为not_cancelled，以便我们可以在接下来的几个示例中继续使用： not_cancelled &lt;- flights %&gt;% filter(!is.na(dep_delay), !is.na(arr_delay)) not_cancelled %&gt;% group_by(year,month,day) %&gt;% summarize(delay = mean(dep_delay)) #&gt; # A tibble: 365 x 4 #&gt; # Groups: year, month [12] #&gt; year month day delay #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 11.4 #&gt; 2 2013 1 2 13.7 #&gt; 3 2013 1 3 10.9 #&gt; 4 2013 1 4 8.97 #&gt; 5 2013 1 5 5.73 #&gt; 6 2013 1 6 7.15 #&gt; # ... with 359 more rows 1.4.2 计数函数 n() 函数是一个与摘要函数 summarize() 配合的计数函数，它不需要任何参数，单独使用时，它计算的就是行计数： flights %&gt;% summarize(n = n()) #&gt; # A tibble: 1 x 1 #&gt; n #&gt; &lt;int&gt; #&gt; 1 336776 和 group_by() 联合使用时，它可以计算分组变量的每个水平上各有多少个观测： ## 每个月各有多少趟航班 flights %&gt;% group_by(month) %&gt;% summarize(n = n()) ## 等价于summarize(n = sum(month)) #&gt; # A tibble: 12 x 2 #&gt; month n #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 27004 #&gt; 2 2 24951 #&gt; 3 3 28834 #&gt; 4 4 28330 #&gt; 5 5 28796 #&gt; 6 6 28243 #&gt; # ... with 6 more rows n()会把缺失值也包含到计数中，如果想要计算出非缺失值的数量，可以使用sum(is.na(x))。如果想要计算唯一值的数量，可以使用n_distinct() ## 哪个目的地有最多的航空公司？ flights %&gt;% group_by(dest) %&gt;% summarize(carriers = n_distinct(carrier)) %&gt;% arrange(desc(carriers)) #&gt; # A tibble: 105 x 2 #&gt; dest carriers #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 ATL 7 #&gt; 2 BOS 7 #&gt; 3 CLT 7 #&gt; 4 ORD 7 #&gt; 5 TPA 7 #&gt; 6 AUS 6 #&gt; # ... with 99 more rows 除了n() 以外， dplyr 提供了 4 个正式的计数函数： tally(x, wt = NULL, sort = FALSE, name = &quot;n&quot;) count(x, ..., wt = NULL, sort = FALSE, name = &quot;n&quot;, .drop = group_by_drop_default(x)) add_tally(x, wt, sort = FALSE, name = &quot;n&quot;) add_count(x, ..., wt = NULL, sort = FALSE, name = &quot;n&quot;) x %&gt;% group_by(var) %&gt;% tally() 是简化版的 group_by(var) + summarize(n()) x %&gt;% count(var) 等价于 x %&gt;% gruop_by(var) %&gt;% tally() x %&gt;% group_by(var) %&gt;% add_tally 在 原数据集 中增添一列，记录 var 的不同水平的计数，等价于 x %&gt;% add_count(var)，注意这两个函数返回值的维度和原数据框相同(摘要数据框往往不利于细节观察)！！它们等价于 group_by(var) %&gt;% mutate(n()) # 无分组时，tally()即为样本数 mtcars %&gt;% tally() #&gt; n #&gt; 1 32 # tally() 的一般用法 mtcars %&gt;% group_by(cyl) %&gt;% tally() #&gt; # A tibble: 3 x 2 #&gt; cyl n #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 4 11 #&gt; 2 6 7 #&gt; 3 8 14 # count() 等价 group_by() + tally() mtcars %&gt;% count(cyl) #&gt; # A tibble: 3 x 2 #&gt; cyl n #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 4 11 #&gt; 2 6 7 #&gt; 3 8 14 # count() 也可以在已有分组上继续分组 mtcars %&gt;% group_by(gear) %&gt;% count(carb) #&gt; # A tibble: 11 x 3 #&gt; # Groups: gear [3] #&gt; gear carb n #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 3 1 3 #&gt; 2 3 2 4 #&gt; 3 3 3 3 #&gt; 4 3 4 5 #&gt; 5 4 1 4 #&gt; 6 4 2 4 #&gt; # ... with 5 more rows # add_tally() is short-hand for mutate() mtcars %&gt;% add_tally() #&gt; # A tibble: 32 x 12 #&gt; mpg cyl disp hp drat wt qsec vs am gear carb n #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 32 #&gt; 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4 32 #&gt; 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 32 #&gt; 4 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 32 #&gt; 5 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 32 #&gt; 6 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 32 #&gt; # ... with 26 more rows # add_count() is a short-hand for group_by() + add_tally() mtcars %&gt;% add_count(cyl, name = &quot;count&quot;) %&gt;% select(cyl, count) #&gt; # A tibble: 32 x 2 #&gt; cyl count #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 6 7 #&gt; 2 6 7 #&gt; 3 4 11 #&gt; 4 6 7 #&gt; 5 8 14 #&gt; 6 6 7 #&gt; # ... with 26 more rows # add_count() is useful for groupwise filtering # e.g.: show details for species that have a single member starwars %&gt;% add_count(species) %&gt;% filter(n == 1) #&gt; # A tibble: 29 x 14 #&gt; name height mass hair_color skin_color eye_color birth_year gender homeworld #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Gree~ 173 74 &lt;NA&gt; green black 44 male Rodia #&gt; 2 Jabb~ 175 1358 &lt;NA&gt; green-tan~ orange 600 herma~ Nal Hutta #&gt; 3 Yoda 66 17 white green brown 896 male &lt;NA&gt; #&gt; 4 Bossk 190 113 none green red 53 male Trandosha #&gt; 5 Ackb~ 180 83 none brown mot~ orange 41 male Mon Cala #&gt; 6 Wick~ 88 20 brown brown brown 8 male Endor #&gt; # ... with 23 more rows, and 5 more variables: species &lt;chr&gt;, films &lt;list&gt;, #&gt; # vehicles &lt;list&gt;, starships &lt;list&gt;, n &lt;int&gt; 另外， sort = T 可以使观测按照计数倒序排列，name 可以指定新生成的计数行名字（默认为“n”): not_cancelled %&gt;% count(dest, sort = T, name = &quot;count&quot;) #&gt; # A tibble: 104 x 2 #&gt; dest count #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 ATL 16837 #&gt; 2 ORD 16566 #&gt; 3 LAX 16026 #&gt; 4 BOS 15022 #&gt; 5 MCO 13967 #&gt; 6 CLT 13674 #&gt; # ... with 98 more rows 还可以提供一个加权变量。例如，可以使用一下代码算出每架飞机飞行的总里程（实际上就是按计算某变量分组上另一个变量的和）： not_cancelled %&gt;% count(tailnum, wt = distance) #&gt; # A tibble: 4,037 x 2 #&gt; tailnum n #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 D942DN 3418 #&gt; 2 N0EGMQ 239143 #&gt; 3 N10156 109664 #&gt; 4 N102UW 25722 #&gt; 5 N103US 24619 #&gt; 6 N104UW 24616 #&gt; # ... with 4,031 more rows 进行聚合时，包含一列计数 n() 或非缺失值的计数 sum(!is.na()) 很有用。这样就可以检查一下，以确保自己没有基于非常有限的样本做结论。 例如，查看一下具有最长平均到达延误时间的飞机（基于飞机编号进行识别): delays &lt;- not_cancelled %&gt;% group_by(tailnum) %&gt;% summarize(delay = mean(arr_delay)) ggplot(delays) + geom_histogram(aes(delay)) 有些飞机的平均到达延误事件竟然接近 300 分钟，我们可以画一张航班数量和平均延误时间的散点图，一遍获得更深刻的理解: ## n = n()对group_by中的变量水平进行计数，生成一个计数变量命名为n delays &lt;- not_cancelled %&gt;% group_by(tailnum) %&gt;% summarize( delay = mean(arr_delay), n = n()) ggplot(delays) + geom_point(aes(x = n,y = delay),alpha = 0.1) 从散点图可以看出，如果航班对应的出航次数非常少时，平均延误时间的变动特别大，所有延误时间较长的航班的出航次数几乎都在 0 右边一点点。这张图的形状非常能说明问题:当绘制均值（或其他摘要统计量）和分组规模的关系时，总能看到样本量的增加，变动在不断减小。（样本统计量的方差随样本数变小）。 这种数据模式还有另外一种常见的变体。我们来看一下棒球击球手的平均表现与击球次数之间的关系。我们用Lahman包中的数据埃及算棒球大联盟中的每个棒球队员的加大率（安打数 / 打数): library(Lahman) batters &lt;- Batting %&gt;% group_by(playerID) %&gt;% summarize( ba = sum(H,na.rm = T) / sum(AB,na.rm = T), ab = sum(AB,na.rm = T)) batters %&gt;% filter(ab &gt; 100) %&gt;% ggplot(aes(ab, ba)) + geom_point() + geom_smooth(se = FALSE) 当绘制击球手的能力（用打击率 ba 衡量）与击球机会数量（用总打数ab衡量）之间的关系时，可以看到两个趋势： 总大数越多，不同击球手的打击率之间变动越小 能力（ba）和击球机会数量（ab）之间存在正相关。这是因为球队会控制击球手的出场，很显然，球队会优先选择最好的队员。 这对球员排名也有重要印象，如果只是使用desc(ba)进行排序，明显受益的将是那些因为出场数很少而侥幸有很高击打率的球员，而不是真正能力最高的球员： batters %&gt;% arrange(desc(ba)) #&gt; # A tibble: 19,428 x 3 #&gt; playerID ba ab #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 abramge01 1 1 #&gt; 2 alberan01 1 1 #&gt; 3 allarko01 1 1 #&gt; 4 banisje01 1 1 #&gt; 5 bartocl01 1 1 #&gt; 6 bassdo01 1 1 #&gt; # ... with 19,422 more rows 1.4.3 逻辑值的计数和比例:sum(x &gt; 10) 和 mean(y == 0) 当与数值型函数一同使用时，TRUE会转换为1，FALSE会转换为0。这使得sum()和mean()非常适用于逻辑值：sum()可以找出x中TRUE的数量，mean()则可以找出比例。 ## 每天中有多少架航班是在早上5点前出发的？（这通常表明前一天延误的航班数量） not_cancelled %&gt;% group_by(year,month,day) %&gt;% summarize(n_early = sum(dep_time &lt; 500)) #&gt; # A tibble: 365 x 4 #&gt; # Groups: year, month [12] #&gt; year month day n_early #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 0 #&gt; 2 2013 1 2 3 #&gt; 3 2013 1 3 4 #&gt; 4 2013 1 4 3 #&gt; 5 2013 1 5 3 #&gt; 6 2013 1 6 2 #&gt; # ... with 359 more rows ## 每天中到达时间误超过一小时的航班比例是多少？ not_cancelled %&gt;% group_by(year,month,day) %&gt;% summarize(hour_perc = mean(arr_delay &gt; 60)) #&gt; # A tibble: 365 x 4 #&gt; # Groups: year, month [12] #&gt; year month day hour_perc #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 0.0722 #&gt; 2 2013 1 2 0.0851 #&gt; 3 2013 1 3 0.0567 #&gt; 4 2013 1 4 0.0396 #&gt; 5 2013 1 5 0.0349 #&gt; 6 2013 1 6 0.0470 #&gt; # ... with 359 more rows 1.4.4 其他常用的摘要函数 R中还提供了许多常用的摘要函数 位置度量 我们已经使用过mean(x)、但用 median(x) 计算中位数也非常有用。 ## 将聚合函数和逻辑筛选组合起来使用 not_cancelled %&gt;% group_by(year,month,day) %&gt;% summarize( ## 延误时间的中位数 arr_delay1 = median(arr_delay), ## 正延误时间的中位数 arr_delay2 = median(arr_delay[arr_delay &gt; 0]) ) #&gt; # A tibble: 365 x 5 #&gt; # Groups: year, month [12] #&gt; year month day arr_delay1 arr_delay2 #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2013 1 1 3 17 #&gt; 2 2013 1 2 4 16 #&gt; 3 2013 1 3 1 16 #&gt; 4 2013 1 4 -8 16 #&gt; 5 2013 1 5 -7 11 #&gt; 6 2013 1 6 -1 15 #&gt; # ... with 359 more rows 分散程度度量sd(x)、IQR(x)和mad(x) 标准差是分散程度的标准度量方式。四分位距INterquartile RangeIQR(x)和绝对中位差mad(x)基本等价，更适合有离群点的情况： ## 为什么到某些目的地距离比到其他目的地更多变？ not_cancelled %&gt;% group_by(dest) %&gt;% summarize(distance_sd = sd(distance)) %&gt;% arrange(desc(distance_sd)) #&gt; # A tibble: 104 x 2 #&gt; dest distance_sd #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 EGE 10.5 #&gt; 2 SAN 10.4 #&gt; 3 SFO 10.2 #&gt; 4 HNL 10.0 #&gt; 5 SEA 9.98 #&gt; 6 LAS 9.91 #&gt; # ... with 98 more rows 秩的度量:min(x)、quantile(x,0.25)和max(x) 分位数是中位数的扩展。例如quantile(x,0.25)会找出x中按从小到大顺序大于前25%而小于后75%的值（即下四分位数） ## 每天最早和最晚的航班何时出发？ not_cancelled %&gt;% group_by(year,month,day) %&gt;% summarize(first = min(dep_time),last = max(dep_time)) #&gt; # A tibble: 365 x 5 #&gt; # Groups: year, month [12] #&gt; year month day first last #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 2356 #&gt; 2 2013 1 2 42 2354 #&gt; 3 2013 1 3 32 2349 #&gt; 4 2013 1 4 25 2358 #&gt; 5 2013 1 5 14 2357 #&gt; 6 2013 1 6 16 2355 #&gt; # ... with 359 more rows 定位度量:first(x)、nth(x,n)、last(x) 这几个函数的作用与x[1]、x[n]和x[length(x)]相同，只是当定位不存在时（比如尝试从只有两个元素的分组中得到第三个元素），这些函数允许通过参数default设置一个默认值，而后者不能正常工作。 ## 找出每天排在第10的的出发时间记录 not_cancelled %&gt;% group_by(month,year,day) %&gt;% summarize(tenth_dep = nth(dep_time,10)) #&gt; # A tibble: 365 x 4 #&gt; # Groups: month, year [12] #&gt; month year day tenth_dep #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 2013 1 558 #&gt; 2 1 2013 2 554 #&gt; 3 1 2013 3 552 #&gt; 4 1 2013 4 553 #&gt; 5 1 2013 5 555 #&gt; 6 1 2013 6 558 #&gt; # ... with 359 more rows 1.4.5 多个分组变量的消耗 当时用多个分组变量时，每使用一次summarize就会消耗掉一个分组变量，如group_by(year,month,day)经过一次summarize后生成的数据集默认在(year,month)上分组，这使得我们可以对数据集进行循序渐进的分析： daily &lt;- not_cancelled %&gt;% group_by(year,month,day) ## 每天有多少架航班记录 (per_day &lt;- daily %&gt;% summarize(flights = n())) #&gt; # A tibble: 365 x 4 #&gt; # Groups: year, month [12] #&gt; year month day flights #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 831 #&gt; 2 2013 1 2 928 #&gt; 3 2013 1 3 900 #&gt; 4 2013 1 4 908 #&gt; 5 2013 1 5 717 #&gt; 6 2013 1 6 829 #&gt; # ... with 359 more rows ## 每月有多少架航班记录 (per_month &lt;- per_day %&gt;% summarize(flights = sum(flights))) #&gt; # A tibble: 12 x 3 #&gt; # Groups: year [1] #&gt; year month flights #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 26398 #&gt; 2 2013 2 23611 #&gt; 3 2013 3 27902 #&gt; 4 2013 4 27564 #&gt; 5 2013 5 28128 #&gt; 6 2013 6 27075 #&gt; # ... with 6 more rows ## 等价于not_cancelled %&gt;% group_by(year,month) %&gt;% summarize(flights = n()) ## 每年有多少架航班记录 (per_year &lt;- per_month %&gt;% summarize(flights = sum(flights))) #&gt; # A tibble: 1 x 2 #&gt; year flights #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 327346 ## 等价于not_cancelled %&gt;% group_by(year) %&gt;% summarize(flights = n()) 由于分组操作拥有这样的“继承性质”，有的时候可能想要取消分组，并回到未分组的数据继续操作，那么可以使用ungroup()函数取消分组： daily %&gt;% ungroup() %&gt;% summarize(flights = n()) ## 对数据集整体进行摘要统计 #&gt; # A tibble: 1 x 1 #&gt; flights #&gt; &lt;int&gt; #&gt; 1 327346 在循序渐进地进行摘要分析的时候，需要小心：使用求和与计数操作是没有问题的，但如果想要使用加权平均和方差的话，就要仔细考虑一下，任何基于秩的统计数据（如中位数，分为差）都不支持这样的操作。换句话说，对分组结果再求和就是对整体求和，但各分组中的中位数的中位数可不是整体的中位数。 "],
["group-by-combined-with-other-functions.html", "1.5 group_by() combined with other functions", " 1.5 group_by() combined with other functions 虽然与 summarize() 结合起来使用是最常见的，但分组也可以和 mutate() , filter() 和 arrange() 结合使用,以完成非常便捷的操作。 当group_by和mutate()函数结合使用时，摘要函数(summary functions，如mean(), median() 等) 将会自动以分组为基础，一些非摘要函数也会受到分组的影响，如偏移函数 lead()、lag() 和排秩函数 min_rank(), row_number()。而普通的数字运算符+ , -、逻辑运算符&lt; , ==，对数运算log()和余数运算 %/%, %%等将无视分组。 arrange() 默认无视分组, .group = TRUE 避免这一点。 # 分组前后 mutate() df &lt;- tibble( x = 1:9, group = rep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), each = 3) ) gf &lt;- df %&gt;% group_by(group) df %&gt;% mutate(mean(x)) #&gt; # A tibble: 9 x 3 #&gt; x group `mean(x)` #&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 a 5 #&gt; 2 2 a 5 #&gt; 3 3 a 5 #&gt; 4 4 b 5 #&gt; 5 5 b 5 #&gt; 6 6 b 5 #&gt; # ... with 3 more rows gf %&gt;% mutate(mean(x)) #&gt; # A tibble: 9 x 3 #&gt; # Groups: group [3] #&gt; x group `mean(x)` #&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 a 2 #&gt; 2 2 a 2 #&gt; 3 3 a 2 #&gt; 4 4 b 5 #&gt; 5 5 b 5 #&gt; 6 6 b 5 #&gt; # ... with 3 more rows # Arithmetic operators +, -, *, /, ^ are not affected by group_by(). df %&gt;% mutate(y = x + 2) #&gt; # A tibble: 9 x 3 #&gt; x group y #&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 a 3 #&gt; 2 2 a 4 #&gt; 3 3 a 5 #&gt; 4 4 b 6 #&gt; 5 5 b 7 #&gt; 6 6 b 8 #&gt; # ... with 3 more rows gf %&gt;% mutate(z = x + 2) #&gt; # A tibble: 9 x 3 #&gt; # Groups: group [3] #&gt; x group z #&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 a 3 #&gt; 2 2 a 4 #&gt; 3 3 a 5 #&gt; 4 4 b 6 #&gt; 5 5 b 7 #&gt; 6 6 b 8 #&gt; # ... with 3 more rows # The offset functions lead() and lag() respect the groupings in group_by(). The functions lag() and lead() will only return values within each group. df %&gt;% mutate(lag_x = lag(x), lead_x = lead(x)) #&gt; # A tibble: 9 x 4 #&gt; x group lag_x lead_x #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 a NA 2 #&gt; 2 2 a 1 3 #&gt; 3 3 a 2 4 #&gt; 4 4 b 3 5 #&gt; 5 5 b 4 6 #&gt; 6 6 b 5 7 #&gt; # ... with 3 more rows gf %&gt;% mutate(lag_x = lag(x), lead_x = lead(x)) #&gt; # A tibble: 9 x 4 #&gt; # Groups: group [3] #&gt; x group lag_x lead_x #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 a NA 2 #&gt; 2 2 a 1 3 #&gt; 3 3 a 2 NA #&gt; 4 4 b NA 5 #&gt; 5 5 b 4 6 #&gt; 6 6 b 5 NA #&gt; # ... with 3 more rows # The cumulative and rolling aggregate functions cumsum(), cumprod(), cummin(), cummax(), and cummean() calculate values within each group. df %&gt;% mutate(cumsum(x)) #&gt; # A tibble: 9 x 3 #&gt; x group `cumsum(x)` #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 1 a 1 #&gt; 2 2 a 3 #&gt; 3 3 a 6 #&gt; 4 4 b 10 #&gt; 5 5 b 15 #&gt; 6 6 b 21 #&gt; # ... with 3 more rows gf %&gt;% mutate(cumsum(x)) #&gt; # A tibble: 9 x 3 #&gt; # Groups: group [3] #&gt; x group `cumsum(x)` #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 1 a 1 #&gt; 2 2 a 3 #&gt; 3 3 a 6 #&gt; 4 4 b 4 #&gt; 5 5 b 9 #&gt; 6 6 b 15 #&gt; # ... with 3 more rows # Logical comparisons, &lt;, &lt;=, &gt;, &gt;=, !=, and == are not affected by group_by(). df %&gt;% mutate(x &gt; 0.5) #&gt; # A tibble: 9 x 3 #&gt; x group `x &gt; 0.5` #&gt; &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; #&gt; 1 1 a TRUE #&gt; 2 2 a TRUE #&gt; 3 3 a TRUE #&gt; 4 4 b TRUE #&gt; 5 5 b TRUE #&gt; 6 6 b TRUE #&gt; # ... with 3 more rows gf %&gt;% mutate(x &gt; 0.5) #&gt; # A tibble: 9 x 3 #&gt; # Groups: group [3] #&gt; x group `x &gt; 0.5` #&gt; &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; #&gt; 1 1 a TRUE #&gt; 2 2 a TRUE #&gt; 3 3 a TRUE #&gt; 4 4 b TRUE #&gt; 5 5 b TRUE #&gt; 6 6 b TRUE #&gt; # ... with 3 more rows # Ranking functions like min_rank() work within each group when used with group_by(). df %&gt;% mutate(min_rank(x)) #&gt; # A tibble: 9 x 3 #&gt; x group `min_rank(x)` #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 1 a 1 #&gt; 2 2 a 2 #&gt; 3 3 a 3 #&gt; 4 4 b 4 #&gt; 5 5 b 5 #&gt; 6 6 b 6 #&gt; # ... with 3 more rows gf %&gt;% mutate(min_rank(x)) #&gt; # A tibble: 9 x 3 #&gt; # Groups: group [3] #&gt; x group `min_rank(x)` #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 1 a 1 #&gt; 2 2 a 2 #&gt; 3 3 a 3 #&gt; 4 4 b 1 #&gt; 5 5 b 2 #&gt; 6 6 b 3 #&gt; # ... with 3 more rows # filter works the similar way df %&gt;% filter(min_rank(x) == 1) #&gt; # A tibble: 1 x 2 #&gt; x group #&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 1 a gf %&gt;% filter(min_rank(x) == 1) #&gt; # A tibble: 3 x 2 #&gt; # Groups: group [3] #&gt; x group #&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 1 a #&gt; 2 4 b #&gt; 3 7 c # arrange() ignores groups when sorting values. df &lt;- tibble( x = runif(9), group = rep(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), each = 3) ) df %&gt;% group_by(group) %&gt;% arrange(x) #&gt; # A tibble: 9 x 2 #&gt; # Groups: group [3] #&gt; x group #&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 0.0562 c #&gt; 2 0.337 a #&gt; 3 0.514 b #&gt; 4 0.523 b #&gt; 5 0.525 a #&gt; 6 0.557 b #&gt; # ... with 3 more rows # .by_group = TRUE df %&gt;% group_by(group) %&gt;% arrange(x, .by_group = TRUE) #&gt; # A tibble: 9 x 2 #&gt; # Groups: group [3] #&gt; x group #&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 0.337 a #&gt; 2 0.525 a #&gt; 3 0.785 a #&gt; 4 0.514 b #&gt; 5 0.523 b #&gt; 6 0.557 b #&gt; # ... with 3 more rows "],
["dplyr-exercise.html", "1.6 Exercises", " 1.6 Exercises Exercise 1.10 找到每个日期分组中到达时间延迟最长的10条记录 # min_rank自动在分组内排序 not_cancelled %&gt;% group_by(year,month,day) %&gt;% filter(rank(desc(arr_delay)) &lt;= 10) %&gt;% select(month,year,day,arr_delay) #&gt; # A tibble: 3,609 x 4 #&gt; # Groups: year, month, day [365] #&gt; month year day arr_delay #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 2013 1 851 #&gt; 2 1 2013 1 338 #&gt; 3 1 2013 1 263 #&gt; 4 1 2013 1 166 #&gt; 5 1 2013 1 174 #&gt; 6 1 2013 1 222 #&gt; # ... with 3,603 more rows Exercise 1.11 找出一年中到达航班多于 365 次的目的地： not_cancelled %&gt;% count(dest, sort = T) %&gt;% filter(n &gt; 365) #&gt; # A tibble: 75 x 2 #&gt; dest n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 ATL 16837 #&gt; 2 ORD 16566 #&gt; 3 LAX 16026 #&gt; 4 BOS 15022 #&gt; 5 MCO 13967 #&gt; 6 CLT 13674 #&gt; # ... with 69 more rows # another solution not_cancelled %&gt;% group_by(dest) %&gt;% filter(n() &gt; 365) %&gt;% distinct(dest) #&gt; # A tibble: 75 x 1 #&gt; # Groups: dest [75] #&gt; dest #&gt; &lt;chr&gt; #&gt; 1 IAH #&gt; 2 MIA #&gt; 3 BQN #&gt; 4 ATL #&gt; 5 ORD #&gt; 6 FLL #&gt; # ... with 69 more rows Exercise 1.12 哪一架飞机(tailnum)具有最差的准点记录： 衡量一个飞机的准点情况有很多种可能的选择，这里只提供两个方向： 该航班未取消且延误（到达和出发）次数占总飞行次数的比例最小 该航班的平均到达延误时间最长 从第一个方向出发： ## 查看飞机飞行次数的分布 count_by_tail &lt;- flights %&gt;% group_by(tailnum) %&gt;% summarize(n = n()) quantile(count_by_tail$n) #&gt; 0% 25% 50% 75% 100% #&gt; 1 23 54 110 2512 flights %&gt;% filter(!is.na(tailnum)) %&gt;% mutate(on_time = !is.na(arr_time) &amp; (arr_delay &lt;= 0) &amp; (dep_delay &lt;= 0) ) %&gt;% group_by(tailnum) %&gt;% summarize(on_time = mean(on_time), n = n()) %&gt;% ## mean和逻辑值结合计算比例 filter(n &gt; 20) %&gt;% ## 避免因频次过少做出结论,选择20是因为它是飞行次数的下四分位数 filter(min_rank(on_time) == 1) ## 未取消且准点比例最低的飞机 #&gt; # A tibble: 1 x 3 #&gt; tailnum on_time n #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 N988AT 0.0541 37 接下来是第二个方向： flights %&gt;% group_by(tailnum) %&gt;% summarize(arr_delay = mean(arr_delay,na.rm = T),n = n()) %&gt;% ##注意这一步的mean中的参数 filter(n &gt; 20) %&gt;% filter(min_rank(desc(arr_delay)) == 1) #&gt; # A tibble: 1 x 3 #&gt; tailnum arr_delay n #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 N203FR 59.1 41 Exercise 1.13 如果想要尽量避免航班延误，应该在一天中的哪个时间搭乘飞机？ 对 hour 分组计算平均到达延误时间： flights %&gt;% group_by(hour) %&gt;% summarize(arr_delay = mean(arr_delay, na.rm = T)) %&gt;% arrange(arr_delay) #&gt; # A tibble: 20 x 2 #&gt; hour arr_delay #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 7 -5.30 #&gt; 2 5 -4.80 #&gt; 3 6 -3.38 #&gt; 4 9 -1.45 #&gt; 5 8 -1.11 #&gt; 6 10 0.954 #&gt; # ... with 14 more rows Exercise 1.14 计算每个目的地的到达延误总时间的分钟数，以及每条记录到每个目的地的延误时间比例 flights %&gt;% filter(arr_delay &gt; 0) %&gt;% group_by(dest) %&gt;% mutate( arr_delay_total = sum(arr_delay), ## 经过分组后，求和在组内操作 arr_delay_prop = arr_delay / arr_delay_total ) %&gt;% select( dest, tailnum, arr_delay, arr_delay_prop ) %&gt;% arrange(dest, desc(arr_delay_prop)) #&gt; # A tibble: 133,004 x 4 #&gt; # Groups: dest [103] #&gt; dest tailnum arr_delay arr_delay_prop #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 ABQ N784JB 153 0.0341 #&gt; 2 ABQ N659JB 149 0.0332 #&gt; 3 ABQ N640JB 138 0.0308 #&gt; 4 ABQ N589JB 137 0.0305 #&gt; 5 ABQ N556JB 136 0.0303 #&gt; 6 ABQ N598JB 126 0.0281 #&gt; # ... with 132,998 more rows Exercise 1.15 延误通常是由临时原因造成的：即使最初引起延误的问题已经解决，但因为要让前面的航班先起飞，所以后面的航班也会延误。使用lag() 探究一架航班延误与前一架航班延误之间的关系。 ## This calculates the departure delay of the preceding flight from the same airport. (lagged_delays &lt;- flights %&gt;% arrange(origin, month, day, dep_time) %&gt;% ## 这一步排序确保了偏移是有意义的 group_by(origin) %&gt;% mutate(dep_delay_lag = lag(dep_delay)) %&gt;% ## 偏移函数基于分组 filter(!is.na(dep_delay),!is.na(dep_delay_lag))) #&gt; # A tibble: 327,649 x 20 #&gt; # Groups: origin [3] #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 554 558 -4 740 728 #&gt; 2 2013 1 1 555 600 -5 913 854 #&gt; 3 2013 1 1 558 600 -2 923 937 #&gt; 4 2013 1 1 559 600 -1 854 902 #&gt; 5 2013 1 1 601 600 1 844 850 #&gt; 6 2013 1 1 606 610 -4 858 910 #&gt; # ... with 327,643 more rows, and 12 more variables: arr_delay &lt;dbl&gt;, #&gt; # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, #&gt; # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, #&gt; # dep_delay_lag &lt;dbl&gt; lagged_delays %&gt;% group_by(dep_delay_lag) %&gt;% summarise(dep_delay_mean = mean(dep_delay)) %&gt;% ggplot(aes(y = dep_delay_mean, x = dep_delay_lag)) + geom_point() + scale_x_continuous(breaks = seq(0, 1500, by = 120)) + labs(y = &quot;Departure Delay&quot;, x = &quot;Previous Departure Delay&quot;) + theme_classic() ## This plots the relationship between the mean delay of a flight for all values of the previous flight. For delays less than two hours, the relationship between the delay of the preceding flight and the current flight is nearly a line. After that the relationship becomes more variable, as long-delayed flights are interspersed with flights leaving on-time. After about 8-hours, a delayed flight is likely to be followed by a flight leaving on time. Exercise 1.16 根据到达地点的数量，对航空公司进行排序 ; 找出至少有两个航空公司的目的地 ## rank carriers by numer of destinations flights %&gt;% group_by(carrier) %&gt;% summarize(n_dest = n_distinct(dest)) %&gt;% arrange(desc(n_dest)) #&gt; # A tibble: 16 x 2 #&gt; carrier n_dest #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 EV 61 #&gt; 2 9E 49 #&gt; 3 UA 47 #&gt; 4 B6 42 #&gt; 5 DL 40 #&gt; 6 MQ 20 #&gt; # ... with 10 more rows ## find all airports with &gt; 1 carrier flights %&gt;% group_by(dest) %&gt;% summarize(n_carriers = n_distinct(carrier)) %&gt;% filter(n_carriers &gt; 1) #&gt; # A tibble: 76 x 2 #&gt; dest n_carriers #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 ATL 7 #&gt; 2 AUS 6 #&gt; 3 AVL 2 #&gt; 4 BDL 2 #&gt; 5 BGR 2 #&gt; 6 BNA 5 #&gt; # ... with 70 more rows Exercise 1.17 每天取消的航班数量和总航班数量存在什么关系？每天的平均到达延误时间和取消航班的比例有什么关系？ 取消的航班定义为is.na(arr_delay) | is.na(dep_delay) ## One pattern in cancelled flights per day is that ## the number of cancelled flights increases with the total number of flights per day. flights %&gt;% group_by(year,month,day) %&gt;% summarize(n_cancelled = sum(is.na(arr_delay) | is.na(dep_delay)),n_total = n()) %&gt;% ggplot(aes(x = n_total,y = n_cancelled)) + geom_point() #2 flights %&gt;% group_by(year,month,day) %&gt;% summarize(cancelled_prop = mean(is.na(arr_delay) | is.na(dep_delay)), avg_arr_delay = mean(arr_delay,na.rm = T)) %&gt;% ggplot() + geom_point(aes(x = avg_arr_delay,y = cancelled_prop)) Exercise 1.18 哪个航空公司的延误情况最严重？你能否分清这是因为糟糕的机场设备，还是航空公司的问题？（考虑一下flights %&gt;% group_by(carrier,dest) %&gt;% summarize(n()) flights %&gt;% group_by(carrier) %&gt;% summarize(avg_arr_delay = mean(arr_delay,na.rm = T)) %&gt;% arrange(desc(avg_arr_delay)) #&gt; # A tibble: 16 x 2 #&gt; carrier avg_arr_delay #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 F9 21.9 #&gt; 2 FL 20.1 #&gt; 3 EV 15.8 #&gt; 4 YV 15.6 #&gt; 5 OO 11.9 #&gt; 6 MQ 10.8 #&gt; # ... with 10 more rows ## What airline corresponds to the &quot;F9&quot; carrier code? filter(airlines,carrier == &quot;F9&quot;) #&gt; # A tibble: 1 x 2 #&gt; carrier name #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 F9 Frontier Airlines Inc. You can get part of the way to disentangling the effects of airports versus bad carriers by comparing the average delay of each carrier to the average delay of flights within a route (flights from the same origin to the same destination). Comparing delays between carriers and within each route disentangles the effect of carriers and airports. A better analysis would compare the average delay of a carrier’s flights to the average delay of all other carrier’s flights within a route. flights %&gt;% filter(!is.na(arr_delay)) %&gt;% # Total delay by carrier within each origin, dest group_by(origin, dest, carrier) %&gt;% summarise( arr_delay = sum(arr_delay), flights = n() ) %&gt;% # Total delay within each origin dest group_by(origin, dest) %&gt;% mutate( arr_delay_total = sum(arr_delay), flights_total = sum(flights) ) %&gt;% # average delay of each carrier - average delay of other carriers ungroup() %&gt;% mutate( arr_delay_others = (arr_delay_total - arr_delay) / (flights_total - flights), arr_delay_mean = arr_delay / flights, arr_delay_diff = arr_delay_mean - arr_delay_others ) %&gt;% # remove NaN values (when there is only one carrier) filter(is.finite(arr_delay_diff)) %&gt;% # average over all airports it flies to group_by(carrier) %&gt;% summarise(arr_delay_diff = mean(arr_delay_diff)) %&gt;% arrange(desc(arr_delay_diff)) #&gt; # A tibble: 15 x 2 #&gt; carrier arr_delay_diff #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 OO 27.3 #&gt; 2 F9 17.3 #&gt; 3 EV 11.0 #&gt; 4 B6 6.41 #&gt; 5 FL 2.57 #&gt; 6 VX -0.202 #&gt; # ... with 9 more rows "],
["tibble-modern-data-frames.html", "2 tibble: Modern data frames", " 2 tibble: Modern data frames Tibbles are a modern reimagining of the trandition data.frame, keeping what time has shown to be effective, and throwing out what is not (stringasFactors = TRUE), with nicer display when obseravtions exceeds 20 rows. The usage of tibble also underlyies one of the principles of the whole tidyverse universe. In most places, I’ll use the term tibble and data frame interchangeably; when I want to draw particular attention to R’s built-in data frame, I’ll call them data.frames. "],
["tibble-intro.html", "2.1 Introduction", " 2.1 Introduction Almost all of the functions that you’ll use in the tidyverse produce tibbles. To coerce a data.frame into tibble, we can use as_tibble(): class(iris) #&gt; [1] &quot;data.frame&quot; ## 转换为 tibble iris_tibble &lt;- as_tibble(iris) iris_tibble #&gt; # A tibble: 150 x 5 #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa #&gt; # ... with 144 more rows 相比于 data.frame() ，tibble() 创建数据矿的附加操作要少的多：它不能改变输入的类型（例如，不会默认将字符串转变为因子)、变量的名称,也不能创建行名称。 有些比较旧的函数不支持 tibble，如果遇到这种函数，可以使用 as_data_frame() 转换到传统的 data.frame 上。 add_row(.data, ..., .before = NULL, .after = NULL) s a convenient way to add one or more rows of data to an existing data frame. ... should be name-value pairs corresponding to column names and its value. By default new row are added after the last row. # add_row --------------------------------- df &lt;- tibble(x = 1:3, y = 3:1) add_row(df, x = 4, y = 0) #&gt; # A tibble: 4 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 3 #&gt; 2 2 2 #&gt; 3 3 1 #&gt; 4 4 0 # You can specify where to add the new rows add_row(df, x = 4, y = 0, .before = 2) #&gt; # A tibble: 4 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 3 #&gt; 2 4 0 #&gt; 3 2 2 #&gt; 4 3 1 # You can supply vectors, to add multiple rows (this isn&#39;t # recommended because it&#39;s a bit hard to read) add_row(df, x = 4:5, y = 0:-1) #&gt; # A tibble: 5 x 2 #&gt; x y #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 3 #&gt; 2 2 2 #&gt; 3 3 1 #&gt; 4 4 0 #&gt; 5 5 -1 # Absent variables get missing values add_row(df, x = 4) #&gt; # A tibble: 4 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1 3 #&gt; 2 2 2 #&gt; 3 3 1 #&gt; 4 4 NA Yet we cannot create new variables with add_row: add_row(df, z = 0) #&gt; Error: New rows can&#39;t add columns. #&gt; x Can&#39;t find column `z` in `.data`. enframe() converts named atomic vectors or lists to one- or two-column data frames . For vectors with more than 2 elements, a list column is created. enframe(c(a = 1, b = 2)) #&gt; # A tibble: 2 x 2 #&gt; name value #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 a 1 #&gt; 2 b 2 # For unnamed vectors, the natural sequence is used as name column. enframe(1:2) #&gt; # A tibble: 2 x 2 #&gt; name value #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 1 #&gt; 2 2 2 # vectors with more than 2 elements enframe(1:3) #&gt; # A tibble: 3 x 2 #&gt; name value #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 1 #&gt; 2 2 2 #&gt; 3 3 3 For a list, the result will also be a list column. enframe(list(one = 1, two = 2:3, three = 4:6, four = &quot;four&quot;)) #&gt; # A tibble: 4 x 2 #&gt; name value #&gt; &lt;chr&gt; &lt;list&gt; #&gt; 1 one &lt;dbl [1]&gt; #&gt; 2 two &lt;int [2]&gt; #&gt; 3 three &lt;int [3]&gt; #&gt; 4 four &lt;chr [1]&gt; deframe() is the opposite of enframe() list(one = 1, two = 2:3, three = 4:6, four = &quot;four&quot;) %&gt;% enframe() %&gt;% deframe() #&gt; $one #&gt; [1] 1 #&gt; #&gt; $two #&gt; [1] 2 3 #&gt; #&gt; $three #&gt; [1] 4 5 6 #&gt; #&gt; $four #&gt; [1] &quot;four&quot; "],
["tibble-data-frame.html", "2.2 Comparing tibble and data.frame", " 2.2 Comparing tibble and data.frame tibble 和传统 data.frame 的机理主要有三处不同：创建、打印和取子集。 2.2.1 Creating tibble() 创建一个 tibble df &lt;- tibble( x = 1:3, y = c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;) ) df #&gt; # A tibble: 3 x 2 #&gt; x y #&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 1 a #&gt; 2 2 b #&gt; 3 3 c 可以发现 tibble() 不会默认更改 y 类型，它将原原本本地被当做一个字符向量处理。 而 data.frame() 用于创建一个传统数据框： df &lt;- data.frame( x = 1:3, y = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) ) str(df) #&gt; &#39;data.frame&#39;: 3 obs. of 2 variables: #&gt; $ x: int 1 2 3 #&gt; $ y: Factor w/ 3 levels &quot;a&quot;,&quot;b&quot;,&quot;c&quot;: 1 2 3 2.2.1.1 Coersion 在 data.frame 中，为了防止 y 被强制转换为因子，必须设置 stringAsFactors = FALSE df &lt;- data.frame( x = 1:3, y = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), stringsAsFactors = F ) str(df) #&gt; &#39;data.frame&#39;: 3 obs. of 2 variables: #&gt; $ x: int 1 2 3 #&gt; $ y: chr &quot;a&quot; &quot;b&quot; &quot;c&quot; By the way, 创建 tibble 的另一种方法是使用tribble()，transposed tibble 的缩写。tribble() 是高度定制化的，就像在 Markdown 中创建表格一样，一个一个填入元素：列标题以波浪线 ~ 开头，不同列的元素之间以逗号分隔，这样就可以用易读的方式对少量数据创建一个 tibble ： tribble( ~x, ~y, ~z, &quot;a&quot;, 1, 2, &quot;b&quot;, 1, 8.5 ) #&gt; # A tibble: 2 x 3 #&gt; x y z #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 a 1 2 #&gt; 2 b 1 8.5 2.2.1.2 Row labels data.frame 支持提供一个不包含重复元素的字符向量作为行标签： df &lt;- data.frame( age = c(35, 27, 18), hair = c(&quot;blond&quot;, &quot;brown&quot;, &quot;black&quot;), row.names = c(&quot;Bob&quot;, &quot;Susan&quot;, &quot;Sam&quot;) ) df #&gt; age hair #&gt; Bob 35 blond #&gt; Susan 27 brown #&gt; Sam 18 black attributes(df) #&gt; $names #&gt; [1] &quot;age&quot; &quot;hair&quot; #&gt; #&gt; $class #&gt; [1] &quot;data.frame&quot; #&gt; #&gt; $row.names #&gt; [1] &quot;Bob&quot; &quot;Susan&quot; &quot;Sam&quot; row.names参数为data.frame创建了一个同名的属性，rownames()函数可以提取这个属性： rownames(df) #&gt; [1] &quot;Bob&quot; &quot;Susan&quot; &quot;Sam&quot; 如果我们认为数据框是二维矩阵的自然拓展(不止包含数值)，那么行标签的存在似乎是很自然的，毕竟矩阵有i、j两个索引。但是矩阵和数据框有本质区别，矩阵是添加了维度属性的原子向量，数据框则是列表，我们可以对矩阵取转置，因为它们的行和列可以互换，一个矩阵的转置是另一个矩阵。数据框则是不可转置的，它的行列互换之后就不再是数据框(一行未必能构成一个原子向量)。 出于以下三个原因，我们不应该使用row.names这一属性，也不应该考虑在任何场合添加行标签： 元数据也是数据，所以把行标签从其他变量中抽出来单独对待不是个好主意。否则我们可能要对行标签单独发展出一套操作工具，而不能直接应用我们已经习惯的对变量的操作方法 行标签很多时候不能完成唯一标识观测的任务，因为 row.names 要求只能传入数值或者字符串向量。如果我们想要用时间日期型数据标识观测呢？或者需要传入不止一个向量(例如标识位置同时需要经度和纬度) 行标签中的元素必须是唯一的，但很多场合(比如bootstrapping)中同一个对象也可能有多条记录 所以，tibble 的设计思想就是不支持添加行标签，且提供了一套很方便的、处理已有行标签的工具，要么移除它，要么把它直接变成tibble中的一列： Tools for working with row names Description While a tibble can have row names (e.g., when converting from a regular data frame), they are removed when subsetting with the [ operator. A warning will be raised when attempting to assign non-NULL row names to a tibble. Generally, it is best to avoid row names, because they are basically a character column with different semantics to every other column. These functions allow to you detect if a data frame has row names (has_rownames()), remove them (remove_rownames()), or convert them back-and-forth between an explicit column (rownames_to_column() and column_to_rownames()). Also included is rowid_to_column() which adds a column at the start of the dataframe of ascending sequential row ids starting at 1. Note that this will remove any existing row names. Usage has_rownames(.data) remove_rownames(.data) rownames_to_column(.data, var = \"rowname\") rowid_to_column(.data, var = \"rowid\") column_to_rownames(.data, var = \"rowname\") Arguments .data A data frame. var Name of column to use for rownames. Value column_to_rownames() always returns a data frame. has_rownames() returns a scalar logical. All other functions return an object of the same class as the input. 一些示例： mtcars %&gt;% has_rownames() #&gt; [1] TRUE mtcars %&gt;% remove_rownames() #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; 1 21.0 6 160.0 110 3.90 2.62 16.5 0 1 4 4 #&gt; 2 21.0 6 160.0 110 3.90 2.88 17.0 0 1 4 4 #&gt; 3 22.8 4 108.0 93 3.85 2.32 18.6 1 1 4 1 #&gt; 4 21.4 6 258.0 110 3.08 3.21 19.4 1 0 3 1 #&gt; 5 18.7 8 360.0 175 3.15 3.44 17.0 0 0 3 2 #&gt; 6 18.1 6 225.0 105 2.76 3.46 20.2 1 0 3 1 #&gt; 7 14.3 8 360.0 245 3.21 3.57 15.8 0 0 3 4 #&gt; 8 24.4 4 146.7 62 3.69 3.19 20.0 1 0 4 2 #&gt; 9 22.8 4 140.8 95 3.92 3.15 22.9 1 0 4 2 #&gt; 10 19.2 6 167.6 123 3.92 3.44 18.3 1 0 4 4 #&gt; 11 17.8 6 167.6 123 3.92 3.44 18.9 1 0 4 4 #&gt; 12 16.4 8 275.8 180 3.07 4.07 17.4 0 0 3 3 #&gt; 13 17.3 8 275.8 180 3.07 3.73 17.6 0 0 3 3 #&gt; 14 15.2 8 275.8 180 3.07 3.78 18.0 0 0 3 3 #&gt; 15 10.4 8 472.0 205 2.93 5.25 18.0 0 0 3 4 #&gt; 16 10.4 8 460.0 215 3.00 5.42 17.8 0 0 3 4 #&gt; 17 14.7 8 440.0 230 3.23 5.34 17.4 0 0 3 4 #&gt; 18 32.4 4 78.7 66 4.08 2.20 19.5 1 1 4 1 #&gt; 19 30.4 4 75.7 52 4.93 1.61 18.5 1 1 4 2 #&gt; 20 33.9 4 71.1 65 4.22 1.83 19.9 1 1 4 1 #&gt; 21 21.5 4 120.1 97 3.70 2.46 20.0 1 0 3 1 #&gt; 22 15.5 8 318.0 150 2.76 3.52 16.9 0 0 3 2 #&gt; 23 15.2 8 304.0 150 3.15 3.44 17.3 0 0 3 2 #&gt; 24 13.3 8 350.0 245 3.73 3.84 15.4 0 0 3 4 #&gt; 25 19.2 8 400.0 175 3.08 3.85 17.1 0 0 3 2 #&gt; 26 27.3 4 79.0 66 4.08 1.94 18.9 1 1 4 1 #&gt; 27 26.0 4 120.3 91 4.43 2.14 16.7 0 1 5 2 #&gt; 28 30.4 4 95.1 113 3.77 1.51 16.9 1 1 5 2 #&gt; 29 15.8 8 351.0 264 4.22 3.17 14.5 0 1 5 4 #&gt; 30 19.7 6 145.0 175 3.62 2.77 15.5 0 1 5 6 #&gt; 31 15.0 8 301.0 335 3.54 3.57 14.6 0 1 5 8 #&gt; 32 21.4 4 121.0 109 4.11 2.78 18.6 1 1 4 2 mtcars %&gt;% rownames_to_column(var = &quot;car_type&quot;) #&gt; car_type mpg cyl disp hp drat wt qsec vs am gear carb #&gt; 1 Mazda RX4 21.0 6 160.0 110 3.90 2.62 16.5 0 1 4 4 #&gt; 2 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.88 17.0 0 1 4 4 #&gt; 3 Datsun 710 22.8 4 108.0 93 3.85 2.32 18.6 1 1 4 1 #&gt; 4 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.21 19.4 1 0 3 1 #&gt; 5 Hornet Sportabout 18.7 8 360.0 175 3.15 3.44 17.0 0 0 3 2 #&gt; 6 Valiant 18.1 6 225.0 105 2.76 3.46 20.2 1 0 3 1 #&gt; 7 Duster 360 14.3 8 360.0 245 3.21 3.57 15.8 0 0 3 4 #&gt; 8 Merc 240D 24.4 4 146.7 62 3.69 3.19 20.0 1 0 4 2 #&gt; 9 Merc 230 22.8 4 140.8 95 3.92 3.15 22.9 1 0 4 2 #&gt; 10 Merc 280 19.2 6 167.6 123 3.92 3.44 18.3 1 0 4 4 #&gt; 11 Merc 280C 17.8 6 167.6 123 3.92 3.44 18.9 1 0 4 4 #&gt; 12 Merc 450SE 16.4 8 275.8 180 3.07 4.07 17.4 0 0 3 3 #&gt; 13 Merc 450SL 17.3 8 275.8 180 3.07 3.73 17.6 0 0 3 3 #&gt; 14 Merc 450SLC 15.2 8 275.8 180 3.07 3.78 18.0 0 0 3 3 #&gt; 15 Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.25 18.0 0 0 3 4 #&gt; 16 Lincoln Continental 10.4 8 460.0 215 3.00 5.42 17.8 0 0 3 4 #&gt; 17 Chrysler Imperial 14.7 8 440.0 230 3.23 5.34 17.4 0 0 3 4 #&gt; 18 Fiat 128 32.4 4 78.7 66 4.08 2.20 19.5 1 1 4 1 #&gt; 19 Honda Civic 30.4 4 75.7 52 4.93 1.61 18.5 1 1 4 2 #&gt; 20 Toyota Corolla 33.9 4 71.1 65 4.22 1.83 19.9 1 1 4 1 #&gt; 21 Toyota Corona 21.5 4 120.1 97 3.70 2.46 20.0 1 0 3 1 #&gt; 22 Dodge Challenger 15.5 8 318.0 150 2.76 3.52 16.9 0 0 3 2 #&gt; 23 AMC Javelin 15.2 8 304.0 150 3.15 3.44 17.3 0 0 3 2 #&gt; 24 Camaro Z28 13.3 8 350.0 245 3.73 3.84 15.4 0 0 3 4 #&gt; 25 Pontiac Firebird 19.2 8 400.0 175 3.08 3.85 17.1 0 0 3 2 #&gt; 26 Fiat X1-9 27.3 4 79.0 66 4.08 1.94 18.9 1 1 4 1 #&gt; 27 Porsche 914-2 26.0 4 120.3 91 4.43 2.14 16.7 0 1 5 2 #&gt; 28 Lotus Europa 30.4 4 95.1 113 3.77 1.51 16.9 1 1 5 2 #&gt; 29 Ford Pantera L 15.8 8 351.0 264 4.22 3.17 14.5 0 1 5 4 #&gt; 30 Ferrari Dino 19.7 6 145.0 175 3.62 2.77 15.5 0 1 5 6 #&gt; 31 Maserati Bora 15.0 8 301.0 335 3.54 3.57 14.6 0 1 5 8 #&gt; 32 Volvo 142E 21.4 4 121.0 109 4.11 2.78 18.6 1 1 4 2 mtcars %&gt;% rowid_to_column() #&gt; rowid mpg cyl disp hp drat wt qsec vs am gear carb #&gt; 1 1 21.0 6 160.0 110 3.90 2.62 16.5 0 1 4 4 #&gt; 2 2 21.0 6 160.0 110 3.90 2.88 17.0 0 1 4 4 #&gt; 3 3 22.8 4 108.0 93 3.85 2.32 18.6 1 1 4 1 #&gt; 4 4 21.4 6 258.0 110 3.08 3.21 19.4 1 0 3 1 #&gt; 5 5 18.7 8 360.0 175 3.15 3.44 17.0 0 0 3 2 #&gt; 6 6 18.1 6 225.0 105 2.76 3.46 20.2 1 0 3 1 #&gt; 7 7 14.3 8 360.0 245 3.21 3.57 15.8 0 0 3 4 #&gt; 8 8 24.4 4 146.7 62 3.69 3.19 20.0 1 0 4 2 #&gt; 9 9 22.8 4 140.8 95 3.92 3.15 22.9 1 0 4 2 #&gt; 10 10 19.2 6 167.6 123 3.92 3.44 18.3 1 0 4 4 #&gt; 11 11 17.8 6 167.6 123 3.92 3.44 18.9 1 0 4 4 #&gt; 12 12 16.4 8 275.8 180 3.07 4.07 17.4 0 0 3 3 #&gt; 13 13 17.3 8 275.8 180 3.07 3.73 17.6 0 0 3 3 #&gt; 14 14 15.2 8 275.8 180 3.07 3.78 18.0 0 0 3 3 #&gt; 15 15 10.4 8 472.0 205 2.93 5.25 18.0 0 0 3 4 #&gt; 16 16 10.4 8 460.0 215 3.00 5.42 17.8 0 0 3 4 #&gt; 17 17 14.7 8 440.0 230 3.23 5.34 17.4 0 0 3 4 #&gt; 18 18 32.4 4 78.7 66 4.08 2.20 19.5 1 1 4 1 #&gt; 19 19 30.4 4 75.7 52 4.93 1.61 18.5 1 1 4 2 #&gt; 20 20 33.9 4 71.1 65 4.22 1.83 19.9 1 1 4 1 #&gt; 21 21 21.5 4 120.1 97 3.70 2.46 20.0 1 0 3 1 #&gt; 22 22 15.5 8 318.0 150 2.76 3.52 16.9 0 0 3 2 #&gt; 23 23 15.2 8 304.0 150 3.15 3.44 17.3 0 0 3 2 #&gt; 24 24 13.3 8 350.0 245 3.73 3.84 15.4 0 0 3 4 #&gt; 25 25 19.2 8 400.0 175 3.08 3.85 17.1 0 0 3 2 #&gt; 26 26 27.3 4 79.0 66 4.08 1.94 18.9 1 1 4 1 #&gt; 27 27 26.0 4 120.3 91 4.43 2.14 16.7 0 1 5 2 #&gt; 28 28 30.4 4 95.1 113 3.77 1.51 16.9 1 1 5 2 #&gt; 29 29 15.8 8 351.0 264 4.22 3.17 14.5 0 1 5 4 #&gt; 30 30 19.7 6 145.0 175 3.62 2.77 15.5 0 1 5 6 #&gt; 31 31 15.0 8 301.0 335 3.54 3.57 14.6 0 1 5 8 #&gt; 32 32 21.4 4 121.0 109 4.11 2.78 18.6 1 1 4 2 2.2.1.3 Recycling tibble()会循环使用那些长度为1的列，将其自动扩展到最长的列的长度。而长度不为1，且和其他列元素个数不同的列不会被循环 ； data.frame()会自动循环长度可被最长一列的长度整除的列： tibble(x = 1:4, y = 1) #&gt; # A tibble: 4 x 2 #&gt; x y #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 1 #&gt; 2 2 1 #&gt; 3 3 1 #&gt; 4 4 1 tibble(x = 1:4, y = 1:2) #&gt; Error: Tibble columns must have compatible sizes. #&gt; * Size 4: Existing data. #&gt; * Size 2: Column `y`. #&gt; i Only values of size one are recycled. data.frame(x = 1:4, y = 1:2) #&gt; x y #&gt; 1 1 1 #&gt; 2 2 2 #&gt; 3 3 1 #&gt; 4 4 2 data.frame(x = 1:4, y = 1:3) #&gt; Error in data.frame(x = 1:4, y = 1:3): arguments imply differing number of rows: 4, 3 2.2.1.4 Invalid column names tibble的一个很大特色是可以使用在 R 中无效的变量名称，即不符合变量命名规定的名称可以在 tibble 中成为列名，实际上这个规则约束了 R 中所有“名称”的设定。R 规定名称只能包含字母、数字、点.和下划线_，必须以字母开头，数字不能跟在.之后，也不能和R中保留的关键字重名(see ?Reserved)。 如果想创建不合法的列名，可以用反引号```将它们括起来： tb &lt;- tibble( `:)` = &quot;smile&quot;, ` ` = &quot;space&quot;, `2000` = &quot;number&quot; ) tb #&gt; # A tibble: 1 x 3 #&gt; `:)` ` ` `2000` #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 smile space number 类似地，如果想要在 ggolot2 或者其他tidyverse包中使用这些名称特殊的变量，也需要用反引号括起来。 相比之下，data.frame()会依据make.names()的规则自行更改无效的列名称(除非设置check.names = FALSE)。如果你的名称不以字母开头(字母的界定依据当前电脑的地域设置，但不能超越ASCII字符集)，这个函数会地添加X作为前缀；如果包含特殊字符，用.代替；未给出的名称用NA代替；与R的保留关键字重名的，在后面添加.: names(data.frame(`1` = 1)) #&gt; [1] &quot;X1&quot; names(data.frame(`1` = 1,check.names = F)) #&gt; [1] &quot;1&quot; 2.2.1.5 Referencing a column 最后，我们可以在创建 tibble 创建过程中就引用其中的变量，因为变量在 tibble 中是被从左到右依次添加的(而 data.frame() 不支持这一点)： tibble( x = 1:3, y = x * 2 ) #&gt; # A tibble: 3 x 2 #&gt; x y #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 2 #&gt; 2 2 4 #&gt; 3 3 6 2.2.2 Printing tibble 的打印方法进行了优化，只显示前 10 行结果，显示列的数目将自动适应屏幕的宽度，这种打印方式非常适合大数据集。除了打印列名，tibble 还会第一行的下面打印出列的类型，这项功能有些类似于 str() 函数 tibble( a = lubridate::now() + runif(1e3) * 96400, b = lubridate::today() + runif(1e3) * 30, c = 1:1e3, d = runif(1e3), e = sample(letters,1e3, replace = T) ) #&gt; # A tibble: 1,000 x 5 #&gt; a b c d e #&gt; &lt;dttm&gt; &lt;date&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 2020-04-30 12:00:25 2020-05-01 1 0.392 i #&gt; 2 2020-04-30 05:03:59 2020-05-03 2 0.409 p #&gt; 3 2020-04-30 00:00:33 2020-05-14 3 0.944 t #&gt; 4 2020-04-30 04:45:02 2020-05-23 4 0.614 z #&gt; 5 2020-04-30 04:59:41 2020-05-15 5 0.672 f #&gt; 6 2020-04-30 05:54:30 2020-05-09 6 0.113 q #&gt; # ... with 994 more rows 在打印大数据框时，tibble的这种设计避免了输出一下子占据控制台的很多行。 有时需要比默认显示更多的输出，这是要设置几个参数。 首先，可以明确使用print()函数来打印数据框（实际上是print.tbl())，并控制打印的行数（n）和显示的宽度（width）。width = Inf可以显示出所有列: nycflights13::flights %&gt;% print(n = 10, width = Inf) #&gt; # A tibble: 336,776 x 19 #&gt; year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2013 1 1 517 515 2 830 819 #&gt; 2 2013 1 1 533 529 4 850 830 #&gt; 3 2013 1 1 542 540 2 923 850 #&gt; 4 2013 1 1 544 545 -1 1004 1022 #&gt; 5 2013 1 1 554 600 -6 812 837 #&gt; 6 2013 1 1 554 558 -4 740 728 #&gt; 7 2013 1 1 555 600 -5 913 854 #&gt; 8 2013 1 1 557 600 -3 709 723 #&gt; 9 2013 1 1 557 600 -3 838 846 #&gt; 10 2013 1 1 558 600 -2 753 745 #&gt; arr_delay carrier flight tailnum origin dest air_time distance hour minute #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 11 UA 1545 N14228 EWR IAH 227 1400 5 15 #&gt; 2 20 UA 1714 N24211 LGA IAH 227 1416 5 29 #&gt; 3 33 AA 1141 N619AA JFK MIA 160 1089 5 40 #&gt; 4 -18 B6 725 N804JB JFK BQN 183 1576 5 45 #&gt; 5 -25 DL 461 N668DN LGA ATL 116 762 6 0 #&gt; 6 12 UA 1696 N39463 EWR ORD 150 719 5 58 #&gt; 7 19 B6 507 N516JB EWR FLL 158 1065 6 0 #&gt; 8 -14 EV 5708 N829AS LGA IAD 53 229 6 0 #&gt; 9 -8 B6 79 N593JB JFK MCO 140 944 6 0 #&gt; 10 8 AA 301 N3ALAA LGA ORD 138 733 6 0 #&gt; time_hour #&gt; &lt;dttm&gt; #&gt; 1 2013-01-01 05:00:00 #&gt; 2 2013-01-01 05:00:00 #&gt; 3 2013-01-01 05:00:00 #&gt; 4 2013-01-01 05:00:00 #&gt; 5 2013-01-01 06:00:00 #&gt; 6 2013-01-01 05:00:00 #&gt; 7 2013-01-01 06:00:00 #&gt; 8 2013-01-01 06:00:00 #&gt; 9 2013-01-01 06:00:00 #&gt; 10 2013-01-01 06:00:00 #&gt; # ... with 336,766 more rows 2.2.3 Subsetting 取子集(Subsetting)时的行为又是区分 data.frame 和 tibble 很重要的一个特性。简单来讲，R中有两种取子集的系统。一种是用[在原子向量、列表、矩阵、数组和数据框中提取任意数量的元素，一种是用 [[ 或者 $ 在以上对象中提取单个元素。 传统的数据框 data.frame 在这两种方式上均有缺陷： 当想用df[, vars] 在 data.frame 中提取列时，如果vars包含多个变量，则返回一个数据框；如果vars只包含一个变量，则返回一个向量(因为[不要求必须提取多于一个元素)。这种不一致性有时这会导致很多bug，因为很多函数不能作用于向量。 df &lt;- data.frame( x = rnorm(10), y = rnorm(10), z = rnorm(10) ) ## 向量 df[,&quot;x&quot;] #&gt; [1] -1.5723 0.8783 -0.0461 -0.0778 0.1201 0.7874 -0.7307 -0.3133 -0.6744 #&gt; [10] -0.9759 ## 数据框 df[,c(&quot;x&quot;,&quot;y&quot;)] #&gt; x y #&gt; 1 -1.5723 1.442 #&gt; 2 0.8783 -0.738 #&gt; 3 -0.0461 -0.280 #&gt; 4 -0.0778 0.451 #&gt; 5 0.1201 1.437 #&gt; 6 0.7874 1.531 #&gt; 7 -0.7307 0.728 #&gt; 8 -0.3133 0.018 #&gt; 9 -0.6744 -0.108 #&gt; 10 -0.9759 1.533 ## drop = FALSE始终返回数据框 df[,&quot;x&quot;, drop = FALSE] #&gt; x #&gt; 1 -1.5723 #&gt; 2 0.8783 #&gt; 3 -0.0461 #&gt; 4 -0.0778 #&gt; 5 0.1201 #&gt; 6 0.7874 #&gt; 7 -0.7307 #&gt; 8 -0.3133 #&gt; 9 -0.6744 #&gt; 10 -0.9759 当想用df$x提取出变量x时，如果x不在data.frame中，data.frame会返回一个名字以x开始的变量(这种行为被称为部分匹配(partial matching))，如果不存在这样的变量，则返回NULL。这使得我们很容易选取到错误的变量 tibble 在这两点缺陷上做了改进。首先，当 df[, vars]作用于tibble 时，无论 vars 包含多少个变量，返回值总是一个tibble: df &lt;- tibble( x = runif(5), y = rnorm(5) ) df[, c(&quot;x&quot;,&quot;y&quot;)] #&gt; # A tibble: 5 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.548 0.281 #&gt; 2 0.804 -0.272 #&gt; 3 0.837 0.388 #&gt; 4 0.930 1.79 #&gt; 5 0.785 -0.308 df[, &quot;x&quot;] #&gt; # A tibble: 5 x 1 #&gt; x #&gt; &lt;dbl&gt; #&gt; 1 0.548 #&gt; 2 0.804 #&gt; 3 0.837 #&gt; 4 0.930 #&gt; 5 0.785 其次，使用$或者[[]]时,tibble不会进行部分匹配，如果该变量不存在，直接报错： df_1 &lt;- data.frame(xyz = 1) df_2 &lt;- tibble(xyz = 1) df_1$xy #&gt; [1] 1 df_2$xy #&gt; NULL 另外，[[可以按名称和位置提取变量，$只能按名称提取变量，但可以减少一些输入： ## 按名称提取 df$x #&gt; [1] 0.548 0.804 0.837 0.930 0.785 df[[&quot;x&quot;]] #&gt; [1] 0.548 0.804 0.837 0.930 0.785 ## 按位置提取 df[[1]] ## 提取第一列 #&gt; [1] 0.548 0.804 0.837 0.930 0.785 如果想在管道中使用这些取子集操作，需要使用特殊的占位符 . df %&gt;% .[, &quot;x&quot;] #&gt; # A tibble: 5 x 1 #&gt; x #&gt; &lt;dbl&gt; #&gt; 1 0.548 #&gt; 2 0.804 #&gt; 3 0.837 #&gt; 4 0.930 #&gt; 5 0.785 df %&gt;% .$x #&gt; [1] 0.548 0.804 0.837 0.930 0.785 df %&gt;% .[[&quot;x&quot;]] #&gt; [1] 0.548 0.804 0.837 0.930 0.785 "],
["comparing-two-data-frames-tibbles.html", "2.3 Comparing two data frames (tibbles)", " 2.3 Comparing two data frames (tibbles) https://sharla.party/post/comparing-two-dfs/ A summary table from the blog: dplyr::all_equal() janitor::compare_df_cols() vetr::alike() diffdf::diffdf() iris is iris ✅ ✅ ✅ ✅ column swapped iris is iris ✅ ✅ ❌ ✅ missing columns ✅ ✅ ✅ ✅ extra columns ✅ ✅ ✅ ✅ missing and extra columns ✅ ✅ ❌ ✅ difference in class ✅ ✅ ✅ ✅ different columns and classes ❌ ✅ ❌ ✅ nice strings to use for messages ✅ ❌ ✅❌ ❌ returns data on differences ❌ ✅ ❌ ✅ First, take iris data as a reference for comparison: df &lt;- iris %&gt;% as_tibble() Then create some iris variants for the purpose of comparison: df_missing and df_extra for less or more columns df_class for wrong class df_order for new order of same set of columns df_missing &lt;- df %&gt;% select(-Species) df_extra &lt;- df %&gt;% mutate(extra = &quot;extra&quot;) df_class &lt;- df %&gt;% mutate(Species = as.character(Species)) df_order &lt;- df %&gt;% select(Species, everything()) 2.3.1 dplyr::all_equal() dplyr::all_equal(target, current) compare if current and target are identical ,and it could only compares 2 data frames at the same time, with several other arguments: ignore_col_order = TRUE: Should order of columns be ignored? ignore_row_order = TRUE: Should order of rows be ignored? convert = FALSE: Should similar classes be converted? Currently this will convert factor to character and integer to double. if there are missing and extra columns? all_equal(df, df_missing) #&gt; [1] &quot;Cols in x but not y: `Species`. &quot; all_equal(df, df_extra) #&gt; [1] &quot;Cols in y but not x: `extra`. &quot; if there’s an incorrect variable class? all_equal(df, df_class) #&gt; [1] &quot;Incompatible type for column `Species`: x factor, y character&quot; all_equal(df, df_class, convert = TRUE) #&gt; [1] TRUE 2.3.2 janitor::compare_df_cols() Unlike dplyr::all_equal, janitor::compare_df_cols() returns a comparison of the columns in data frames being compared (what’s in both data frames, and their classes in each). It does not cares about rows, since it mean to show wheather several data frames can be row-binded, instead of identity(Although here we have the same rows). library(janitor) compare_df_cols(df, df_missing, df_extra, df_class, df_order) #&gt; column_name df df_missing df_extra df_class df_order #&gt; 1 extra &lt;NA&gt; &lt;NA&gt; character &lt;NA&gt; &lt;NA&gt; #&gt; 2 Petal.Length numeric numeric numeric numeric numeric #&gt; 3 Petal.Width numeric numeric numeric numeric numeric #&gt; 4 Sepal.Length numeric numeric numeric numeric numeric #&gt; 5 Sepal.Width numeric numeric numeric numeric numeric #&gt; 6 Species factor &lt;NA&gt; factor character factor We can set an option return only to return things that don’t match (or things that do): compare_df_cols(df, df_missing, df_extra, df_class, df_order, return = &quot;mismatch&quot;) #&gt; column_name df df_missing df_extra df_class df_order #&gt; 1 Species factor &lt;NA&gt; factor character factor Here only the wrong class case is returned, and df_missing, df_extra, df_order are considered matching when compared to df.That is because compare_df_cols() won’t be affected by order of columns, and it use either of dplyr::bind_rows() or rbind() to decide mathcing. bind_rows() are looser in the sense that columns missing from a data frame would be considered a matching (i.e, select() on a data frame will not generate a “new” one). with rbind(), columns missing from a data.frame would be considered a mismatch # missing column is considered a sort of &quot;matching&quot; when bind_method = &quot;bind_rows&quot; compare_df_cols(df, df_missing, df_extra, df_class, df_order, return = &quot;match&quot;) #&gt; column_name df df_missing df_extra df_class df_order #&gt; 1 extra &lt;NA&gt; &lt;NA&gt; character &lt;NA&gt; &lt;NA&gt; #&gt; 2 Petal.Length numeric numeric numeric numeric numeric #&gt; 3 Petal.Width numeric numeric numeric numeric numeric #&gt; 4 Sepal.Length numeric numeric numeric numeric numeric #&gt; 5 Sepal.Width numeric numeric numeric numeric numeric # method = &quot;rbind&quot; compare_df_cols(df, df_missing, df_extra, df_class, df_order, return = &quot;match&quot;, bind_method = &quot;rbind&quot;) #&gt; column_name df df_missing df_extra df_class df_order #&gt; 1 Petal.Length numeric numeric numeric numeric numeric #&gt; 2 Petal.Width numeric numeric numeric numeric numeric #&gt; 3 Sepal.Length numeric numeric numeric numeric numeric #&gt; 4 Sepal.Width numeric numeric numeric numeric numeric Note that janitor::compare_df_cols() returns a data frame, which can be easily incorporated into custom message using the glue package: compare_df_cols(df, df_missing, df_extra, df_class, df_order) %&gt;% mutate(comparison = glue::glue(&quot;Column: {column_name}: {df} in df,{df_missing} in df_missing, {df_extra} in df_extra,{df_order} in df_order&quot;)) %&gt;% select(comparison) #&gt; comparison #&gt; 1 Column: extra: NA in df,NA in df_missing, character in df_extra,NA in df_order #&gt; 2 Column: Petal.Length: numeric in df,numeric in df_missing, numeric in df_extra,numeric in df_order #&gt; 3 Column: Petal.Width: numeric in df,numeric in df_missing, numeric in df_extra,numeric in df_order #&gt; 4 Column: Sepal.Length: numeric in df,numeric in df_missing, numeric in df_extra,numeric in df_order #&gt; 5 Column: Sepal.Width: numeric in df,numeric in df_missing, numeric in df_extra,numeric in df_order #&gt; 6 Column: Species: factor in df,NA in df_missing, factor in df_extra,factor in df_order and the resulting data frame can be filtered manually when the filters from return aren’t what i want, to see all differences: compare_df_cols(df, df_missing, df_extra, df_class, df_order) %&gt;% filter(is.na(df) | df_class != df_order) #&gt; column_name df df_missing df_extra df_class df_order #&gt; 1 extra &lt;NA&gt; &lt;NA&gt; character &lt;NA&gt; &lt;NA&gt; #&gt; 2 Species factor &lt;NA&gt; factor character factor To get a binary message to see whether a set of data.frames are row-bindable, use janitor::compare_df_cols_sames() compare_df_cols_same(df, df_missing) #&gt; [1] TRUE compare_df_cols_same(df, df_missing, bind_method = &quot;rbind&quot;) #&gt; column_name ..1 ..2 #&gt; 1 Species factor &lt;NA&gt; #&gt; [1] FALSE 2.3.3 vetr::alike() vetr::alike(target, current) is similar to base::all.equal() (dplyr::all_equal()’s conuterparts in base R), but it only compares object structure. In the case of data frames, vetr::alike() compares columns and ignores rows. It is useful for all kinds of objects, but we focus on comparing data frames here. library(vetr) alike(df, df_missing) #&gt; [1] &quot;`df_missing` should have 5 columns (has 4)&quot; alike(df, df_extra) #&gt; [1] &quot;`df_extra` should have 5 columns (has 6)&quot; alike(df, df_class) #&gt; [1] &quot;`df_class$Species` should be class \\&quot;factor\\&quot; (is \\&quot;character\\&quot;)&quot; alike(df, df_order) #&gt; [1] &quot;`names(df_order)[1]` should be \\&quot;Sepal.Length\\&quot; (is \\&quot;Species\\&quot;)&quot; As it turns out, vetr::alike() detects all differences, and makes a declarative comparison. 2.3.4 diffdf::diffdf() diffdf is a package dedicated to providing tools for working with data frame difference. diffdf(base, compare) comapres 2 data frames (compare against base) and outputs any differences : library(diffdf) diffdf(df, df_missing) #&gt; Warning in diffdf(df, df_missing): #&gt; There are columns in BASE that are not in COMPARE !! #&gt; Differences found between the objects! #&gt; #&gt; A summary is given below. #&gt; #&gt; There are columns in BASE that are not in COMPARE !! #&gt; All rows are shown in table below #&gt; #&gt; ========= #&gt; COLUMNS #&gt; --------- #&gt; Species #&gt; --------- diffdf(df, df_extra) #&gt; Warning in diffdf(df, df_extra): #&gt; There are columns in COMPARE that are not in BASE !! #&gt; Differences found between the objects! #&gt; #&gt; A summary is given below. #&gt; #&gt; There are columns in COMPARE that are not in BASE !! #&gt; All rows are shown in table below #&gt; #&gt; ========= #&gt; COLUMNS #&gt; --------- #&gt; extra #&gt; --------- diffdf(df, df_class) #&gt; Warning in diffdf(df, df_class): #&gt; There are columns in BASE and COMPARE with different modes !! #&gt; There are columns in BASE and COMPARE with different classes !! #&gt; Differences found between the objects! #&gt; #&gt; A summary is given below. #&gt; #&gt; There are columns in BASE and COMPARE with different modes !! #&gt; All rows are shown in table below #&gt; #&gt; ================================ #&gt; VARIABLE MODE.BASE MODE.COMP #&gt; -------------------------------- #&gt; Species numeric character #&gt; -------------------------------- #&gt; #&gt; There are columns in BASE and COMPARE with different classes !! #&gt; All rows are shown in table below #&gt; #&gt; ================================== #&gt; VARIABLE CLASS.BASE CLASS.COMP #&gt; ---------------------------------- #&gt; Species factor character #&gt; ---------------------------------- diffdf(df, df_order) #&gt; No issues were found! diffdf() is sensitive to missing or extra columns, wrong classes and not to order. This function also returns a list of data frames with issues invisibly, similar to janitor::compare_df_cols(): issues &lt;- diffdf(df, df_missing) #&gt; Warning in diffdf(df, df_missing): #&gt; There are columns in BASE that are not in COMPARE !! issues$ExtColsBase #&gt; # A tibble: 1 x 1 #&gt; COLUMNS #&gt; * &lt;chr&gt; #&gt; 1 Species issues &lt;- diffdf(df, df_extra) #&gt; Warning in diffdf(df, df_extra): #&gt; There are columns in COMPARE that are not in BASE !! issues$ExtColsComp #&gt; # A tibble: 1 x 1 #&gt; COLUMNS #&gt; * &lt;chr&gt; #&gt; 1 extra issues &lt;- diffdf(df, df_class) #&gt; Warning in diffdf(df, df_class): #&gt; There are columns in BASE and COMPARE with different modes !! #&gt; There are columns in BASE and COMPARE with different classes !! issues$VarModeDiffs #&gt; VARIABLE MODE.BASE MODE.COMP #&gt; 6 Species numeric character issues$VarClassDiffs %&gt;% unnest(CLASS.BASE) %&gt;% unnest(CLASS.COMP) #&gt; # A tibble: 1 x 3 #&gt; VARIABLE CLASS.BASE CLASS.COMP #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Species factor character "],
["tibble-exercise.html", "2.4 Exercises", " 2.4 Exercises Exercise 2.1 Practice referring to non-syntactic names in the following data frame annoying &lt;- tibble( `1` = 1:10, `2` = `1` * 2 + rnorm(length(`1`)) ) # extracting the variable called 1 annoying$`1` #&gt; [1] 1 2 3 4 5 6 7 8 9 10 # Plotting a scatterplot of `1` vs `2` ggplot(annoying,mapping = aes(x = `1`, y = `2`)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) # Creating a new column called 3 which is `2` divided by `1` mutate(annoying,`3`= `2`/`1`) #&gt; # A tibble: 10 x 3 #&gt; `1` `2` `3` #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2.17 2.17 #&gt; 2 2 3.85 1.93 #&gt; 3 3 5.58 1.86 #&gt; 4 4 7.20 1.80 #&gt; 5 5 9.12 1.82 #&gt; 6 6 12.1 2.02 #&gt; # ... with 4 more rows # Renaming the columns to `one`, `two` and `three` (annoying &lt;- rename(annoying,one = `1`, two = `2`)) #&gt; # A tibble: 10 x 2 #&gt; one two #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 2.17 #&gt; 2 2 3.85 #&gt; 3 3 5.58 #&gt; 4 4 7.20 #&gt; 5 5 9.12 #&gt; 6 6 12.1 #&gt; # ... with 4 more rows "],
["readr-data-import.html", "3 readr: Data import", " 3 readr: Data import library(readr) "],
["importing-data-in-base-r.html", "3.1 Importing data in base R", " 3.1 Importing data in base R base R中有 3 个最主要的数据导入函数： 函数 用途 read.csv() 读取以逗号分隔的文件(sep = \",\", header = T) read.delim() 读取以制表符分隔的文件(sep = \"\\t\", header = T) read.table() 读取其他一般性的文件，默认添加表头(sep = \"\", header = F)。read.csv()和read.delim()可以看做read.table()针对于特定情况的封装。 用于导入的函数一般都有大量的参数，上述的三个函数中，比较重要的参数有： dec: 指定文件中用于表示小数点的符号，很多文件中可能会用;或者,当做小数点，这时不仅需要设置dec，也要适当调整sep。read.csv2()和read.delim2()是对一些设置的封装。 col.names 用于在 header = F 时指定列名，默认情况下将被命名为 “V1”、“V2”、··· row.names用于设置行名，既可以传入一个向量直接指定，也可以选择数据中的某一列作为列名（用一个数字表明该列是第几列） colClasses:指定列的数据类型。特别是当你希望有些列作为因子，有些列作为字符串，这个参数便很有用。常用的类别有“numeric”,“factor”,“logical”,“character”。如果某列被指定为\"NULL\"，则该列不会被读入。 skip指定读取数据前跳过的行数 nrows读入的最大行数 我们使用本地数据集hotdogs.txt简要说明这些函数的用法，它没有列名，且用空格分隔： ## 分别使用三个函数 hotdogs1 &lt;- read.table(&quot;data/hotdogs.txt&quot;, sep = &quot;\\t&quot;) head(hotdogs1) #&gt; V1 V2 V3 #&gt; 1 Beef 186 495 #&gt; 2 Beef 181 477 #&gt; 3 Beef 176 425 #&gt; 4 Beef 149 322 #&gt; 5 Beef 184 482 #&gt; 6 Beef 190 587 hotdogs2 &lt;- read.csv(&quot;data/hotdogs.txt&quot;, sep = &quot;\\t&quot;, header = F) head(hotdogs2) #&gt; V1 V2 V3 #&gt; 1 Beef 186 495 #&gt; 2 Beef 181 477 #&gt; 3 Beef 176 425 #&gt; 4 Beef 149 322 #&gt; 5 Beef 184 482 #&gt; 6 Beef 190 587 hotdogs2 &lt;- read.delim(&quot;data/hotdogs.txt&quot;, header = F) head(hotdogs2) #&gt; V1 V2 V3 #&gt; 1 Beef 186 495 #&gt; 2 Beef 181 477 #&gt; 3 Beef 176 425 #&gt; 4 Beef 149 322 #&gt; 5 Beef 184 482 #&gt; 6 Beef 190 587 设定 col.names 和 colClasses： hotdogs &lt;- read.delim(&quot;data\\\\hotdogs.txt&quot;, header = FALSE, col.names = c(&quot;type&quot;, &quot;calories&quot;, &quot;sodium&quot;), colClasses = c(&quot;factor&quot;, &quot;NULL&quot;, &quot;numeric&quot;)) head(hotdogs) #&gt; type sodium #&gt; 1 Beef 495 #&gt; 2 Beef 477 #&gt; 3 Beef 425 #&gt; 4 Beef 322 #&gt; 5 Beef 482 #&gt; 6 Beef 587 "],
["importing-data-in-readr.html", "3.2 Importing data in readr", " 3.2 Importing data in readr 3.2.1 Introduction readr 是 tidyverse 的核心 R 包之一，作用是将平面数据(flat files)快速读取至 R 中，并转换为易用的 tibble 格式。相对于 base R 中有的数据读取函数，它有以下的优点： 一般来说，它们比基础模块中的函数速度更快（约快 10 倍）。第 ?? 章中的 fread() 和第 10 章中的 vroom() 提供了更快速的数据读取 它们可以生成 tibble，而且不会将字符串向量转换为因子，不使用行名称，也不会随意改动列名称，这些都是 Base R 读取的痛点 它们更容易重复使用。base R 中的函数会继承操作系统的功能，并依赖环境变量，因此，可以在你的计算机上正常运行的代码未必适用于他人的计算机 主要函数有： read_csv() 读取逗号分割文件、read_csv2() 读取分号分隔文件（这在用逗号表示小数点的国家非常普遍），read_tsv() 读取制表符分隔文件，read_delim()函数可以读取使用任意分隔符的文件(通过指定 delim 参数) read_fwf() 读取固定宽度的文件(fixed width file)。既可以使用 fwf_width() 函按照宽度来设定域，也可以使用 fwf_positions() 函数按照位置来设定域。read_table() 读取固定宽度文件的一种常用变体，其中使用空白字符来分隔各列 重要参数： delim 指定分隔符 col_names = T：以上的函数默认将第一行用作列名，如果设定col_names = F则列名为X1、X2、···。也可以指定一个字符向量。 col_types用一个字符串指定各列的数据类型 字符 含义 “i” integer “d” double “c” character “l” logical \"_\" 舍弃该列 n_max最大读取行数 na表明文件中缺失值的表示方法 下面用一些例子演示read_delim()函数的用法，因为它是最一般的形式，一旦掌握它，我们就可将从中学到的经验轻松应用于readr的其他函数。 ## column names properties &lt;- c(&quot;area&quot;, &quot;temp&quot;, &quot;size&quot;, &quot;storage&quot;, &quot;method&quot;, &quot;texture&quot;, &quot;flavor&quot;, &quot;moistness&quot;) ## 文件中用制表符为分隔符 potatoes &lt;- read_delim(&quot;data\\\\potatoes.txt&quot;,delim = &quot;\\t&quot;, col_names = properties) potatoes #&gt; # A tibble: 160 x 8 #&gt; area temp size storage method texture flavor moistness #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 1 1 1 2.9 3.2 3 #&gt; 2 1 1 1 1 2 2.3 2.5 2.6 #&gt; 3 1 1 1 1 3 2.5 2.8 2.8 #&gt; 4 1 1 1 1 4 2.1 2.9 2.4 #&gt; 5 1 1 1 1 5 1.9 2.8 2.2 #&gt; 6 1 1 1 2 1 1.8 3 1.7 #&gt; # ... with 154 more rows 当运行readr中的数据导入函数时，会打印出一份数据了说明，给出每个列的名称和类型。后面我们学习解析时，还会继续讨论这项功能。 通过col_types指定前五列为integer类型： properties &lt;- c(&quot;area&quot;, &quot;temp&quot;, &quot;size&quot;, &quot;storage&quot;, &quot;method&quot;, &quot;texture&quot;, &quot;flavor&quot;, &quot;moistness&quot;) potatoes &lt;- read_delim(&quot;data\\\\potatoes.txt&quot;,delim = &quot;\\t&quot;, col_names = properties, col_types = &quot;iiiiiddd&quot;) potatoes #&gt; # A tibble: 160 x 8 #&gt; area temp size storage method texture flavor moistness #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 1 1 1 2.9 3.2 3 #&gt; 2 1 1 1 1 2 2.3 2.5 2.6 #&gt; 3 1 1 1 1 3 2.5 2.8 2.8 #&gt; 4 1 1 1 1 4 2.1 2.9 2.4 #&gt; 5 1 1 1 1 5 1.9 2.8 2.2 #&gt; 6 1 1 1 2 1 1.8 3 1.7 #&gt; # ... with 154 more rows 我们还可以创建一个行内 csv 文件。这种文件非常适合用readr进行实验，以及与他人分享可重现的例子： read_delim(&quot;a, b, c 1, 2 , 3 4, 5, 6&quot;, delim = &quot;,&quot;) #&gt; # A tibble: 2 x 3 #&gt; a ` b` ` c` #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 &quot; 1&quot; &quot; 2 &quot; &quot; 3&quot; #&gt; 2 &quot; 4&quot; &quot; 5&quot; &quot; 6&quot; 有时文件开头会有几行元数据。可以使用 skip = n 来跳过前 n 行；或者使用 comment = \"#\" 丢弃所有以 # 开头的行： read_delim(&quot;The first line metadata The second line of metadata x,y,z 1,2,3&quot;, delim = &quot;,&quot;, skip = 2) #&gt; # A tibble: 1 x 3 #&gt; ` x` y z #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 &quot; 1&quot; 2 3 read_delim(&quot;# A comment I want to skip x,y,z 1,2,3&quot;, delim = &quot;,&quot;, comment = &quot;#&quot;) #&gt; # A tibble: 1 x 3 #&gt; ` x` y z #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 &quot; 1&quot; 2 3 设置na: read_delim(&quot;a,b,c 1,2,.&quot;, delim = &quot;,&quot;, na = &quot;.&quot;) #&gt; # A tibble: 1 x 3 #&gt; a b c #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; #&gt; 1 &quot; 1&quot; 2 NA 3.2.2 Writing data readr还提供了两个非常有用的函数，用于将数据写回到磁盘：write_csv() 和 write_tsv()，这两个函数输出的文件能够顺利读取的概率更高，因为： 它们总使用 UTF-8 对字符串进行编码 它们都是用 ISO 8601 日期格式来保存日期和日期时间数据 如果想要将 CSV 文件导为 Excel 文件，可以使用 write_excel_csv() 函数，该函数会在文件开头写入一个特殊字符（字节顺序标记），告诉 Excel 这个文件采用的是 UTF-8 编码。 这几个函数中最重要的参数是 x (要保存的数据框)和path(保存文件的位置)。还可以使用 na 参数设定如何写入缺失值。默认情况下，写入函数会创建一个新文件或清空原有的文件再导入数据(Python中open()函数的\"mode = w\"模式)，如果想要追加到现有的文件，可以设置 append = T (Python中的mode = \"a\"模式)： library(gapminder) write_csv(x = gapminder, path = &quot;data\\\\gapminder.csv&quot;) 打开对应的文件： 3.2.3 Exercises Exercise 3.1 如果一个文件中的域是由“|”分隔的，那么应该使用哪个函数读取这个文件？ 应该使用read_delim(path, delim = \"|\") Exercise 3.2 read_fwf() 中最重要的参数是什么？ read_fwf()用于固定宽度文件(fixed width files)。在固定宽度文件中，每一列的的宽度是固定的（不足的用某种填充符号填充），如第一列总是10个字符长度，第二列 5 个字符长度，第三列8个字符长度，每列内采取统一的对齐方式。readr安装时附带了一个固定宽度文件的示例,我们用一个变量存储它的路径： fwf_sample &lt;- readr_example(&quot;fwf-sample.txt&quot;) fwf_sample #&gt; [1] &quot;C:/Users/Lenovo/Documents/R/win-library/3.6/readr/extdata/fwf-sample.txt&quot; txt 文件内的内容： 读取固定宽度文件时，最重要的是告诉 R 每列的位置，参数 col_positions 用于这项工作，有几种不同的表示方式： # You can specify column positions in several ways: # 1. Guess based on position of empty columns read_fwf(fwf_sample, col_positions = fwf_empty(fwf_sample, col_names = c(&quot;first&quot;, &quot;last&quot;, &quot;state&quot;, &quot;ssn&quot;))) #&gt; # A tibble: 3 x 4 #&gt; first last state ssn #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John Smith WA 418-Y11-4111 #&gt; 2 Mary Hartford CA 319-Z19-4341 #&gt; 3 Evan Nolan IL 219-532-c301 # 2. A vector of field widths read_fwf(fwf_sample, col_positions = fwf_widths(c(20, 10, 12), col_names = c(&quot;name&quot;, &quot;state&quot;, &quot;ssn&quot;))) #&gt; # A tibble: 3 x 3 #&gt; name state ssn #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John Smith WA 418-Y11-4111 #&gt; 2 Mary Hartford CA 319-Z19-4341 #&gt; 3 Evan Nolan IL 219-532-c301 # 3. Paired vectors of start and end positions read_fwf(fwf_sample, col_positions = fwf_positions(c(1, 30), c(20, 42), col_names = c(&quot;name&quot;, &quot;ssn&quot;))) #&gt; # A tibble: 3 x 2 #&gt; name ssn #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John Smith 418-Y11-4111 #&gt; 2 Mary Hartford 319-Z19-4341 #&gt; 3 Evan Nolan 219-532-c301 # 4. Named arguments with start and end positions read_fwf(fwf_sample, col_positions = fwf_cols(name = c(1, 20), ssn = c(30, 42))) #&gt; # A tibble: 3 x 2 #&gt; name ssn #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John Smith 418-Y11-4111 #&gt; 2 Mary Hartford 319-Z19-4341 #&gt; 3 Evan Nolan 219-532-c301 # 5. Named arguments with column widths read_fwf(fwf_sample, col_positions = fwf_cols(name = 20, state = 10, ssn = 12)) #&gt; # A tibble: 3 x 3 #&gt; name state ssn #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John Smith WA 418-Y11-4111 #&gt; 2 Mary Hartford CA 319-Z19-4341 #&gt; 3 Evan Nolan IL 219-532-c301 "],
["parsing-a-vector.html", "3.3 Parsing a vector", " 3.3 Parsing a vector 在详细介绍 readr 如何从磁盘读取文件之前，我们需要先讨论一下 parse_*()函数族。这些函数接受一个字符向量（因为文件中的数据全部是以字符串的形式进入 R 的），并返回一个特定向量，如逻辑、整数或日期向量： ## 用str()函数返回类别 str(parse_logical(c(&quot;True&quot;, &quot;False&quot;, &quot;True&quot;))) #&gt; logi [1:3] TRUE FALSE TRUE str(parse_integer(c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;))) #&gt; int [1:3] 1 2 3 str(parse_date(c(&quot;2010-01-01&quot;, &quot;2019-08-15&quot;))) #&gt; Date[1:2], format: &quot;2010-01-01&quot; &quot;2019-08-15&quot; 这些函数各司其职，且都是 readr 的重要组成部分。一旦掌握了本节中这些单个解析函数的用法，我们就可以继续讨论如何综合使用它们来解析整个文件了。 和 tidyverse 中出现的函数族一样，parse_*() 函数族有着相同的参数结构。第一个参数是需要解析的字符向量，na参数设定了哪些字符串应该当做缺失值处理： parse_integer(c(&quot;1&quot;,&quot;2&quot;,&quot;.&quot;,&quot;456&quot;),na = &quot;.&quot;) #&gt; [1] 1 2 NA 456 如果解析失败，你会收到一条警告： x &lt;- parse_integer(c(&quot;123&quot;, &quot;345&quot;, &quot;abc&quot;, &quot;123.45&quot;)) 解析失败的值在输出中以缺失值的形式存在: x #&gt; [1] 123 345 NA NA #&gt; attr(,&quot;problems&quot;) #&gt; # A tibble: 2 x 4 #&gt; row col expected actual #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 3 NA an integer abc #&gt; 2 4 NA no trailing characters .45 如果解析失败的值很多，那么就应该使用 problems() 函数来获取完整的失败信息集合。这个函数会返回一个 tibble，可以使用 dplyr 来处理它： problems(x) #&gt; # A tibble: 2 x 4 #&gt; row col expected actual #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 3 NA an integer abc #&gt; 2 4 NA no trailing characters .45 在解析函数的使用方面，最重要的是知道有哪些解析函数，以及每种解析函数用来处理哪种类型的输入。具体来说，重要的解析函数有8中： parse_logical()和parse_integer()函数分别解析逻辑值和整数，因为这两个解析函数基本不会出现问题，所以我们不再进行更多介绍 parse_double()是严格的数值型解析函数，而parse_number()则是灵活的数值型解析函数，这两个函数背后很复杂，因为世界各地书写数值的方式不尽相同 parse_character()函数似乎太过简单，甚至没必要存在，因为R读取的文件本身就是字符串形式。但一个棘手的问题使得这个函数非常重要：字符编码 parse_factor()函数可以创建因子，R使用这种数据结构表示分类变量，该变量具有固定数目的已知值 parse_datetime()、parse_date()和parse_time()函数可以解析不同类型的日期和时间，它们是最复杂的，因为有太多不同的日期书写形式 3.3.1 Numeric 解析数值似乎是直截了当的，但以下 3 个问题增加了数値解析的复杂性： 世界各地的人们书写数值的方式不尽相同。例如有些国家使用.当做小数点，而有些国家使用, 数值周围可能会有其他字符，例如 $1000、15℃ 和10% 数值经常包含某种形式的分组(grouping)，以便更易读，如1 000 000，而且世界各地用来分组的字符也不统一 为了解决第一个问题，readr使用了地区的概念(locale)，使得可以按照不同地区设置解析选项。在解析数值时，最重要的选项就是用来表示小数点的字符，通过设置一个新的地区对象并设定decimal_mark参数，可以覆盖.的默认值： parse_double(&quot;1.23&quot;) #&gt; [1] 1.23 parse_double(&quot;1,23&quot;, locale = locale(decimal_mark = &quot;,&quot;)) #&gt; [1] 1.23 locale()函数可以通过decimal_mark,grouping_mark,date_format,time_format,tz,encoding等参数创建一个地区对象，设定该地区内的一些表示习惯，这里我们告诉R哪个符号被用来当做小数点。 readr的默认地区是US-centric。获取默认地区的另一种方法是利用操作系统，但这可能让你的代码只能在你的电脑上运行，通过电子邮件共享给另一个国家的同事时，就可能失效。 parse_number()解决了第二个问题：它可以忽略数值前后的非数值型字符。这个函数特别适合处理货币和百分比，你可以提取嵌在文本中的数值： parse_number(&quot;$100&quot;) #&gt; [1] 100 parse_number(&quot;20%&quot;) #&gt; [1] 20 parse_number(&quot;37℃&quot;) #&gt; [1] 37 parse_number(&quot;It cost $123.45&quot;) #&gt; [1] 123 组合使用 parse_number() 和 locale() 可以解决最后一个问题，因为 parse_number() 可以忽略“分组符号” ： ## 适用于美国，忽略分组符号 parse_number(&quot;123,456,789&quot;) #&gt; [1] 123456789 ## 适用于多数欧洲国家，需要用locale设置分组符号，因为.被默认为小数点 parse_number(&quot;123.456.789&quot;, locale = locale(grouping_mark = &quot;.&quot;)) #&gt; [1] 123456789 ## 适用于瑞士 parse_number(&quot;123&#39;456&#39;789&quot;, locale = locale(grouping_mark = &quot;&#39;&quot;)) #&gt; [1] 123456789 3.3.2 Character To better illustrate the challenges of parsing a character vector, we need to have a reasonable understanding of the following questions. 什么是字符？ 字符是各种文字和符号的总称，包括各个国家文字、标点符号、图形符号、数字等，甚至还包括表情符号。 什么是字符集？ 字符集是多个字符的集合，字符集种类较多，每个字符集包含的字符个数不同，常见字符集有：ASCII 字符集、ISO 8859字符集、GB2312 字符集、BIG5字符集、GB18030 字符集、Unicode 字符集等。ASCII 可以非常好地表示英文字符，因为它就是美国信息交换标准代码(American Standard Code for Information Interchange)的缩写。Unicode是国际组织制定的可以容纳世界上所有文字和符号的字符编码方案。 什么是字符编码？ 计算机要准确的处理各种字符集文字，需要进行字符编码，以便计算机能够识别和存储各种文字。 字符编码(encoding)和字符集不同。字符集只是字符的集合，不一定适合作网络传送、处理，有时须经编码(encode)后才能应用。如Unicode可依不同需要以UTF-8、UTF-16、UTF-32等方式编码。 字符编码就是以二进制的数字来对应字符集的字符。因此，对字符进行编码，是信息交流的技术基础。 概括 使用哪些字符。也就是说哪些汉字，字母和符号会被收入标准中。所包含“字符”的集合就叫做“字符集”。 规定每个“字符”分别用一个字节还是多个字节存储，用哪些字节来存储，这个规定就叫做“编码”。 各个国家和地区在制定编码标准的时候，“字符的集合”和“编码”一般都是同时制定的。因此，平常我们所说的“字符集”，比如：GB2312, GBK, JIS等，除了有“字符的集合”这层含义外，同时也包含了“编码”的含义。 注意：Unicode字符集有多种编码方式，如UTF-8、UTF-16等；ASCII只有一种；大多数MBCS（包括GB2312，GBK）也只有一种。 在 R 中，我们可以使用charToRaw()函数获得一个字符串的底层表示（underlying representation）： char_raw &lt;- charToRaw(&quot;Maxine&quot;) char_raw #&gt; [1] 4d 61 78 69 6e 65 charToRaw()返回的尚不是编码结果（二进制），而是十六进制的表示。每个十六进制数表示字符串的一个字节：4d是M，61是a等。charToRaw()返回的对象在R中被称为raw type，想要得到真正的二进制编码，要对raw type再使用rawToBits()函数： rawToBits(char_raw) #&gt; [1] 01 00 01 01 00 00 01 00 01 00 00 00 00 01 01 00 00 00 00 01 01 01 01 00 01 #&gt; [26] 00 00 01 00 01 01 00 00 01 01 01 00 01 01 00 01 00 01 00 00 01 01 00 readr默认使用UTF-8编码。其中的含义在于，每当接受到一个字符串，R收到的不是字符串本身，而是它背后的二进制表示，于是R便尝试按照UTF-8的规则解读这些二进制码，把它们还原为人类所能理解的字符。问题是，如果你的文件不是用UTF-8编码，这就像用英文字典来解释汉语拼音，可能小张计算机存储字母”A”是1100001，而小王存储字母”A”是11000010，这样双方交换信息时就会误解。比如小张把1100001发送给小王，小王并不认为1100001是字母”A”，可能认为这是字母”X”，于是小王在用记事本访问存储在硬盘上的1100001时，在屏幕上显示的就是字母”X”。 要解决这个问题，需要在parse_character()函数中通过locale(encoding = )参数设定编码方式： 如何才能找到正确的编码方式呢？有可能数据来源会注明，但如果没有到话，readr包提供了guess_encoding()函数来帮助你找出编码方式。这个函数并非万无一失，如果有大量文本效果就会更好，它的第一个参数可以是直接的文件路径，也可以是一个raw type: guess_encoding(charToRaw(&quot;中国&quot;)) #&gt; # A tibble: 1 x 2 #&gt; encoding confidence #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 ASCII 1 parse_character(&quot;中国&quot;,locale = locale(encoding = &quot;UTF-8&quot;)) #&gt; [1] &quot;&lt;U+4E2D&gt;&lt;U+56FD&gt;&quot; parse_character(&quot;中国&quot;,locale = locale(encoding = &quot;windows-1252&quot;)) #&gt; [1] &quot;&lt;U+4E2D&gt;&lt;U+56FD&gt;&quot; guess_encoding(&quot;data\\\\hotdogs.txt&quot;) #&gt; # A tibble: 1 x 2 #&gt; encoding confidence #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 ASCII 1 编码问题博大精深，这里只是蜻蜓点水式地介绍一下。如果想要学习更多相关知识，可以阅读http://kunststube.net/encoding/ 3.3.3 Factor 因子对应的解析函数是 parse_factor() ，其中 levels 参数被赋予一个包含所有因子可能水平的向量，如果要解析的列存在 levels 中没有的值，就会生成一条警告。 fruit &lt;- c(&quot;apple&quot;, &quot;banana&quot;) f &lt;- parse_factor(c(&quot;apple&quot;, &quot;banana&quot;, &quot;bananana&quot;), levels = fruit) problems(f) #&gt; # A tibble: 1 x 4 #&gt; row col expected actual #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 3 NA value in level set bananana 如果有很多问题条目的话，最简单的是它们当做字符串来解析，然后用forcats包进行后续处理。 3.3.4 Date and time 根据需要的是日期型数据（从 1970-01-01 开始的天数）、日期时间型数据（从 1970-01-01 开始的秒数）还是时间型数据（从午夜开始的描述），我们可以在3中解析函数之间进行选择。在没有使用任何附加参数时调用，具体情况如下： parse_datetime()期待的是符合 ISO 8601 标准的日期时间。ISO 8601是一种国际标准，其中日期的各个部分按从大到小的顺序排列，即年、月、日、小时、分钟、秒： parse_datetime(&quot;2010-10-01 201002&quot;) #&gt; [1] &quot;2010-10-01 20:10:02 UTC&quot; ## 如果时间被忽略了，就会被设置为午夜 parse_datetime(&quot;20101001&quot;) #&gt; [1] &quot;2010-10-01 UTC&quot; 这是最重要的日期/时间标准，如果经常使用日期和时间，可以阅读以下维基百科上的ISO 8601标准 parse_date()期待的是四位数的年份、一个-或者/作为分隔符，月，一个-或者/作为分隔符，然后是日： parse_date(&quot;2010-10-01&quot;) #&gt; [1] &quot;2010-10-01&quot; parse_date(&quot;2010/10/01&quot;) #&gt; [1] &quot;2010-10-01&quot; parse_time()期待的是小时，:作为分隔符，分钟，可选的:和后面的秒，以及一个可选的 am/pm 标识符： parse_time(&quot;8:20 pm&quot;) #&gt; 20:20:00 parse_time(&quot;8:20:05&quot;) #&gt; 08:20:05 如果默认数据不适合实际数据，那么可以为 parse_date() 的第二个参数 format 传入一个字符串指定自己的日期时间格式(这个参数是解析日期时间的函数在parse_*()函数族中特有的)，格式由以下各部分组成： 成分 符号 年 %Y(四位数)%y(两位数；00-69被解释为2000-2069、70-99被解释为1970-1999) 月 %m(两位数) %b（简写名称，如“Jan”） %B（完整名称，如January） 日 %d(一位数或两位数) %e(两位数) 时间 %H(0-23小时)%I(0-12小时，必须和%p一起使用) %p(表示am/pm) %M(分钟) S(整数秒) %OS(实数秒) %Z(时区) 非数值字符 %.(跳过一个非数值字符) %*跳过所有非数值字符 找出正确格式的最好方法是创建几个解析字符向量的示例，并使用某种解析函数进行测试（如果数据中有分隔符，则）： parse_date(&quot;100101&quot;,format = &quot;%m%d%y&quot;) #&gt; [1] &quot;2001-10-01&quot; parse_date(&quot;01/02/15&quot;,&quot;%y/%m/%d&quot;) #&gt; [1] &quot;2001-02-15&quot; parse_date(&quot;Nov/12/1998&quot;,&quot;%b/%d/%Y&quot;) #&gt; [1] &quot;1998-11-12&quot; 3.3.5 Exercises Exercise 1.3 如果在locale()函数中把decimal_mark和grouping_mark设为同一个字符，会发生什么情况？如果将decimal_mark设为逗号，grouping_mark的默认值会发生什么变化？如果将grouping_mark设置为句点，decimal_mark的默认值会发生什么变化？ 不能将decimal_mark 和 group_mark 设为同一个字符： locale(decimal_mark = &quot;.&quot;,grouping_mark = &quot;.&quot;) #&gt; Error: `decimal_mark` and `grouping_mark` must be different 将decimal_mark设为逗号时，grouping_mark的默认值将变为.(见第一行): locale(decimal_mark = &quot;,&quot;) #&gt; &lt;locale&gt; #&gt; Numbers: 123.456,78 #&gt; Formats: %AD / %AT #&gt; Timezone: UTC #&gt; Encoding: UTF-8 #&gt; &lt;date_names&gt; #&gt; Days: Sunday (Sun), Monday (Mon), Tuesday (Tue), Wednesday (Wed), Thursday #&gt; (Thu), Friday (Fri), Saturday (Sat) #&gt; Months: January (Jan), February (Feb), March (Mar), April (Apr), May (May), #&gt; June (Jun), July (Jul), August (Aug), September (Sep), October #&gt; (Oct), November (Nov), December (Dec) #&gt; AM/PM: AM/PM 将grouping_mark设为句点时，decimal_mark的默认值将变为,: locale(grouping_mark = &quot;.&quot;) #&gt; &lt;locale&gt; #&gt; Numbers: 123.456,78 #&gt; Formats: %AD / %AT #&gt; Timezone: UTC #&gt; Encoding: UTF-8 #&gt; &lt;date_names&gt; #&gt; Days: Sunday (Sun), Monday (Mon), Tuesday (Tue), Wednesday (Wed), Thursday #&gt; (Thu), Friday (Fri), Saturday (Sat) #&gt; Months: January (Jan), February (Feb), March (Mar), April (Apr), May (May), #&gt; June (Jun), July (Jul), August (Aug), September (Sep), October #&gt; (Oct), November (Nov), December (Dec) #&gt; AM/PM: AM/PM Exercise 3.3 locale()函数中的date_format()和time_format()参数有什么用？ date_format() 和 time_format() 和 format 参数的功能一样，用以指定日期时间数据的格式，如果在 locale() 中设定了以上两个参数，就不需要再设定 format；反之亦然： parse_date(&quot;170625&quot;, locale = locale(date_format = &quot;%y%m%d&quot;)) #&gt; [1] &quot;2017-06-25&quot; 生成正确行使的字符串来解析以下日期和时间。 d1 &lt;- &quot;January 1,2010&quot; parse_date(d1,&quot;%B %d,%Y&quot;) #&gt; [1] &quot;2010-01-01&quot; d2 &lt;- &quot;2015-Mar-07&quot; parse_date(d2,&quot;%Y-%b-%e&quot;) #&gt; [1] &quot;2015-03-07&quot; d3 &lt;- c(&quot;August 19 (2015)&quot;,&quot;July 1 (2015)&quot;) parse_date(d3,&quot;%B %d (%Y)&quot;) #&gt; [1] &quot;2015-08-19&quot; &quot;2015-07-01&quot; t1 &lt;- &quot;1705&quot; parse_time(&quot;1705&quot;,&quot;%H%M&quot;) #&gt; 17:05:00 t2 &lt;- &quot;11:15:10.12 PM&quot; parse_time(t2,&quot;%I:%M:%OS %p&quot;) #&gt; 23:15:10.12 "],
["parsing-a-file.html", "3.4 Parsing a file", " 3.4 Parsing a file 现在我们已经学会了如何用 parse_*() 函数族解析单个向量，接下来就能回到本章的最初目标，研究readr是如何解析文件的。我们将关注以下两点： readr如何自动猜出文件每列的数据类型 如何修改默认设置 3.4.1 Strategies readr 通过一种启发式过程(heuristic)来确定每列的类型：先读取文件的前1000行，然后使用（相对保守的）某种启发式算法确定每列的类型。readr 中的导入函数会先用 guess_parser() 函数返回对于所需解析函数最可信的猜测，然后尝试用可能性最大的解析函数解析该列： guess_parser(&quot;123.45&quot;) #&gt; [1] &quot;double&quot; guess_parser(&quot;12,352,561&quot;) #&gt; [1] &quot;number&quot; guess_parser(&quot;1998-11-12&quot;) #&gt; [1] &quot;date&quot; guess_parser(c(&quot;True&quot;, &quot;False&quot;)) #&gt; [1] &quot;logical&quot; 这个过程会依次尝试以下每种数据类型，直到找到匹配的类型。 逻辑值(logical)： 只包括F、T、FALSE和True 整数(integer) 只包括数值型字符（以及-） 双精度浮点数(double) 只包括有效的双精度浮点数 数值(number) 只包括带有分组符号的有效双精度浮点数 时间 与默认的time_format匹配的值 日期 与默认的date_format匹配的值 日期时间 符合ISO 8601标准的任何日期 如果以上数据不符合上述要求中的任意一个，那么这一列就是一个字符串向量，readr将使用parse_character()解析它。 3.4.2 Possible challenges 这些默认设置对更大的文件并不总是有效。以下是两个可能遇到的主要问题： readr 通过前 1000 行猜测数据类型，但是前 1000 行可能只是一种特殊情况，不足以代表整列。例如，一列双精度数值的前1000行有可能都是整数 列中可能包含大量缺失值。如果前 1000 行都是 NA，那么readr 会认为这是一个字符向量，但你其实想将这一类解析为更具体的值。 readr的安装包里包含了一份文件challenge.csv，用来说明解析过程中可能遇到的问题。这个csv文件包含两列x，y和2001行观测。x 列的前 1001 行均为整数，但之后的值均为双精度整数。y 列的前 1001 行均为NA，后面是日期型数据： # readr_example() find path for a built-in readr file challenge &lt;- read_csv(readr_example(&quot;challenge.csv&quot;)) #&gt; Parsed with column specification: #&gt; cols( #&gt; x = col_double(), #&gt; y = col_logical() #&gt; ) 可以看到，read_csv() 成功解析了 x，但对于 y 则无从下手，因为使用了错误的解析函数col_logical()。使用 problems() 函数明确列出这些失败记录，以便深入探究其中的问题： problems(challenge) #&gt; # A tibble: 1,000 x 5 #&gt; row col expected actual file #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1001 y 1/0/T/F/TRUE/~ 2015-01~ &#39;C:/Users/Lenovo/Documents/R/win-library/~ #&gt; 2 1002 y 1/0/T/F/TRUE/~ 2018-05~ &#39;C:/Users/Lenovo/Documents/R/win-library/~ #&gt; 3 1003 y 1/0/T/F/TRUE/~ 2015-09~ &#39;C:/Users/Lenovo/Documents/R/win-library/~ #&gt; 4 1004 y 1/0/T/F/TRUE/~ 2012-11~ &#39;C:/Users/Lenovo/Documents/R/win-library/~ #&gt; 5 1005 y 1/0/T/F/TRUE/~ 2020-01~ &#39;C:/Users/Lenovo/Documents/R/win-library/~ #&gt; 6 1006 y 1/0/T/F/TRUE/~ 2016-04~ &#39;C:/Users/Lenovo/Documents/R/win-library/~ #&gt; # ... with 994 more rows 可以使用spec_csv() 来直接查看 readr 在默认情况下用那种类型的解析函数解析数据： spec_csv(readr_example(&quot;challenge.csv&quot;)) #&gt; cols( #&gt; x = col_double(), #&gt; y = col_logical() #&gt; ) 为了解决这个问题，我们用read_csv()函数中的col_types指定每列的解析方法(column specification)，之前我们向col_types传入一个字符串说明各列的类别，但这里是要直接指明解析函数了。这样做的指定必须通过cols()函数来创建(具体格式和spec_csv()或者read_csv()自动打印的说明是一样的)： challenge &lt;- read_csv(readr_example(&quot;challenge.csv&quot;), col_types =cols( x = col_double(), y = col_date() )) tail(challenge) #&gt; # A tibble: 6 x 2 #&gt; x y #&gt; &lt;dbl&gt; &lt;date&gt; #&gt; 1 0.805 2019-11-21 #&gt; 2 0.164 2018-03-29 #&gt; 3 0.472 2014-08-04 #&gt; 4 0.718 2015-08-16 #&gt; 5 0.270 2020-02-04 #&gt; 6 0.608 2019-01-06 每个parse_*()函数都有一个对应的col_*()函数。如果数据已经保存在R的字符向量中，那么可以使用parse_*()，如果要告诉readr如何加载数据，则应该使用col_*()。 The available specifications are: (with string abbreviations in brackets) col_logical() [l], containing only T, F, TRUE or FALSE. col_integer() [i], integers. col_double() [d], doubles. col_character() [c], everything else. col_factor(levels, ordered) [f], a fixed set of values. col_date(format = \"\") [D]: with the locale’s date_format. col_time(format = \"\") [t]: with the locale’s time_format. col_datetime(format = \"\") [T]: ISO8601 date times col_number() [n], numbers containing the grouping_mark col_skip() [_, -], don’t import this column. col_guess() [?], parse using the “best” type based on the input. cols_only()代替cols()可以仅指定部分列的解析方式；.default表示未提及的所有列(read_csv()的默认设置可以表示为read_csv( col_type = cols(.default = col_guess())) 一旦我们指定了正确的解析函数，问题便迎刃而解。 3.4.3 Other tips 我们再介绍其他几种有注意解析文件的通用技巧： 在前面的示例中，如果比默认方式再多检查一行，就可以解析成功： challenge &lt;- read_csv(readr_example(&quot;challenge.csv&quot;), guess_max = 1001) type_convert() re-convert character columns in existing data frame. This is useful if you need to do some manual munging - you can read the columns in as character, clean it up with (e.g.) regular expressions and then let readr take another stab at parsing it. df &lt;- tibble( x = as.character(runif(10)), y = as.character(sample(10)), z = FALSE ) df %&gt;% glimpse() #&gt; Rows: 10 #&gt; Columns: 3 #&gt; $ x &lt;chr&gt; &quot;0.78455981053412&quot;, &quot;0.525363321648911&quot;, &quot;0.336503465892747&quot;, &quot;0.... #&gt; $ y &lt;chr&gt; &quot;2&quot;, &quot;10&quot;, &quot;1&quot;, &quot;6&quot;, &quot;8&quot;, &quot;7&quot;, &quot;4&quot;, &quot;5&quot;, &quot;3&quot;, &quot;9&quot; #&gt; $ z &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE # note changes in column types df %&gt;% type_convert() %&gt;% glimpse() #&gt; Parsed with column specification: #&gt; cols( #&gt; x = col_double(), #&gt; y = col_double() #&gt; ) #&gt; Rows: 10 #&gt; Columns: 3 #&gt; $ x &lt;dbl&gt; 0.7846, 0.5254, 0.3365, 0.5136, 0.5227, 0.5568, 0.7613, 0.5780, 0... #&gt; $ y &lt;dbl&gt; 2, 10, 1, 6, 8, 7, 4, 5, 3, 9 #&gt; $ z &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE 如果正在读取一个非常大的文件，那么应该将 n_max 设置为一个较小的数，比如10,000 或者 100,000，这可以让加速重复试验的过程。 3.4.4 Example: Dealing with metadata https://alison.rbind.io/post/2018-02-23-read-multiple-header-rows/ This dataset is from an article published in PLOS ONE called “Being Sticker Rich: Numerical Context Influences Children’s Sharing Behavior”. In this study, children (ages 3–11) received a small (12, “sticker poor”) or large (30, “sticker rich”) number of stickers, and were then given the opportunity to share their windfall with either one or multiple anonymous recipients. Data in a plain text editor: To read in a .tab file, use read_tsv() # https://dataverse.harvard.edu/api/access/datafile/2712105 stickers &lt;- read_tsv(&quot;data/sticker.tab&quot;) spec(stickers) #&gt; cols( #&gt; SubjectNumber = col_character(), #&gt; Condition = col_character(), #&gt; NumberStickers = col_character(), #&gt; NumberEnvelopes = col_character(), #&gt; Gender = col_character(), #&gt; Agemonths = col_double(), #&gt; Ageyears = col_double(), #&gt; Agegroups = col_character(), #&gt; `Subject&#39;sEnvelope` = col_character(), #&gt; LeftEnvelope = col_character(), #&gt; RightEnvelope = col_character(), #&gt; `absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)` = col_character(), #&gt; `PercentGiven(Outof100percent)` = col_character(), #&gt; Giveornot = col_character(), #&gt; LargerEnvelopeabs = col_character(), #&gt; LargeEnvelopepercent = col_character(), #&gt; SmallerEnvelopeabs = col_character(), #&gt; SmallEnvelopepercent = col_character() #&gt; ) The problem here is that the second row is actually metadata or descriptions about each column header. But this make read_tsv() recognize as character type many of our columns. To verify this, see the first and last 6 rows: head(stickers) #&gt; # A tibble: 6 x 18 #&gt; SubjectNumber Condition NumberStickers NumberEnvelopes Gender Agemonths #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 [Included Sa~ 1=12:1; ~ 1=12; 2=30 1=1 recipient;~ 1=fem~ NA #&gt; 2 1 1 1 1 1 36 #&gt; 3 2 1 1 1 2 36 #&gt; 4 3 1 1 1 2 36 #&gt; 5 4 1 1 1 1 36 #&gt; 6 5 1 1 1 2 36 #&gt; # ... with 12 more variables: Ageyears &lt;dbl&gt;, Agegroups &lt;chr&gt;, #&gt; # `Subject&#39;sEnvelope` &lt;chr&gt;, LeftEnvelope &lt;chr&gt;, RightEnvelope &lt;chr&gt;, #&gt; # `absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)` &lt;chr&gt;, #&gt; # `PercentGiven(Outof100percent)` &lt;chr&gt;, Giveornot &lt;chr&gt;, #&gt; # LargerEnvelopeabs &lt;chr&gt;, LargeEnvelopepercent &lt;chr&gt;, #&gt; # SmallerEnvelopeabs &lt;chr&gt;, SmallEnvelopepercent &lt;chr&gt; tail(stickers) #&gt; # A tibble: 6 x 18 #&gt; SubjectNumber Condition NumberStickers NumberEnvelopes Gender Agemonths #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 396 1 1 1 2 136 #&gt; 2 397 4 2 2 1 136 #&gt; 3 398 1 1 1 1 137 #&gt; 4 399 1 1 1 2 137 #&gt; 5 400 4 2 2 2 139 #&gt; 6 401 3 2 1 1 143 #&gt; # ... with 12 more variables: Ageyears &lt;dbl&gt;, Agegroups &lt;chr&gt;, #&gt; # `Subject&#39;sEnvelope` &lt;chr&gt;, LeftEnvelope &lt;chr&gt;, RightEnvelope &lt;chr&gt;, #&gt; # `absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)` &lt;chr&gt;, #&gt; # `PercentGiven(Outof100percent)` &lt;chr&gt;, Giveornot &lt;chr&gt;, #&gt; # LargerEnvelopeabs &lt;chr&gt;, LargeEnvelopepercent &lt;chr&gt;, #&gt; # SmallerEnvelopeabs &lt;chr&gt;, SmallEnvelopepercent &lt;chr&gt; To solve this, we will first create a (tidier) character vector of the column names only. Then we’ll read in the actual data and skip the multiple header rows at the top. When we do this, we lose the column names, so we use the character vector of column names we created in the first place instead. When we set n_max = 0 and col_names = TRUE(the default), ony column headers will be read. Then we could ask janitor::clean_names() to produce a tidier set of names: sticker_names &lt;- read_tsv(&quot;data/sticker.tab&quot;, n_max = 0) %&gt;% # default: col_names = TRUE rename(&quot;stickers_give&quot; = &#39;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)&#39;) %&gt;% janitor::clean_names() %&gt;% names() sticker_names #&gt; [1] &quot;subject_number&quot; &quot;condition&quot; #&gt; [3] &quot;number_stickers&quot; &quot;number_envelopes&quot; #&gt; [5] &quot;gender&quot; &quot;agemonths&quot; #&gt; [7] &quot;ageyears&quot; &quot;agegroups&quot; #&gt; [9] &quot;subjects_envelope&quot; &quot;left_envelope&quot; #&gt; [11] &quot;right_envelope&quot; &quot;stickers_give&quot; #&gt; [13] &quot;percent_given_outof100percent&quot; &quot;giveornot&quot; #&gt; [15] &quot;larger_envelopeabs&quot; &quot;large_envelopepercent&quot; #&gt; [17] &quot;smaller_envelopeabs&quot; &quot;small_envelopepercent&quot; sticker_tidy &lt;- read_tsv(&quot;data/sticker.tab&quot;, col_names = sticker_names, skip = 2) # or read_tsv(&quot;data/sticker.tab&quot;, col_names = FALSE, skip = 2) %&gt;% set_names(sticker_names) #&gt; # A tibble: 401 x 18 #&gt; subject_number condition number_stickers number_envelopes gender agemonths #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 1 1 1 36 #&gt; 2 2 1 1 1 2 36 #&gt; 3 3 1 1 1 2 36 #&gt; 4 4 1 1 1 1 36 #&gt; 5 5 1 1 1 2 36 #&gt; 6 6 1 1 1 2 36 #&gt; # ... with 395 more rows, and 12 more variables: ageyears &lt;dbl&gt;, #&gt; # agegroups &lt;dbl&gt;, subjects_envelope &lt;dbl&gt;, left_envelope &lt;dbl&gt;, #&gt; # right_envelope &lt;dbl&gt;, stickers_give &lt;dbl&gt;, #&gt; # percent_given_outof100percent &lt;dbl&gt;, giveornot &lt;dbl&gt;, #&gt; # larger_envelopeabs &lt;dbl&gt;, large_envelopepercent &lt;dbl&gt;, #&gt; # smaller_envelopeabs &lt;dbl&gt;, small_envelopepercent &lt;dbl&gt; What if we want to include that meta data? Create a variable description column use pivot_longer() sticker_dict &lt;- read_tsv(&quot;data/sticker.tab&quot;, n_max = 1) %&gt;% rename(stickersgiven = &#39;absolutenumberofstickersgiven(Conditions1or3:Outof12;Conditions2or4:Outof30)&#39;) %&gt;% janitor::clean_names() %&gt;% pivot_longer(everything(), names_to = &quot;variable_name&quot;, values_to = &quot;variable_description&quot;) sticker_dict #&gt; # A tibble: 18 x 2 #&gt; variable_name variable_description #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 subject_number [Included Sample Only] #&gt; 2 condition 1=12:1; 2=12:2, 3=30:1, 4=30:2 #&gt; 3 number_stickers 1=12; 2=30 #&gt; 4 number_envelopes 1=1 recipient; 2=2 recipients #&gt; 5 gender 1=female; 2=male #&gt; 6 agemonths &lt;NA&gt; #&gt; # ... with 12 more rows 3.4.5 Example: multi-row headers https://debruine.github.io/posts/multi-row-headers/ In most cases, a header will only take up one row. And when multi-row headers present itself, readr fails to recognize all these rows as header # create a small demo csv demo_csv &lt;- &quot;SUB1, SUB1, SUB1, SUB1, SUB2, SUB2, SUB2, SUB2 COND1, COND1, COND2, COND2, COND1, COND1, COND2, COND2 X, Y, X, Y, X, Y, X, Y 10, 15, 6, 2, 42, 4, 32, 5 4, 43, 7, 34, 56, 43, 2, 33 77, 12, 14, 75, 36, 85, 3, 2&quot; read_csv(demo_csv) #&gt; Warning: Duplicated column names deduplicated: &#39;SUB1&#39; =&gt; &#39;SUB1_1&#39; [2], &#39;SUB1&#39; #&gt; =&gt; &#39;SUB1_2&#39; [3], &#39;SUB1&#39; =&gt; &#39;SUB1_3&#39; [4], &#39;SUB2&#39; =&gt; &#39;SUB2_1&#39; [6], &#39;SUB2&#39; =&gt; #&gt; &#39;SUB2_2&#39; [7], &#39;SUB2&#39; =&gt; &#39;SUB2_3&#39; [8] #&gt; # A tibble: 5 x 8 #&gt; SUB1 SUB1_1 SUB1_2 SUB1_3 SUB2 SUB2_1 SUB2_2 SUB2_3 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 COND1 COND1 COND2 COND2 COND1 COND1 COND2 COND2 #&gt; 2 X Y X Y X Y X Y #&gt; 3 10 15 6 2 42 4 32 5 #&gt; 4 4 43 7 34 56 43 2 33 #&gt; 5 77 12 14 75 36 85 3 2 In demo_csv, the first three rows are intended to be an “overall” header. read_csv() outputs a message and modifes the first row for a unqiue set of column names, since it assumes this is a one-row header. Based on what we have learned in the previous example, the solution should be easy. First set n_max to gain rows of interest, then compose a one-row header to set column names. demo_names &lt;- read_csv(demo_csv, n_max = 3, col_names = FALSE) %&gt;% map_chr(~ str_c(.x, collapse = &quot;&quot;)) %&gt;% unname() demo_names #&gt; [1] &quot;SUB1COND1X&quot; &quot;SUB1COND1Y&quot; &quot;SUB1COND2X&quot; &quot;SUB1COND2Y&quot; &quot;SUB2COND1X&quot; #&gt; [6] &quot;SUB2COND1Y&quot; &quot;SUB2COND2X&quot; &quot;SUB2COND2Y&quot; read_csv(demo_csv, col_names = demo_names, skip = 3) #&gt; # A tibble: 3 x 8 #&gt; SUB1COND1X SUB1COND1Y SUB1COND2X SUB1COND2Y SUB2COND1X SUB2COND1Y SUB2COND2X #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 10 15 6 2 42 4 32 #&gt; 2 4 43 7 34 56 43 2 #&gt; 3 77 12 14 75 36 85 3 #&gt; # ... with 1 more variable: SUB2COND2Y &lt;dbl&gt; For reading multi-row headers in Excel, check section 3.5.1 "],
["readxl.html", "3.5 readxl", " 3.5 readxl 要想将其他类型的数据导入 R 中，可以先从下列的 tidyverse 包开始。对矩形型数据来说： haven 可以读取 SPSS、Stata 和 SAS文件: 配合专用的数据库后端程序（如 RMySQL，RSQLite，RpostgreSQL等），DBI可以对相应数据库进行SQL查询，并返回一个数据框 readxl 专门为读取 Excel 文件打造(.xlsx和xls均可) 下面主要介绍 readxl 的用法，不同于readr，readxl 不是 tidyverse 的核心包，我们总需要显示地加载它： library(readxl) read_excel() 是 readxl 中的核心函数，它的第一个参数path接受 xlsx 或 xls 文件的路径。但由于一个 Excel 文件（工作簿）经常包含多个工作表，所以我们在读取时需要指明某张工作表，excel_sheets() 函数返回一个工作簿文件中各个工作表(sheet)的名字： path &lt;- readxl_example(&quot;deaths.xlsx&quot;) excel_sheets(path) #&gt; [1] &quot;arts&quot; &quot;other&quot; 获悉一个工作簿内部的表结构以后，可以使用 sheets 参数指定要读取的表，可以是一个整数（第几张彪），也可以是字符串（表明）, read_excel() 默认读取第一张表： deaths &lt;- read_excel(path, sheet = &quot;arts&quot;) 经常会在读取 Excel 文件时遇到的一个问题是,有些人喜欢在表格的前面几行或最后几行添加一些元数据（metadata），比如在 deaths.xlsx 中： 默认设置下，read_excel() 会将这些元数据一并读入： deaths #&gt; # A tibble: 18 x 6 #&gt; `Lots of people` ...2 ...3 ...4 ...5 ...6 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 simply cannot resist writ~ &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; some notes #&gt; 2 at the top &lt;NA&gt; of their spreadsh~ #&gt; 3 or merging &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; cells #&gt; 4 Name Professi~ Age Has ki~ Date of bi~ Date of death #&gt; 5 David Bowie musician 69 TRUE 17175 42379 #&gt; 6 Carrie Fisher actor 60 TRUE 20749 42731 #&gt; # ... with 12 more rows 一个很有用的方法是通过 Sstudio 的 File \\(\\rightarrow\\) Import Dataset \\(\\rightarrow\\) From Excel 接口导入数据，我们可以通过预览来观察导入后的数据，还可以对read_excel()的导入参数进行设置： 对于deaths.xlsx这个Excel文件，可以结合使用n_max 和 skip 参数去除不想读取的部分（分别对应图形界面里的Max Rows和Skip： read_excel(path, skip= 4, n_max = 10) #&gt; # A tibble: 10 x 6 #&gt; Name Profession Age `Has kids` `Date of birth` `Date of death` #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dttm&gt; &lt;dttm&gt; #&gt; 1 David Bow~ musician 69 TRUE 1947-01-08 00:00:00 2016-01-10 00:00:00 #&gt; 2 Carrie Fi~ actor 60 TRUE 1956-10-21 00:00:00 2016-12-27 00:00:00 #&gt; 3 Chuck Ber~ musician 90 TRUE 1926-10-18 00:00:00 2017-03-18 00:00:00 #&gt; 4 Bill Paxt~ actor 61 TRUE 1955-05-17 00:00:00 2017-02-25 00:00:00 #&gt; 5 Prince musician 57 TRUE 1958-06-07 00:00:00 2016-04-21 00:00:00 #&gt; 6 Alan Rick~ actor 69 FALSE 1946-02-21 00:00:00 2016-01-14 00:00:00 #&gt; # ... with 4 more rows read_excel() 中另一个很有用的参数是 range，用于指定一块 Excel 表中要读取的区域： read_excel(path, sheet = &quot;arts&quot;, range = &quot;A5:F15&quot;) #&gt; # A tibble: 10 x 6 #&gt; Name Profession Age `Has kids` `Date of birth` `Date of death` #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dttm&gt; &lt;dttm&gt; #&gt; 1 David Bow~ musician 69 TRUE 1947-01-08 00:00:00 2016-01-10 00:00:00 #&gt; 2 Carrie Fi~ actor 60 TRUE 1956-10-21 00:00:00 2016-12-27 00:00:00 #&gt; 3 Chuck Ber~ musician 90 TRUE 1926-10-18 00:00:00 2017-03-18 00:00:00 #&gt; 4 Bill Paxt~ actor 61 TRUE 1955-05-17 00:00:00 2017-02-25 00:00:00 #&gt; 5 Prince musician 57 TRUE 1958-06-07 00:00:00 2016-04-21 00:00:00 #&gt; 6 Alan Rick~ actor 69 FALSE 1946-02-21 00:00:00 2016-01-14 00:00:00 #&gt; # ... with 4 more rows ## 还可以在range中指定列名 read_excel(path, range = &quot;arts!A5:F15&quot;) #&gt; # A tibble: 10 x 6 #&gt; Name Profession Age `Has kids` `Date of birth` `Date of death` #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dttm&gt; &lt;dttm&gt; #&gt; 1 David Bow~ musician 69 TRUE 1947-01-08 00:00:00 2016-01-10 00:00:00 #&gt; 2 Carrie Fi~ actor 60 TRUE 1956-10-21 00:00:00 2016-12-27 00:00:00 #&gt; 3 Chuck Ber~ musician 90 TRUE 1926-10-18 00:00:00 2017-03-18 00:00:00 #&gt; 4 Bill Paxt~ actor 61 TRUE 1955-05-17 00:00:00 2017-02-25 00:00:00 #&gt; 5 Prince musician 57 TRUE 1958-06-07 00:00:00 2016-04-21 00:00:00 #&gt; 6 Alan Rick~ actor 69 FALSE 1946-02-21 00:00:00 2016-01-14 00:00:00 #&gt; # ... with 4 more rows 与 range 相关的帮助函数cell_rows()、cell_cols()和cell_limits()可以为区域选择提供更大的自由度，下面的示例中使用文件geometry.xls，读取预览： path &lt;- readxl_example(&quot;geometry.xls&quot;) # Specify only the rows or only the columns read_excel(path, range = cell_rows(3:6)) #&gt; # A tibble: 3 x 3 #&gt; B3 C3 D3 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 B4 C4 D4 #&gt; 2 B5 C5 D5 #&gt; 3 B6 C6 D6 read_excel(path, range = cell_cols(&quot;C:D&quot;)) #&gt; # A tibble: 3 x 2 #&gt; C3 D3 #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 C4 D4 #&gt; 2 C5 D5 #&gt; 3 C6 D6 read_excel(path, range = cell_cols(2)) #&gt; # A tibble: 3 x 1 #&gt; B3 #&gt; &lt;chr&gt; #&gt; 1 B4 #&gt; 2 B5 #&gt; 3 B6 # Specify exactly one row or column bound read_excel(path, range = cell_rows(c(5, NA))) #&gt; # A tibble: 1 x 3 #&gt; B5 C5 D5 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 B6 C6 D6 read_excel(path, range = cell_rows(c(NA, 4))) #&gt; # A tibble: 3 x 3 #&gt; ...1 ...2 ...3 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 2 B3 C3 D3 #&gt; 3 B4 C4 D4 read_excel(path, range = cell_cols(c(&quot;C&quot;, NA))) #&gt; # A tibble: 3 x 2 #&gt; C3 D3 #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 C4 D4 #&gt; 2 C5 D5 #&gt; 3 C6 D6 read_excel(path, range = cell_cols(c(NA, 2))) #&gt; # A tibble: 3 x 2 #&gt; ...1 B3 #&gt; &lt;lgl&gt; &lt;chr&gt; #&gt; 1 NA B4 #&gt; 2 NA B5 #&gt; 3 NA B6 # General open rectangles # upper left = C4, everything else unspecified read_excel(path, range = cell_limits(c(4, 3), c(NA, NA))) #&gt; # A tibble: 2 x 2 #&gt; C4 D4 #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 C5 D5 #&gt; 2 C6 D6 # upper right = D4, everything else unspecified read_excel(path, range = cell_limits(c(4, NA), c(NA, 4))) #&gt; # A tibble: 2 x 4 #&gt; ...1 B4 C4 D4 #&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 NA B5 C5 D5 #&gt; 2 NA B6 C6 D6 3.5.1 Multi-row headers in Excel https://debruine.github.io/posts/multi-row-headers/ In section 3.4.5 (Example: multi-row headers) we learn how to tackle with multi-row-headers in a text File. In Excel this is trickier. But the ultimate goal is always firrst extracting header rows (more than 1), composing one formatted row header and then set it to col_names when importing, and skip the some rows in the original data. Extracting head rows data_head &lt;- read_excel(&quot;data/3headers_demo.xlsx&quot;, col_names = FALSE, n_max = 3) #&gt; New names: #&gt; * `` -&gt; ...1 #&gt; * `` -&gt; ...2 #&gt; * `` -&gt; ...3 #&gt; * `` -&gt; ...4 #&gt; * `` -&gt; ...5 #&gt; * ... data_head %&gt;% slider::slide(~ as.vector(fill(.x, .direction = &quot;down&quot;))) #&gt; [[1]] #&gt; # A tibble: 1 x 8 #&gt; ...1 ...2 ...3 ...4 ...5 ...6 ...7 ...8 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 SUB1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; SUB2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; #&gt; [[2]] #&gt; # A tibble: 1 x 8 #&gt; ...1 ...2 ...3 ...4 ...5 ...6 ...7 ...8 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 COND1 &lt;NA&gt; COND2 &lt;NA&gt; COND1 &lt;NA&gt; COND2 &lt;NA&gt; #&gt; #&gt; [[3]] #&gt; # A tibble: 1 x 8 #&gt; ...1 ...2 ...3 ...4 ...5 ...6 ...7 ...8 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 X Y X Y X Y X Y Currently I have come up with no ways of dealing with data_hand in this given form without resorting to the weird t(). So I followed the blog and transpose it to take advantage of tidyr::fill(). Yet I feel that there is some function in the slider package could solve this. I will look into this and may update the solution. names &lt;- data_head %&gt;% t() %&gt;% as_tibble() %&gt;% fill(1:3, .direction = &quot;down&quot;) %&gt;% # from tidyr mutate(names = str_c(V1, V2, V3, sep = &quot;_&quot;)) %&gt;% pull(names) read_excel(&quot;data/3headers_demo.xlsx&quot;, col_names = names, skip = 3) #&gt; # A tibble: 6 x 8 #&gt; SUB1_COND1_X SUB1_COND1_Y SUB1_COND2_X SUB1_COND2_Y SUB2_COND1_X SUB2_COND1_Y #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.832 0.788 0.394 0.206 0.933 0.153 #&gt; 2 0.415 0.137 0.981 0.749 0.105 0.657 #&gt; 3 0.558 0.0956 0.305 0.354 0.362 0.846 #&gt; 4 0.433 0.828 0.285 0.624 0.0439 0.538 #&gt; 5 0.655 0.650 0.920 0.253 0.812 0.346 #&gt; 6 0.0679 0.698 0.398 0.692 0.528 0.109 #&gt; # ... with 2 more variables: SUB2_COND2_X &lt;dbl&gt;, SUB2_COND2_Y &lt;dbl&gt; "],
["lubridate-dates-and-times.html", "4 lubridate: Dates and times", " 4 lubridate: Dates and times # lubridate has to be manually loaded library(lubridate) library(nycflights13) library(patchwork) "],
["creating-dates-and-times.html", "4.1 Creating dates and times", " 4.1 Creating dates and times There are generally 3 types of date / time data : date: Often specified by year, month and day. Tibble prints this as &lt;date&gt; time：A time within a day, specified by hour, minutes and seconds. Tibble prints this as &lt;time&gt; date-time: is a date plus a time. Tibbles print this as &lt;dttm&gt;. Elsewhere in R these are called POSIXct 如果能够满足需要，就应该使用最简单的数据类型。这意味着只要能够使用日期型数据，那么就不应该使用日期时间型数据。 要想得到当前日期或当前时期时间，可以使用 today() 或 now() ： today() #&gt; [1] &quot;2020-04-29&quot; now() #&gt; [1] &quot;2020-04-29 15:14:49 CST&quot; 除此之外，以下 3 种方法也可以创建日期或时间： 通过字符串创建 通过日期时间的各个成分创建 通过现有的日期时间对象创建 4.1.1 From strings 日期时间数据经常用字符串表示。在事先知晓各个组成部分顺序的前提下，通过 lubridate 中的一些辅助函数，可以轻松将字符串转换为日期时间格式。因为要想使用函数，需要先确定年、月、日在日期数据中的顺序，然后按照同样的顺讯排列字母 y、m、d，这样就可以组成能够创建日期格式的 lubridate 函数名称，例如： ymd(&quot;2017-03-01&quot;) #&gt; [1] &quot;2017-03-01&quot; mdy(&quot;January 1st,2017&quot;) #&gt; [1] &quot;2017-01-01&quot; dmy(&quot;31-Jan-2017&quot;) #&gt; [1] &quot;2017-01-31&quot; 这些函数也可以接受不带引号的数值，这是创建单个时期时间对象的最简单的方法。在筛选日期时间数据时，就可以使用这种方法： ymd(20190731) #&gt; [1] &quot;2019-07-31&quot; ymd() 和其他类似函数可以创建日期数据。想要创建日期时间型数据，可以在后面加一个下划线，以及h、m、s之中的一个或多个字母（依然要遵循顺序），这样就可以得到解析日期时间数据的函数了： ymd_hms(&quot;2017-01-31 20:11:59&quot;) #&gt; [1] &quot;2017-01-31 20:11:59 UTC&quot; mdy_hm(&quot;01/31/2017 08:01&quot;) #&gt; [1] &quot;2017-01-31 08:01:00 UTC&quot; 如果用类似函数尝试解析包含无效内容的字符串，将会返回 NA ： ymd(&quot;2010-10-10&quot;, &quot;bananas&quot;) #&gt; [1] &quot;2010-10-10&quot; NA 通过添加一个时区参数，可以将一个时期强制转换为日期时间： ## 这是一个日期时间型变量 ymd(20170131, tz = &quot;UTC&quot;) #&gt; [1] &quot;2017-01-31 UTC&quot; 4.1.2 From individual components To create a date/time from this sort of input, usemake_date() for dates, or make_datetime() for date-times. Input vectors are silently recycled: make_datetime(year = 1999, month = 12, day = 22, sec = c(10, 11)) #&gt; [1] &quot;1999-12-22 00:00:10 UTC&quot; &quot;1999-12-22 00:00:11 UTC&quot; This is useful when individual components of data / time is seprated across multiple columns, as in flights: flights %&gt;% select(year, month, day, hour, minute) %&gt;% mutate(departure = make_datetime(year = year, month = month, day = day, hour = hour, min = minute)) #&gt; # A tibble: 336,776 x 6 #&gt; year month day hour minute departure #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; #&gt; 1 2013 1 1 5 15 2013-01-01 05:15:00 #&gt; 2 2013 1 1 5 29 2013-01-01 05:29:00 #&gt; 3 2013 1 1 5 40 2013-01-01 05:40:00 #&gt; 4 2013 1 1 5 45 2013-01-01 05:45:00 #&gt; 5 2013 1 1 6 0 2013-01-01 06:00:00 #&gt; 6 2013 1 1 5 58 2013-01-01 05:58:00 #&gt; # ... with 336,770 more rows sec in make_datetime() is unassigned so it defualts to base level 0. This is also how make_date() workds make_date(year = 2020, day = 20) #&gt; [1] &quot;2020-01-20&quot; make_date(month = 12, day = 20) # year starts in 1970 #&gt; [1] &quot;1970-12-20&quot; flights 中 hour 和 time 均是航班起飞时间的预计值。为了算出实际起飞、到达时间，我们需要使用 dep_time 和 arr_time 这两个变量，不过，它们同时包括了小时和分钟数： flights %&gt;% select(dep_time, arr_time, sched_dep_time, sched_arr_time) #&gt; # A tibble: 336,776 x 4 #&gt; dep_time arr_time sched_dep_time sched_arr_time #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 517 830 515 819 #&gt; 2 533 850 529 830 #&gt; 3 542 923 540 850 #&gt; 4 544 1004 545 1022 #&gt; 5 554 812 600 837 #&gt; 6 554 740 558 728 #&gt; # ... with 336,770 more rows 为了创建出表示实际出发和到达时间的日期时间型数据，我们首先编写一个函数以使make_datetime函数适应dep_time和arr_time这种比较奇怪的表示方式，思想是使用模运算将小时成分与分钟成分分离。一旦创建了日期时间变量，我们就在本章剩余部分使用这些变量进行讨论： make_datetime_100 &lt;- function(year, month, day, time) { hour = time %/% 100 minute = time %% 100 make_datetime(year, month, day, hour, minute) } (flights_dt &lt;- flights %&gt;% filter(!is.na(dep_time), !is.na(arr_time)) %&gt;% mutate( dep_time = make_datetime_100(year, month, day, dep_time), arr_time = make_datetime_100(year, month, day, arr_time), sched_dep_time = make_datetime_100(year, month, day, sched_dep_time), sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)) %&gt;% select(origin,dest,ends_with(&quot;delay&quot;), ends_with(&quot;time&quot;)) ) #&gt; # A tibble: 328,063 x 9 #&gt; origin dest dep_delay arr_delay dep_time sched_dep_time #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dttm&gt; #&gt; 1 EWR IAH 2 11 2013-01-01 05:17:00 2013-01-01 05:15:00 #&gt; 2 LGA IAH 4 20 2013-01-01 05:33:00 2013-01-01 05:29:00 #&gt; 3 JFK MIA 2 33 2013-01-01 05:42:00 2013-01-01 05:40:00 #&gt; 4 JFK BQN -1 -18 2013-01-01 05:44:00 2013-01-01 05:45:00 #&gt; 5 LGA ATL -6 -25 2013-01-01 05:54:00 2013-01-01 06:00:00 #&gt; 6 EWR ORD -4 12 2013-01-01 05:54:00 2013-01-01 05:58:00 #&gt; # ... with 328,057 more rows, and 3 more variables: arr_time &lt;dttm&gt;, #&gt; # sched_arr_time &lt;dttm&gt;, air_time &lt;dbl&gt; 我们还可以使用这些数据做出一年间出发时间或某一天内出发时间的可视化分布（精确到分钟）。注意在直方图的分箱宽度中，日期时间数据的单位是秒，而日期数据则是天 ## 一年内起飞时间的分布 flights_dt %&gt;% ggplot() + geom_freqpoly(aes(x = dep_time),binwidth = 86400) ## 86000秒= 1天 ## 1月1日起飞时间的分布 flights_dt %&gt;% filter(dep_time &lt; ymd(20130102)) %&gt;% ggplot(aes(x=dep_time))+ geom_freqpoly(binwidth = 600) ## 600秒 = 10分钟 4.1.3 From other times You may want to switch between a date-time and a date. That’s the job of as_datetime() and as_date(): today() %&gt;% as_datetime() #&gt; [1] &quot;2020-04-29 UTC&quot; now() %&gt;% as_date() #&gt; [1] &quot;2020-04-29&quot; Sometimes you’ll get date/times as numeric offsets from the “Unix Epoch”, 1970-01-01. If the offset is in seconds, use as_datetime(); if it’s in days, use as_date(). # 1 day as_datetime(60 * 60 * 25) #&gt; [1] &quot;1970-01-02 01:00:00 UTC&quot; as_date(1) #&gt; [1] &quot;1970-01-02&quot; 4.1.4 Exercises Exercise 1.1 What happens if you parse a string that contains invalid dates? If returns NA and throws a warning: ymd(c(&quot;2010-10-10&quot;, &quot;bananas&quot;)) #&gt; Warning: 1 failed to parse. #&gt; [1] &quot;2010-10-10&quot; NA Exercise 4.1 Use the appropriate lubridate function to parse each of the following dates: d1 &lt;- &quot;January 1,2010&quot; mdy(d1) #&gt; [1] &quot;2010-01-01&quot; d2 &lt;- &quot;2015-Mar-07&quot; ymd(d2) #&gt; [1] &quot;2015-03-07&quot; d3 &lt;- &quot;06-Jun-2017&quot; dmy(d3) #&gt; [1] &quot;2017-06-06&quot; d4 &lt;- c(&quot;August 19 (2015)&quot;,&quot;July 1 (2015)&quot;) mdy(d4) #&gt; [1] &quot;2015-08-19&quot; &quot;2015-07-01&quot; d5 &lt;- &quot;12/30/14&quot; # 2014年12月30日 mdy(d5) #&gt; [1] &quot;2014-12-30&quot; "],
["date-time-components.html", "4.2 Date-time components", " 4.2 Date-time components This section will focus on the accessor functions that let you get and set individual components of a date / datetime. 4.2.1 Accessing components To pull out individual parts of the date with the accessor functions, use： year(), month(), mday()(day of month), yday()(day of year), wday()(day of week), hour(), minute(), second() ： datetime &lt;- ymd_hms(&quot;2016-07-08 12:34:56&quot;) year(datetime) #&gt; [1] 2016 month(datetime) #&gt; [1] 7 mday(datetime) #&gt; [1] 8 yday(datetime) #&gt; [1] 190 wday(datetime) #&gt; [1] 6 hour(datetime) #&gt; [1] 12 minute(datetime) #&gt; [1] 34 second(datetime) #&gt; [1] 56 For month() and wday() you can set label = TRUE to return the abbreviated name of the month or day of the week and convert it to a factor. Set abbr = FALSE to return the full name. This is useful when plotting in ggplot2 because you want a certain order month(datetime, label = T) #&gt; [1] Jul #&gt; 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec wday(datetime, label = T, abbr = F) #&gt; [1] Friday #&gt; 7 Levels: Sunday &lt; Monday &lt; Tuesday &lt; Wednesday &lt; Thursday &lt; ... &lt; Saturday 通过 wday()函数，我们可以知道在工作日出发的航班要多于周末出发的航班： flights_dt %&gt;% mutate(weekday = wday(dep_time, label = T)) %&gt;% ggplot(aes(weekday)) + geom_bar() 再看一个使用 minute() 函数获取分钟成分的例子。比如我们想知道出发时间的分钟数与平均到达延误时间的关系： flights_dt %&gt;% mutate(minute = minute(dep_time)) %&gt;% group_by(minute) %&gt;% summarize(avg_delay = mean(arr_delay, na.rm = T)) %&gt;% ggplot(aes(minute, avg_delay))+ geom_line() 我们可以发现一个有趣的趋势，似乎在 20 ~ 30 分钟和第 50 ~ 60 分钟出发的航班的到达延误时间远远低于其他时间出发的航班。 4.2.2 Rounding An alternative approach to plotting individual components is to round the date to a nearby unit of time, with round_date(), floor_date() and ceiling_date(). Each function takes a vector of dates to adjust and then the name of the unit round down (floor), round up (ceiling), or round to. This, for example, allows us to plot the number of flights per week: flights_dt %&gt;% transmute(dep_time, week = floor_date(dep_time, &quot;week&quot;)) %&gt;% ggplot(aes(week))+ geom_bar() Note that unlike accessor functions, rounding functions still return a complte time unit, not individual components. More examples: x &lt;- ymd_hms(&quot;2009-08-03 12:01:59.23&quot;) round_date(x, &quot;.5s&quot;) #&gt; [1] &quot;2009-08-03 12:01:59 UTC&quot; round_date(x, &quot;sec&quot;) #&gt; [1] &quot;2009-08-03 12:01:59 UTC&quot; round_date(x, &quot;second&quot;) #&gt; [1] &quot;2009-08-03 12:01:59 UTC&quot; round_date(x, &quot;minute&quot;) #&gt; [1] &quot;2009-08-03 12:02:00 UTC&quot; round_date(x, &quot;5 mins&quot;) #&gt; [1] &quot;2009-08-03 12:00:00 UTC&quot; round_date(x, &quot;hour&quot;) #&gt; [1] &quot;2009-08-03 12:00:00 UTC&quot; round_date(x, &quot;2 hours&quot;) #&gt; [1] &quot;2009-08-03 12:00:00 UTC&quot; round_date(x, &quot;day&quot;) #&gt; [1] &quot;2009-08-04 UTC&quot; round_date(x, &quot;week&quot;) #&gt; [1] &quot;2009-08-02 UTC&quot; round_date(x, &quot;month&quot;) #&gt; [1] &quot;2009-08-01 UTC&quot; round_date(x, &quot;bimonth&quot;) ## 舍入到1月、3月、5月、7月、9月和11月上 #&gt; [1] &quot;2009-09-01 UTC&quot; round_date(x, &quot;quarter&quot;) == round_date(x, &quot;3 months&quot;) #&gt; [1] TRUE round_date(x, &quot;halfyear&quot;) #&gt; [1] &quot;2009-07-01 UTC&quot; round_date(x, &quot;year&quot;) #&gt; [1] &quot;2010-01-01 UTC&quot; 4.2.3 Setting components You can also use each accessor function to set the components of a date/time:： datetime &lt;- ymd_hms(&quot;2016-07-08,12:34:56&quot;) year(datetime) &lt;- 2020 month(datetime) &lt;- 11 mday(datetime) &lt;- 05 hour(datetime) &lt;- 01 datetime #&gt; [1] &quot;2020-11-05 01:34:56 UTC&quot; Alternatively, rather than modifying in place, you can create a new date-time with update(). This also allows you to set multiple values at once, the api is similar to make_datetime(). datetime &lt;- ymd_hms(&quot;2016-07-08,12:34:56&quot;) update(datetime,year = 2000, month = 11, mday = 05, hour = 01) #&gt; [1] &quot;2000-11-05 01:34:56 UTC&quot; 如果修改yday，相当于同时修改 mday 和 month: datetime &lt;- ymd_hms(&quot;2016-07-08,12:34:56&quot;) update(datetime, yday = 1) #&gt; [1] &quot;2016-01-01 12:34:56 UTC&quot; If values are too big, they will roll-over: ymd(&quot;2015-02-01&quot;) %&gt;% update(mday = 30) #&gt; [1] &quot;2015-03-02&quot; ymd(&quot;2015-02-01&quot;) %&gt;% update(hour = 400) #&gt; [1] &quot;2015-02-17 16:00:00 UTC&quot; update() 函数还有一种比较巧妙的用法，比如我们想可视化一年中所有航班的的出发时间在一天中的分布： flights_dt %&gt;% transmute(dep_hour = update(dep_time, yday = 1)) %&gt;% ggplot(aes(dep_hour)) + geom_freqpoly(binwidth = 60 * 5) + # 1 bin per 5 minutes scale_x_datetime(breaks = scales::breaks_width(&quot;3 hours&quot;), label = scales::label_date_short()) + labs(title = &quot;All flight dep time in a day&quot;) 如果不用 update() ，我们可能需要先用hour()、minute()、second()获取三种成分，然后再用make_datetime()对这三种成分进行合并。 4.2.4 Exercises Exercise 1.3 以月份作为分组变量，在一年的范围内，航班时间在一天中的分布是如何变化的？ flights_dt %&gt;% mutate(month = month(dep_time, label = TRUE), # this means month is now a factor dep_time = update(dep_time, yday = 1)) %&gt;% # yday can be an arbitary number ggplot(aes(dep_time)) + geom_freqpoly(binwidth = 60 * 60) + scale_x_time(labels = scales::label_time()) + facet_wrap(vars(month), nrow = 4) Exercise 4.2 如果想要再将延误的几率降至最低，那么应该在星期几搭乘航班？ flights_dt %&gt;% mutate(weekday = wday(dep_time, label = T, abbr = T)) %&gt;% group_by(weekday) %&gt;% summarize(delay_prob = mean(arr_delay &gt; 0, na.rm = T)) %&gt;% ggplot(aes(weekday,delay_prob)) + geom_line(aes(group = 1)) Exercise 3.3 航班预计起飞的小时对应的平均延误时间在一天的范围内是如何变化的？ flights_dt %&gt;% mutate(hour = hour(sched_dep_time)) %&gt;% group_by(hour) %&gt;% summarize(avg_delay = mean(dep_delay, na.rm = T)) %&gt;% ggplot(aes(hour, avg_delay))+ geom_point()+ geom_smooth() "],
["time-span.html", "4.3 Time span", " 4.3 Time span 接下来我们将讨论如何对时间进行数学运算，其中包括减法、加法和除法。我们可以把用于进行数学运算的时间称为时间间隔(time span)，它表示一种跨度，而不是某个静态的时间。本节将介绍3种用于表示时间间隔的重要类： 时期（Durations）：以秒为单位表示一段精确的时间 阶段(Periods)： 用人类单位定义的时间间隔，如几周或几个月 区间(Intervals)：由起点和终点定义的一段时间 4.3.1 时期 Durations 默认情况下，如果我们将两个日期相间，将得到一个 difftime 类对象： my_age &lt;- today() - ymd(19981112) my_age #&gt; Time difference of 7839 days difftime 对象的单位可以是秒、分钟、小时、日或周。这种模棱两可的对象处理起来非常困难，，所以 lubridate提供了总是以秒为单位的另一种时间间隔：时期。 as.duration(my_age) #&gt; [1] &quot;677289600s (~21.46 years)&quot; 可以用很多方便的函数来构造时期，它们有统一的格式d + 时间单位（复数）： dseconds(15) #&gt; [1] &quot;15s&quot; dminutes(10) #&gt; [1] &quot;600s (~10 minutes)&quot; dhours(c(12,24)) #&gt; [1] &quot;43200s (~12 hours)&quot; &quot;86400s (~1 days)&quot; ddays(0:5) #&gt; [1] &quot;0s&quot; &quot;86400s (~1 days)&quot; &quot;172800s (~2 days)&quot; #&gt; [4] &quot;259200s (~3 days)&quot; &quot;345600s (~4 days)&quot; &quot;432000s (~5 days)&quot; dweeks(3) ## 没有dmonths() #&gt; [1] &quot;1814400s (~3 weeks)&quot; dyears(1) #&gt; [1] &quot;31557600s (~1 years)&quot; 时期 Durations 总是以秒为单位来记录时间间隔。使用标准比率（1 分钟为 60 秒，1 小时为 60 分钟，1 天为 24 小时，1 周为 7 天，一年为 365 天）将分钟、小时、周和年转换为秒，从而建立具有更大值的对象。出于相同的原因，没有dmonths()函数, 因为一个月可能有 31 天、30 天、29 天或 28 天，所以 lubridate 不能将它转换为一个确切的秒数。 可以对时期进行加法和乘法操作： 2 * ddays(2) #&gt; [1] &quot;345600s (~4 days)&quot; dyears(1) + dweeks(12) + ddays(10) #&gt; [1] &quot;39679200s (~1.26 years)&quot; 最重要的，时期可以和日期时间型数据进行运算 ： (tomorrow &lt;- today() + ddays(1)) #&gt; [1] &quot;2020-04-30&quot; (last_year &lt;- now() - dyears(1)) #&gt; [1] &quot;2019-04-30 09:15:23 CST&quot; 然而，因为时期表示的是秒为单位的一个精确数值，有时我们会得到意想不到的结果： one_pm &lt;- ymd_hms(&quot;2016-03-12 13:00:00&quot;, tz = &quot;America/New_York&quot;) one_pm #&gt; [1] &quot;2016-03-12 13:00:00 EST&quot; one_pm + ddays(1) #&gt; [1] &quot;2016-03-13 14:00:00 EDT&quot; 为什么3月12日下午 1 点加上一天后变成了下午 2 点？如果仔细观察，就会发现时区发生了变化。因为夏时制，3 月 12 日只有 23 个小时，但我们告诉 R “加上 24 个小时代表的秒数”，所以得到了一个不正确的时间。 4.3.2 阶段 Periods 为了解决时期对象的问题，lubridate 提供了 阶段 对象。阶段也是一种 time span，但是它不以秒为单位 ； 相反，它使用“人工”时间，比如日和月。这使得阶段使用起来更加符合习惯 one_pm #&gt; [1] &quot;2016-03-12 13:00:00 EST&quot; one_pm + days(1) ## 阶段对象 #&gt; [1] &quot;2016-03-13 13:00:00 EDT&quot; one_pm + days(1)告诉 R，加上一天，而不是加上多少秒。 创建阶段对象的函数与时期很类似，只是前面少个“d”，不要把创建阶段的函数与获取时间日期成分的函数搞混了，创建 Periods 的函数都是复数形式： seconds(15) #&gt; [1] &quot;15S&quot; minutes(10) #&gt; [1] &quot;10M 0S&quot; hours(c(12,24)) #&gt; [1] &quot;12H 0M 0S&quot; &quot;24H 0M 0S&quot; days(7) #&gt; [1] &quot;7d 0H 0M 0S&quot; months(1:6) #&gt; [1] &quot;1m 0d 0H 0M 0S&quot; &quot;2m 0d 0H 0M 0S&quot; &quot;3m 0d 0H 0M 0S&quot; &quot;4m 0d 0H 0M 0S&quot; #&gt; [5] &quot;5m 0d 0H 0M 0S&quot; &quot;6m 0d 0H 0M 0S&quot; weeks(3) #&gt; [1] &quot;21d 0H 0M 0S&quot; years(1) #&gt; [1] &quot;1y 0m 0d 0H 0M 0S&quot; 可以对阶段进行加法和乘法操作： 10 * (months(6) + days(10)) #&gt; [1] &quot;60m 100d 0H 0M 0S&quot; days(50) + hours(25) + minutes(2) #&gt; [1] &quot;50d 25H 2M 0S&quot; 当然，阶段也可以和日期时间型数据进行运算。与 Durations 相比，使用 Periods 得到的计算结果更符合我们的预期： ## 闰年 ymd(&quot;2016-01-01&quot;) + dyears(1) #&gt; [1] &quot;2016-12-31 06:00:00 UTC&quot; ymd(&quot;2016-01-01&quot;) + years(1) #&gt; [1] &quot;2017-01-01&quot; ## 夏时制 one_pm + ddays(1) #&gt; [1] &quot;2016-03-13 14:00:00 EDT&quot; one_pm + days(1) #&gt; [1] &quot;2016-03-13 13:00:00 EDT&quot; There is still one specific problem worth mentioning. That is adding months. Adding months frustrates basic arithmetic because consecutive months have different lengths. With other elements, it is helpful for arithmetic to perform automatic roll over. For example, 12:00:00 + 61 seconds becomes 12:01:01. However, people often prefer that this behavior NOT occur with months. For example, we sometimes want January 31 + 1 month = February 28 and not March 3. %m+% performs this type of arithmetic. Date %m+% months(n) always returns a date in the nth month after Date. If you want minus, %m-% does the job. jan &lt;- ymd(&quot;2010-01-31&quot;) jan + months(1:3) # Feb 31 and April 31 returned as NA, because there is no such date #&gt; [1] NA &quot;2010-03-31&quot; NA jan %m+% months(1:3) #&gt; [1] &quot;2010-02-28&quot; &quot;2010-03-31&quot; &quot;2010-04-30&quot; jan %m-% months(1:3) #&gt; [1] &quot;2009-12-31&quot; &quot;2009-11-30&quot; &quot;2009-10-31&quot; %m+% can be also applied to other time span. For example, it is useful when performing arithmetic around a leap year: leap &lt;- ymd(20200229) # test if it is a leap year leap_year(leap) #&gt; [1] TRUE leap + years(c(-1, 1)) #&gt; [1] NA NA leap %m+% years(c(-1, 1)) #&gt; [1] &quot;2019-02-28&quot; &quot;2021-02-28&quot; 下面我们使用 Periods 来解决与航班日期有关的一个怪现象。有些飞机似乎从纽约市起飞前就到达了目的地： flights_dt %&gt;% filter(arr_time &lt; dep_time) %&gt;% select(arr_time, dep_time) #&gt; # A tibble: 10,633 x 2 #&gt; arr_time dep_time #&gt; &lt;dttm&gt; &lt;dttm&gt; #&gt; 1 2013-01-01 00:03:00 2013-01-01 19:29:00 #&gt; 2 2013-01-01 00:29:00 2013-01-01 19:39:00 #&gt; 3 2013-01-01 00:08:00 2013-01-01 20:58:00 #&gt; 4 2013-01-01 01:46:00 2013-01-01 21:02:00 #&gt; 5 2013-01-01 00:25:00 2013-01-01 21:08:00 #&gt; 6 2013-01-01 00:16:00 2013-01-01 21:20:00 #&gt; # ... with 10,627 more rows 这些都是过夜航班。我们使用了同一种日期来表示出发时间和到达时间，但这些航班是在第二天到达的。将每个过夜航班的到达时间加上一个days(1)，就可以解决这个问题了： flights_dt &lt;- flights_dt %&gt;% mutate(overnight = arr_time &lt; dep_time, arr_time = arr_time + days(overnight * 1)) ## 这样一来，航班数据就符合常理了 flights_dt %&gt;% filter(overnight, arr_time &lt; dep_time) #&gt; # A tibble: 0 x 10 #&gt; # ... with 10 variables: origin &lt;chr&gt;, dest &lt;chr&gt;, dep_delay &lt;dbl&gt;, #&gt; # arr_delay &lt;dbl&gt;, dep_time &lt;dttm&gt;, sched_dep_time &lt;dttm&gt;, arr_time &lt;dttm&gt;, #&gt; # sched_arr_time &lt;dttm&gt;, air_time &lt;dbl&gt;, overnight &lt;lgl&gt; 4.3.3 区间 Intervals 显然，dyears(1) / ddays(365)应该返回1，因为时期总是以秒来表示的，表示 1 年的时间就定义为相当于 365 天的秒数。 那么years(1) / days(1)应该返回什么呢？如果年份 是 2015 年，那么结果就是 365，但如果年份是 2016 年，那么结果就是 366！没有足够的信息让 lubridate 返回一个明确的结果。lubridate 的做法是给出一个估计值，同时给出一条警告： years(1) / days(1) #&gt; [1] 365 如果需要更精确的测量方式，那么就必须使用区间。区间是带有明确起点和终点的时期，这使得它非常精确, 可以用 interval() 来创建一个区间： interval(ymd(20090201), ymd(20090101)) #&gt; [1] 2009-02-01 UTC--2009-01-01 UTC 一种更简单的创建区间的方式是使用操作符 %--% next_year &lt;- today() + years(1) today() %--% next_year #&gt; [1] 2020-04-29 UTC--2021-04-29 UTC 要想知道一个区间内有多少个阶段，需要使用整数除法。利用区间进行精确计算： ## 闰年 (ymd(20160101) %--% ymd(20170101)) / days(1) #&gt; [1] 366 ## 平年 (ymd(20170101) %--% ymd(20180101)) / days(1) #&gt; [1] 365 4.3.4 Conclusion 如何在时期、阶段和区间中进行选择呢？只要能够解决问题，我们就应该选择最简单的数据结构。如果只关心物理时间，那么就使用时期 Durations ； 如果还需要考虑人工时间，那么就使用阶段 Periods ； 如果需要找出人工时间范围内有多长的时间间隔，那么就使用区间。 下图总结了不同数据类型之间可以进行的数学运算： 4.3.5 Exercises Exercise 4.3 创建一个日期向量来给出 2015 年每个月的第一天 ymd(20150101) + months(0:11) #&gt; [1] &quot;2015-01-01&quot; &quot;2015-02-01&quot; &quot;2015-03-01&quot; &quot;2015-04-01&quot; &quot;2015-05-01&quot; #&gt; [6] &quot;2015-06-01&quot; &quot;2015-07-01&quot; &quot;2015-08-01&quot; &quot;2015-09-01&quot; &quot;2015-10-01&quot; #&gt; [11] &quot;2015-11-01&quot; &quot;2015-12-01&quot; ## To get the vector of the first day of the month for this year, we first need to figure out what this year is, and get January 1st of it floor_date(today(), &quot;year&quot;) + months(0:11) #&gt; [1] &quot;2020-01-01&quot; &quot;2020-02-01&quot; &quot;2020-03-01&quot; &quot;2020-04-01&quot; &quot;2020-05-01&quot; #&gt; [6] &quot;2020-06-01&quot; &quot;2020-07-01&quot; &quot;2020-08-01&quot; &quot;2020-09-01&quot; &quot;2020-10-01&quot; #&gt; [11] &quot;2020-11-01&quot; &quot;2020-12-01&quot; Exercise 4.4 编写一个函数，输入你的生日（日期型），返回你的年龄（以年为单位）： age &lt;- function(birth) { birth &lt;- ymd(birth) (birth %--% today()) %/% years(1) } age(&quot;19981112&quot;) #&gt; [1] 21 "],
["hms.html", "4.4 hms", " 4.4 hms library(hms) The hms package provides a simple class for storing durations or time-of-day values and displaying them in the hh:mm:ss format. # order: seconds, minutes, hours hms(56, 34, 12) #&gt; 12:34:56 hms(56, 34, 12) %&gt;% class #&gt; [1] &quot;hms&quot; &quot;difftime&quot; data.frame(hours = 1:3, hms = hms(hours = 1:3)) #&gt; hours hms #&gt; 1 1 01:00:00 #&gt; 2 2 02:00:00 #&gt; 3 3 03:00:00 as_hms(1) #&gt; 00:00:01 as_hms(&quot;12:34:56&quot;) #&gt; 12:34:56 hms() is a constructor that accepts second, minute, hour and day components as numeric vectors. round_hms() and trunc_hms() are onvenience functions to round or truncate to a multiple of seconds. They are similar to floor_date() and ceiling_date() in Section 4.2.2, but the time unit can only be seconds round_hms(as_hms(&quot;12:34:56&quot;), sec = 5) #&gt; 12:34:55 round_hms(as_hms(&quot;12:34:56&quot;), sec = 60) #&gt; 12:35:00 trunc_hms(as_hms(&quot;12:34:56&quot;), 60) #&gt; 12:34:00 "],
["dint.html", "4.5 dint", " 4.5 dint While lubridate can handle date &amp; time data in an effective manner, currently it requires the largest unit of a date to be days. This means it does not cover functionality for working with year-quarter, year-month and year-isoweek dates. In contrast, dint provides a toolkit for these 3 types of date. It stores them in an easily human readable integer format, e.q 20141 for the first quarter of 2014 and so forth. Additionally, it goes hand in hand with lubridate in more ways than one. dint is implemented in base R and comes with zero external dependencies. Even if you don’t work with such special dates directly, dint can still help you at formatting dates, labelling plot axes, or getting first / last days of calendar periods (quarters, months, isoweeks). dint provides 4 different S3 classes that inherit from date_xx (a superclass for package development purpose). date_yq() for year-quarter dates date_ym for year-month dates date_yw for year-isoweek dates date_y() for storing years. This is for development purpose, and does not provide notable advantage over storing year as integers. library(dint) 4.5.1 Creation date_xx vectors can be created using explicit constructors date_yq(2015, 1) #&gt; [1] &quot;2015-Q1&quot; # vectorized date_ym(c(2015, 2016), c(1, 2)) #&gt; [1] &quot;2015-M01&quot; &quot;2016-M02&quot; date_yw(c(2008, 2009), 1) #&gt; [1] &quot;2008-W01&quot; &quot;2009-W01&quot; It is worth mentioning that tsibble also provides similar functions like yearquarter(), yearmonth() and yearweek(). But I think they are generally not flexible in this use case. as_date_xx coerce other classes (mainly Date, POSIXct(time) and integer) into date_xx objects as_date_yq(Sys.time()) #&gt; [1] &quot;2020-Q2&quot; as_date_yq(20141) #&gt; [1] &quot;2014-Q1&quot; as_date_ym(201412) #&gt; [1] &quot;2014-M12&quot; as_date_yw(&quot;2018-01-01&quot;) # anything else that can be parsed by as.Date() works #&gt; [1] &quot;2018-W01&quot; 4.5.2 Arithmetic and Sequences All date_xx support addition, subtraction and sequence generation. q &lt;- date_yq(2014, 4) q + 1 #&gt; [1] &quot;2015-Q1&quot; q - 1 #&gt; [1] &quot;2014-Q3&quot; seq(q - 2, q + 2) #&gt; [1] &quot;2014-Q2&quot; &quot;2014-Q3&quot; &quot;2014-Q4&quot; &quot;2015-Q1&quot; &quot;2015-Q2&quot; 4.5.3 Accessors We can access components of date_xx (e.g the quarter of a date_yq) with accessor functions. You can also use these functions to convert between date_xx vectors. q &lt;- date_yq(2014, 4) get_year(q) #&gt; [1] 2014 get_month(q) #&gt; [1] 10 get_isoweek(q) #&gt; [1] 40 Accessor functions in dint are compatible with Date, POSIXct classes, so are year(), month() and day() in lubridate with date_xx classes. # dint accessor functions on other classes get_quarter(Sys.Date()) #&gt; [1] 2 get_month(ymd(20200303)) #&gt; [1] 3 get_isoweek(Sys.time()) #&gt; [1] 18 # lubridate accessor functions on date_xx classes year(q) #&gt; [1] 2014 quarter(q) # default to first month in 4th quarter #&gt; [1] 4 month(q) #&gt; [1] 10 day(q) # default to 1st day in that month #&gt; [1] 1 first_of_xx, last_of_xx are 2 helper functions to access the first or last day within a specifit span from a date_xx, Date and POSIX object. q &lt;- date_yq(2015, 1) # the same as as.Date(q), but more explicit first_of_quarter(q) #&gt; [1] &quot;2015-01-01&quot; last_of_quarter(q) #&gt; [1] &quot;2015-03-31&quot; d &lt;- ymd(&quot;20200303&quot;) # first locate the date in a isoweek, then find the first day in that isoweek first_of_isoweek(d) #&gt; [1] &quot;2020-03-02&quot; last_of_month(d) #&gt; [1] &quot;2020-03-31&quot; # Alternativeley you can use these: first_of_yq(2012, 2) #&gt; [1] &quot;2012-04-01&quot; last_of_ym(2012, 2) #&gt; [1] &quot;2012-02-29&quot; last_of_yw(2012, 2) #&gt; [1] &quot;2012-01-15&quot; 4.5.4 Formatting Formatting date_xx vectors is easy and uses a subset of the placeholders of base::strptime()(plus %q for quarters) q &lt;- date_yq(2014, 4) format(q, &quot;%Y Q%q&quot;) #&gt; [1] &quot;2014 Q4&quot; format(q, &quot;%Y.%q&quot;) #&gt; [1] &quot;2014.4&quot; format(q, &quot;%y.%q&quot;) #&gt; [1] &quot;14.4&quot; m &lt;- date_ym(2014, 12) format(m, &quot;%Y-M%m&quot;) #&gt; [1] &quot;2014-M12&quot; w &lt;- date_yw(2014, 1) format(w, &quot;%Y-W%V&quot;) #&gt; [1] &quot;2014-W01&quot; There are some shorthands functions for formatting format_yq(Sys.Date()) #&gt; [1] &quot;2020-Q2&quot; format_yq_short(Sys.Date()) #&gt; [1] &quot;2020.2&quot; format_yq_shorter(Sys.Date()) #&gt; [1] &quot;20.2&quot; format_ym(Sys.Date()) #&gt; [1] &quot;2020-M04&quot; format_ym_short(Sys.Date()) #&gt; [1] &quot;2020.04&quot; format_ym_shorter(Sys.Date()) #&gt; [1] &quot;20.04&quot; 4.5.5 Labelling functions in ggplot2 There are two ways of making use of dint functionality when working with date axis in ggplot2 use scale scale_date_**(), this is implemented by default pass shorthand format_** functions to argument labels in any scale, this is also applicable to Date axes. q &lt;- tibble( time = seq(date_yq(2016, 1), date_yq(2016, 4)), value = rnorm(4) ) ggplot(q) + geom_line(aes(time, value)) + scale_x_date_ym() + ggtitle(&quot;scale_x_yq() by default&quot;) -&gt; p1 ggplot(q) + geom_line(aes(time, value)) + scale_x_date_ym(labels = format_ym_iso) + ggtitle(&quot;labels = format_ym_iso&quot;) -&gt; p2 p1 + p2 Use format_** in Date axes x &lt;- data.frame( time = seq(as.Date(&quot;2016-01-01&quot;), as.Date(&quot;2016-08-08&quot;), by = &quot;day&quot;), value = rnorm(221) ) p &lt;- ggplot( x, aes( x = time, y = value) ) + geom_point() p + ggtitle(&quot;default&quot;) p + scale_x_date(labels = format_yq_iso) + ggtitle(&quot;date_yq_iso&quot;) p + scale_x_date(labels = format_ym_short) + ggtitle(&quot;date_ym_short&quot;) p + scale_x_date(labels = format_yw_shorter) + ggtitle(&quot;date_yw_shorter&quot;) "],
["forcats-factor.html", "5 forcats: factor", " 5 forcats: factor 因子(factor)在 R 中用于处理分类变量。从历史上看，因子远比字符串容易处理。因此，R 基础包的很多函数都自动将字符串转换为因子。这意味着因子经常出现在并不真正适合他们的地方。好在我们不用担心 tidyverse 中出现这种问题，可以将注意力集中于真正需要因子类型的地方。 Roger Peng 的文章\"stringsAsFactors: An unauthorized bigraphy和Thomas Lumley的文章stringsAsFactors = sigh介绍了有关因子和字符串的一些历史背景。 2006 年, stringsAsFactors 这一设置的前身 charToFactor 被引入了 data.frame() 函数中，后来被纳入到 read.table() 里。默认情况下，stringsAsFactors被设置为True，R便会自动把字符串转换为因子型变量。在当时，这种设置是不难理解的。早期R的用户几乎都是统计科班出身的研究者，他们所用数据集里的字符串几乎都代表了一个定性变量，例如年龄(male/female), 国家(US/other), 地区(East/West)。进一步地，由于统计学家们的工作重点几乎都集中在构建各种统计模型上，而像lm()和glm()的函数只有当一个变量是 factor 类型的时候才开始对其编码，在统计模型中构建虚拟变量。 另一个原因更隐秘一些。在内部的存储机制中，因子变量经过一些编码后用数值存储，使得因子比字符串在占用内存空间上更加划算。2007 年后，R 引入了一种“CHARSXP”的方法，使得字符串也被映射为数值存储，stringsAsFactors = T在这点上的优势便不复存在了。 如今，R 的用户群体大大地多样化了，许多人开始抱怨默认设置stringsAsFactors = T，因为他们数据集中的字符串未必要用来建模，而可能只是一种标签。例如，在基因组学中，基因位点的名字不是某个模型中的变量，而现在把它们转换为因子也不会再有存储上的优势，反而会使得一些分析方法失效（比如使用正则表达式）。 我们将使用forcats包来处理因子，这个包提供了能够处理分类变量（其实就是因子的另一种说法）的工具，其中还包括了处理因子的大量辅助函数。因为 forcats 不是tidyverse的核心 R 包，所以需要手动加载。 library(forcats) 所有 forcats 中用于因子处理的核心函数均以 fct_ 前缀开头，且第一个参数均为要处理的因子向量，这意味着forcats包中的函数在使用管道操作时，传入的必须是你要操作的向量。关于 fct_ 函数族最有用的一点是，它可以接受传入的向量是字符串变量（而不仅仅是因子类型），且不会在输出结果中改变变量的类型。这意味着字符串可以一方面享受 fct_ 函数带来的操作便利，一方面保有字符串的特性。 "],
["factor-basics.html", "5.1 Factor basics", " 5.1 Factor basics 假设我们想要创建一个记录月份的变量： x1 &lt;- c(&quot;Dec&quot;, &quot;Apr&quot;, &quot;Jan&quot;, &quot;Mar&quot;) 使用字符串来记录月份有两个问题： 理论上，月份只有 12 个取值。但使用字符串时，我们没有办法告诉 R 什么样的值才是合法的，即使输入错误，代码也不会有什么反应。 x2 &lt;- c(&quot;Dec&quot;, &quot;Apr&quot;, &quot;Jam&quot;, &quot;Mar&quot;) 对月份的排序没有意义，因为字符串总是按照字母顺序排列的 sort(x1) #&gt; [1] &quot;Apr&quot; &quot;Dec&quot; &quot;Jan&quot; &quot;Mar&quot; 我们可以使用因子来解决以上两个问题。想要创建一个因子，最好先创建指定因子水平顺序的一个向量： month_levels &lt;- c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;) 现在可以开始创建因子了： y1 &lt;- factor(x1, levels = month_levels) y1 #&gt; [1] Dec Apr Jan Mar #&gt; Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 使用因子类型后，不在有效水平向量内的的所有值都会自动转换为 NA： y2 &lt;- factor(x2, levels = month_levels) y2 #&gt; [1] Dec Apr &lt;NA&gt; Mar #&gt; Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 如果要显示错误信息，可以用readr::parse_factor()函数代替 factor(),当 x 的中的某些元素不在有效水平列表时返回错误信息： y2 &lt;- parse_factor(x2, levels = month_levels) 如果省略了定义水平向量这个步骤，那么R会按照字母顺序作为水平由低到高的顺序： factor(x1) #&gt; [1] Dec Apr Jan Mar #&gt; Levels: Apr Dec Jan Mar 有时候我们会想让因子的水平顺序与创建时输入的顺序保持一致。在创建因子时，将levels设定为unique(x)，就可以达到这个目的： x1 #&gt; [1] &quot;Dec&quot; &quot;Apr&quot; &quot;Jan&quot; &quot;Mar&quot; f1 &lt;- factor(x1, levels = unique(x1)) f1 #&gt; [1] Dec Apr Jan Mar #&gt; Levels: Dec Apr Jan Mar 如果想要直接访问因子的有效水平向量或者个数，可以使用levels()和nlevels()函数： levels(f1) #&gt; [1] &quot;Dec&quot; &quot;Apr&quot; &quot;Jan&quot; &quot;Mar&quot; nlevels(f1) #&gt; [1] 4 "],
["sorting.html", "5.2 Sorting", " 5.2 Sorting 5.2.1 Sorting by frequency, appearance, or numeric order fct_infreq() reorder factor levels by frequency of each level, NA levels come last regardless of frequency. # What&#39;s the most frequent hair color in starwars ? ggplot(starwars) + geom_bar(aes(fct_infreq(hair_color))) fct_inorder(): sort a factor by the order in which they first appear. This can be useful when dealing with time series data. f &lt;- factor(c(&quot;b&quot;, &quot;b&quot;, &quot;a&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;)) levels(f) # alphabetic order #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; fct_inorder(f) #&gt; [1] b b a c c c #&gt; Levels: b a c fct_inseq(): sort a factor by numeric value of a level. This is only applicable when at least one existing level can be coercible to numeric f &lt;- factor(1:3, levels = c(&quot;3&quot;, &quot;2&quot;, &quot;1&quot;)) fct_inseq(f) #&gt; [1] 1 2 3 #&gt; Levels: 1 2 3 5.2.2 Sorting by another variable fct_reorder() 其实就是 base::reorder() 在 forcats 中的实现，它根据因子在其他变量上的统计量（中位数、平均数、···）的值对各个水平排序，当绘制非频次条形图时很有用。 Use .fun to set a summarizing function (defaults to median()), .desc = TRUE to sort the factor in descending order, NA levels always come the last regardless of the corresponding variable, fct_explicit_na() in Section 5.3.4 fix this. # reorder hair_color by median of height, then summarize med_height &lt;- starwars %&gt;% mutate(hair_color = fct_reorder(hair_color, height)) %&gt;% group_by(hair_color) %&gt;% summarize(med_height = median(height, na.rm = TRUE)) med_height %&gt;% ggplot(aes(hair_color, med_height)) + geom_col() Sometimes a factor is mapped to a non-position aesthetic, fct_reorder2(.f, .x, .y, .fun = last2) is designed for this kind of 2d displays of a factor. last2() and first2() are helpers for fct_reorder2(); last2() finds the last value of .y when sorted by .x; first2() finds the first value. chks &lt;- ChickWeight %&gt;% as_tibble() %&gt;% filter(as.integer(Chick) &lt; 10) %&gt;% mutate(Chick = fct_shuffle(Chick)) # random order ggplot(chks, aes(Time, weight, color = Chick)) + geom_point() + geom_line() # change the order of weight, # so that points with largest Weight, last time are assigned the first color # Note that lines match order in legend ggplot(chks, aes(Time, weight, color = fct_reorder2(Chick, Time, weight))) + geom_point() + geom_line() + labs(colour = &quot;Chick&quot;) 5.2.3 Sorting manually fct_infreq() 和 fct_reorder() 排序的依据是明确的，但我们有时也需要人工指定、修改排序结果。fct_relevel()接受一个向量调整因子水平的排序。 这个例子中使用forcats::gss_cat，该数据集是综合社会调查（General Social Survey）的一份抽样。综合社会调查是美国芝加哥大学的独立研究组织 NORC 进行的一项长期美国社会调查 gss_cat #&gt; # A tibble: 21,483 x 9 #&gt; year marital age race rincome partyid relig denom tvhours #&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 2000 Never mar~ 26 White $8000 to ~ Ind,near r~ Protesta~ Souther~ 12 #&gt; 2 2000 Divorced 48 White $8000 to ~ Not str re~ Protesta~ Baptist~ NA #&gt; 3 2000 Widowed 67 White Not appli~ Independent Protesta~ No deno~ 2 #&gt; 4 2000 Never mar~ 39 White Not appli~ Ind,near r~ Orthodox~ Not app~ 4 #&gt; 5 2000 Divorced 25 White Not appli~ Not str de~ None Not app~ 1 #&gt; 6 2000 Married 25 White $20000 - ~ Strong dem~ Protesta~ Souther~ NA #&gt; # ... with 21,477 more rows levels(gss_cat$rincome) #&gt; [1] &quot;No answer&quot; &quot;Don&#39;t know&quot; &quot;Refused&quot; &quot;$25000 or more&quot; #&gt; [5] &quot;$20000 - 24999&quot; &quot;$15000 - 19999&quot; &quot;$10000 - 14999&quot; &quot;$8000 to 9999&quot; #&gt; [9] &quot;$7000 to 7999&quot; &quot;$6000 to 6999&quot; &quot;$5000 to 5999&quot; &quot;$4000 to 4999&quot; #&gt; [13] &quot;$3000 to 3999&quot; &quot;$1000 to 2999&quot; &quot;Lt $1000&quot; &quot;Not applicable&quot; 在这个数据集中，因子 rincome 个水平的顺序排列是正确的。为了演示fct_relevel()的用法，先用 fct_shuffle() 打乱该因子的水平顺序： reshuffled_income &lt;- fct_shuffle(gss_cat$rincome) ## reordering the levels of rincome randomly with fct_shuffle(): levels(reshuffled_income) #&gt; [1] &quot;$3000 to 3999&quot; &quot;No answer&quot; &quot;$10000 - 14999&quot; &quot;$15000 - 19999&quot; #&gt; [5] &quot;Not applicable&quot; &quot;$8000 to 9999&quot; &quot;$25000 or more&quot; &quot;Don&#39;t know&quot; #&gt; [9] &quot;$7000 to 7999&quot; &quot;$20000 - 24999&quot; &quot;$5000 to 5999&quot; &quot;$1000 to 2999&quot; #&gt; [13] &quot;Refused&quot; &quot;$6000 to 6999&quot; &quot;Lt $1000&quot; &quot;$4000 to 4999&quot; 在 fct_relevel() 中，通过一个包含水平名称的向量调整排序。默认情况下，向量中的第一个水平被调整到第一个位置上，第二个水平被调整到第二个位置上，以此类推，你只需要指定那些需要调整的水平。可以通过 after 指定向量中各水平被调整到什么地方, after = -Inf 时第一个水平将被调整到排序的最后一位： ## move Lt $1000 and $1000 to 2999 to the front fct_relevel(reshuffled_income, c(&quot;Lt $1000&quot;, &quot;$1000 to 2999&quot;)) %&gt;% levels() #&gt; [1] &quot;Lt $1000&quot; &quot;$1000 to 2999&quot; &quot;$3000 to 3999&quot; &quot;No answer&quot; #&gt; [5] &quot;$10000 - 14999&quot; &quot;$15000 - 19999&quot; &quot;Not applicable&quot; &quot;$8000 to 9999&quot; #&gt; [9] &quot;$25000 or more&quot; &quot;Don&#39;t know&quot; &quot;$7000 to 7999&quot; &quot;$20000 - 24999&quot; #&gt; [13] &quot;$5000 to 5999&quot; &quot;Refused&quot; &quot;$6000 to 6999&quot; &quot;$4000 to 4999&quot; ## move Lt $1000 and $1000 to 2999 to the second and third place fct_relevel(reshuffled_income, c(&quot;Lt $1000&quot;, &quot;$1000 to 2999&quot;), after = 1) %&gt;% levels() #&gt; [1] &quot;$3000 to 3999&quot; &quot;Lt $1000&quot; &quot;$1000 to 2999&quot; &quot;No answer&quot; #&gt; [5] &quot;$10000 - 14999&quot; &quot;$15000 - 19999&quot; &quot;Not applicable&quot; &quot;$8000 to 9999&quot; #&gt; [9] &quot;$25000 or more&quot; &quot;Don&#39;t know&quot; &quot;$7000 to 7999&quot; &quot;$20000 - 24999&quot; #&gt; [13] &quot;$5000 to 5999&quot; &quot;Refused&quot; &quot;$6000 to 6999&quot; &quot;$4000 to 4999&quot; "],
["chaninge-number-of-levels.html", "5.3 Chaninge number of levels", " 5.3 Chaninge number of levels 5.3.1 Lumping levels To demonstrate how to lump multiple levels of a factor, we will start with fct_count() to count factor levels. It’s basically a variant of dplyr::count(), taking a factor (factor) as its first argument instead of a data frame, which makes it a nice function in mutate(). fct_count(starwars$skin_color, sort = TRUE, # sort descendantly prop = TRUE) # compute the fraction of marginal table #&gt; # A tibble: 31 x 3 #&gt; f n p #&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 fair 17 0.195 #&gt; 2 light 11 0.126 #&gt; 3 dark 6 0.0690 #&gt; 4 green 6 0.0690 #&gt; 5 grey 6 0.0690 #&gt; 6 pale 5 0.0575 #&gt; # ... with 25 more rows skin_color has 31 levels overall, and the top 5 to 6 levels occupy more than 50% percent of occurence. In fact, there are 24 levels whose frequency is less than 3%. fct_count(starwars$skin_color, prop = TRUE) %&gt;% filter(p &lt; 0.03) #&gt; # A tibble: 24 x 3 #&gt; f n p #&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 blue 2 0.0230 #&gt; 2 blue, grey 2 0.0230 #&gt; 3 brown mottle 1 0.0115 #&gt; 4 brown, white 1 0.0115 #&gt; 5 fair, green, yellow 1 0.0115 #&gt; 6 gold 1 0.0115 #&gt; # ... with 18 more rows In this case, We may want to collpase some of the less frequent levels into one, say, a level called “other”. forcats provides a family of functions that lumps together factor levels that meet some criteria into a new level “other”. fct_lump_min(): lumps levels that appear fewer than min times fct_lump_prop(): lumps levels that appear fewer than prop * n times fct_lump_n(): lumps all levels except for the n most frequent (or least frequent if n &lt; 0) fct_lump_lowfreq() lumps together the least frequent levels, ensuring that “other” is still the smallest level. # lump levels that appear fewer than 5 times into &quot;other&quot; starwars %&gt;% mutate(skin_color = fct_lump_min(skin_color, min = 5)) %&gt;% count(skin_color, sort = TRUE) #&gt; # A tibble: 7 x 2 #&gt; skin_color n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Other 36 #&gt; 2 fair 17 #&gt; 3 light 11 #&gt; 4 dark 6 #&gt; 5 green 6 #&gt; 6 grey 6 #&gt; # ... with 1 more row # preserve 5 most common levels starwars %&gt;% mutate(skin_color = fct_lump_n(skin_color, n = 5)) %&gt;% count(skin_color, sort = TRUE) #&gt; # A tibble: 6 x 2 #&gt; skin_color n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Other 41 #&gt; 2 fair 17 #&gt; 3 light 11 #&gt; 4 dark 6 #&gt; 5 green 6 #&gt; 6 grey 6 # preserve 5 least common levels starwars %&gt;% mutate(skin_color = fct_lump(skin_color, n = -5)) %&gt;% count(skin_color, sort = TRUE) #&gt; # A tibble: 17 x 2 #&gt; skin_color n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Other 71 #&gt; 2 brown mottle 1 #&gt; 3 brown, white 1 #&gt; 4 fair, green, yellow 1 #&gt; 5 gold 1 #&gt; 6 green-tan, brown 1 #&gt; # ... with 11 more rows Similarly, positive prop preserves values that appear at least prop of the time. Negative prop preserves values that appear at most -prop of the time. Use argument other_level to change default name “other” starwars %&gt;% mutate(skin_color = fct_lump_prop(skin_color, prop = 0.1, other_level = &quot;extra&quot;)) %&gt;% count(skin_color, sort = TRUE) #&gt; # A tibble: 3 x 2 #&gt; skin_color n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 extra 59 #&gt; 2 fair 17 #&gt; 3 light 11 fct_other(f, keep, drop, other_level) provides a way of manually replacing values with “other”. Pcik one of keep and drop: keep will preserve listed levels, replacing all others with other_level drop will replace listed levels with other_level, keeping all as is. x &lt;- factor(rep(LETTERS[1:9], times = c(40, 10, 5, 27, 1, 1, 1, 1, 1))) fct_other(x, keep = c(&quot;A&quot;, &quot;B&quot;), other_level = &quot;I don&#39;t care&quot;) %&gt;% fct_count() #&gt; # A tibble: 3 x 2 #&gt; f n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 A 40 #&gt; 2 B 10 #&gt; 3 I don&#39;t care 37 5.3.2 Expanding levels fct_expand() add additional levels to a factor f &lt;- factor(sample(letters[1:3], 20 , replace = T)) fct_count(f) #&gt; # A tibble: 3 x 2 #&gt; f n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 a 8 #&gt; 2 b 9 #&gt; 3 c 3 # add 3 values f &lt;- fct_expand(f, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;) levels(f) #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; # additional levels are assigned with zero frequncy fct_count(f) #&gt; # A tibble: 6 x 2 #&gt; f n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 a 8 #&gt; 2 b 9 #&gt; 3 c 3 #&gt; 4 d 0 #&gt; 5 e 0 #&gt; 6 f 0 fct_cross() combines levels of multiple input factors in a parallel manner: fruit &lt;- factor(c(&quot;apple&quot;, &quot;kiwi&quot;, &quot;apple&quot;, &quot;apple&quot;)) color &lt;- factor(c(&quot;green&quot;, &quot;green&quot;, &quot;red&quot;, &quot;green&quot;)) fct_cross(fruit, color) #&gt; [1] apple:green kiwi:green apple:red apple:green #&gt; Levels: apple:green kiwi:green apple:red # change deliminator fct_cross(fruit, color, sep = &quot;|&quot;) #&gt; [1] apple|green kiwi|green apple|red apple|green #&gt; Levels: apple|green kiwi|green apple|red By default, fct_cross() does not regard combinations with no observations as valid levels, so kiwi:red didn’t appear in the output. Use keep_empty = TRUE so that fct_croos() keep combinations with no observations as levels fct_cross(fruit, color, keep_empty = TRUE) #&gt; [1] apple:green kiwi:green apple:red apple:green #&gt; Levels: apple:green kiwi:green apple:red kiwi:red 5.3.3 Dropping levels 有时候我们希望在数据中取出一个子集，这可能导致在子集中，因子在某些水平上的频次为 0，但 R 并不会自动舍弃舍弃频次为 0 的水平： ## 在原始数据汇总，hair_color共有12个水平 nlevels(factor(starwars$hair_color)) #&gt; [1] 12 fct_count(starwars$hair_color) #&gt; # A tibble: 13 x 2 #&gt; f n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 auburn 1 #&gt; 2 auburn, grey 1 #&gt; 3 auburn, white 1 #&gt; 4 black 13 #&gt; 5 blond 3 #&gt; 6 blonde 1 #&gt; # ... with 7 more rows ## 筛选重量在70~135的角色，得到一个子集 (starwars_sub &lt;- starwars %&gt;% filter(between(mass, 70, 135))) #&gt; # A tibble: 34 x 13 #&gt; name height mass hair_color skin_color eye_color birth_year gender homeworld #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Luke~ 172 77 blond fair blue 19 male Tatooine #&gt; 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 &lt;NA&gt; Tatooine #&gt; 3 Owen~ 178 120 brown, gr~ light blue 52 male Tatooine #&gt; 4 Beru~ 165 75 brown light blue 47 female Tatooine #&gt; 5 Bigg~ 183 84 black light brown 24 male Tatooine #&gt; 6 Obi-~ 182 77 auburn, w~ fair blue-gray 57 male Stewjon #&gt; # ... with 28 more rows, and 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, #&gt; # vehicles &lt;list&gt;, starships &lt;list&gt; ## 现在hair_color只在8个有效水平上有记录，但是总的水平个数没有改变 nlevels(factor(starwars$hair_color)) #&gt; [1] 12 fct_count(starwars_sub$hair_color) #&gt; # A tibble: 9 x 2 #&gt; f n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 auburn, white 1 #&gt; 2 black 5 #&gt; 3 blond 2 #&gt; 4 brown 7 #&gt; 5 brown, grey 1 #&gt; 6 grey 1 #&gt; # ... with 3 more rows ## 用fct_drop()舍弃频次为0的那些水平 starwars_sub$hair_color %&gt;% fct_drop() %&gt;% nlevels() #&gt; [1] 8 还可以通过给 only 参数指定一个向量指定想要丢弃的水平，只有频次为0且包含在该向量中的水平才会被丢弃： f &lt;- factor(c(&quot;a&quot;, &quot;b&quot;), levels = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) fct_drop(f) #&gt; [1] a b #&gt; Levels: a b # Set only to restrict which levels to drop fct_drop(f, only = &quot;a&quot;) ## a水平上有频次，不会被丢弃；c水平上没有频次，但不在only中，也不会被丢弃 #&gt; [1] a b #&gt; Levels: a b c fct_drop(f, only = &quot;c&quot;) #&gt; [1] a b #&gt; Levels: a b 5.3.4 Transforming NA levels When a factor has missing values, these NAs will not be listed as a valid level. Though in some cases NA in a factor could be meaningful. As such we can replace factor() with fct_explicit_na() if necessary f &lt;- factor(c(&quot;a&quot;, &quot;a&quot;, NA, NA, &quot;a&quot;, &quot;b&quot;, NA, &quot;c&quot;, &quot;a&quot;, &quot;c&quot;, &quot;b&quot;)) levels(f) #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; nlevels(f) #&gt; [1] 3 fct_explicit_na() gives a explicit factor level na_level to the NA: fct_explicit_na(f) #&gt; [1] a a (Missing) (Missing) a b (Missing) #&gt; [8] c a c b #&gt; Levels: a b c (Missing) fct_explicit_na(f, na_level = &quot;Unknown&quot;) #&gt; [1] a a Unknown Unknown a b Unknown c a #&gt; [10] c b #&gt; Levels: a b c Unknown "],
["recoding.html", "5.4 Recoding", " 5.4 Recoding 比修改因子水平顺序、改变水平个数更强大的操作时修改水平的值。修改水平的值不仅可以使图形标签更为美观清晰，以满足出版发行的要求，还可以将水平汇集成更高层次的显示。修改水平最常用、最强大的工具是fct_recode()函数，它可以对每个水平进行修改或重新编码。例如，我们来看一下综合社会调查数据中的因子变量partyid: fct_count(gss_cat$partyid) #&gt; # A tibble: 10 x 2 #&gt; f n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 No answer 154 #&gt; 2 Don&#39;t know 1 #&gt; 3 Other party 393 #&gt; 4 Strong republican 2314 #&gt; 5 Not str republican 3032 #&gt; 6 Ind,near rep 1791 #&gt; # ... with 4 more rows 在这个因子中，对水平的描述太过简单，而且不一致，我们用 fct_recode() 将其修改为较为详细的排比结构，格式为fct_recode(f,level_new = level_old): gss_cat %&gt;% mutate(partyid = fct_recode(partyid, &quot;Republican,strong&quot; = &quot;Strong republican&quot;, &quot;Republican weak&quot; = &quot;Not str republican&quot;, &quot;Independent,near rep&quot; =&quot;Ind,near rep&quot;, &quot;Independent,near dem&quot; = &quot;Ind,near dem&quot;, &quot;Democrat,weak&quot; = &quot;Not str democrat&quot;, &quot;Democrat,strong&quot; = &quot;Strong democrat&quot;)) %&gt;% count(partyid) #&gt; # A tibble: 10 x 2 #&gt; partyid n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 No answer 154 #&gt; 2 Don&#39;t know 1 #&gt; 3 Other party 393 #&gt; 4 Republican,strong 2314 #&gt; 5 Republican weak 3032 #&gt; 6 Independent,near rep 1791 #&gt; # ... with 4 more rows fct_recode() 函数会让没有明确提及的水平保持原样，如果不小心修改了一个不存在的水平，那么它也会给出警告。 可以将多个原水平赋给同一个新水平，这样就可以合并原来的分类: ## 将&quot;no answer&quot;、&quot;Don&#39;t know&quot;和&quot;Other party&quot;合并为&quot;Other&quot; gss_cat %&gt;% mutate(partyid_recode = fct_recode( partyid, &quot;Republican,strong&quot; = &quot;Strong republican&quot;, &quot;Republican weak&quot; = &quot;Not str republican&quot;, &quot;Independent,near rep&quot; =&quot;Ind,near rep&quot;, &quot;Independent,near dem&quot; = &quot;Ind,near dem&quot;, &quot;Democrat,weak&quot; = &quot;Not str democrat&quot;, &quot;Democrat,strong&quot; = &quot;Strong democrat&quot;, &quot;Other&quot; = &quot;No answer&quot;, &quot;Other&quot; = &quot;Don&#39;t know&quot;, &quot;Other&quot; = &quot;Other party&quot; )) %&gt;% count(partyid_recode) #&gt; # A tibble: 8 x 2 #&gt; partyid_recode n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 Other 548 #&gt; 2 Republican,strong 2314 #&gt; 3 Republican weak 3032 #&gt; 4 Independent,near rep 1791 #&gt; 5 Independent 4119 #&gt; 6 Independent,near dem 2499 #&gt; # ... with 2 more rows As a variant of fct_recode(), fct_collapse() collapses factor levels into manually defined groups gss_cat %&gt;% mutate(partyid = fct_collapse(partyid, other = c(&quot;No answer&quot;,&quot;Don&#39;t know&quot;,&quot;Other party&quot;), rep = c(&quot;Strong republican&quot;,&quot;Not str republican&quot;), ind = c(&quot;Ind,near rep&quot;,&quot;Independent&quot;,&quot;Ind,near dem&quot;), dem = c(&quot;Not str democrat&quot;,&quot;Strong democrat&quot;))) %&gt;% count(partyid) #&gt; # A tibble: 4 x 2 #&gt; partyid n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 other 548 #&gt; 2 rep 5346 #&gt; 3 ind 8409 #&gt; 4 dem 7180 Unmentioned levels stay as is. To collapse this levels, specify other_level, this is always placed at the end of levels. # collapse two republican levels into &quot;rep&quot;, and others into &quot;I don&#39;t care&quot; gss_cat %&gt;% mutate(partyid = fct_collapse(partyid, rep = c(&quot;Strong republican&quot;,&quot;Not str republican&quot;), other_level = &quot;I don&#39;t care&quot;)) %&gt;% count(partyid) #&gt; # A tibble: 2 x 2 #&gt; partyid n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 rep 5346 #&gt; 2 I don&#39;t care 16137 5.4.1 Exercises Exercise 5.1 美国民主党，共和党和中间派的人数是如何随时间变化的？ gss_cat_collapse &lt;- gss_cat %&gt;% mutate(partyid = fct_collapse(partyid, other = c(&quot;No answer&quot;,&quot;Don&#39;t know&quot;,&quot;Other party&quot;), rep = c(&quot;Strong republican&quot;,&quot;Not str republican&quot;), ind = c(&quot;Ind,near rep&quot;,&quot;Independent&quot;,&quot;Ind,near dem&quot;), dem = c(&quot;Not str democrat&quot;,&quot;Strong democrat&quot;))) gss_cat_collapse %&gt;% group_by(year) %&gt;% count(partyid) %&gt;% ggplot(aes(year,n,color = partyid))+ geom_line()+ geom_point(size = 2, shape= 1) "],
["tidyr-tidy-data.html", "6 tidyr: Tidy data", " 6 tidyr: Tidy data library(tidylog) # Tidylog provides feedback about dplyr and tidyr operations. library(lubridate) "],
["tidyr-intro.html", "6.1 Tidy data", " 6.1 Tidy data “Happy families are all alike; every unhappy family is unhappy in its own way.” –– Leo Tolstoy “Tidy datasets are all alike, but every messy dataset is messy in its own way.” –– Hadley Wickham 整洁数据 (Tidy data) 是进行数据操作和 ggplot2 可视化的基础，所谓数据整理（清洗、清理），就是把 messy data 转换为 tidy data 的过程。在 tidyverse 生态中，tidyr 负责数据的整理和变型： 如果一个数据集是整洁的，需要满足以下三个要素： 1. 每个变量有一个专属列 (Each variable must have its own column) 2. 每个观测有一个专属行 (Each observation must have its own row) 3. 每个值有一个专属的存储单元 (Each value must its own cell) 这三条规则是互相关联的，不可能只满足三条规则中的两条，所以我们可以更简化地把清洁数据的要求写成： 1. 每列是一个变量(Variables go in columns) 2. 每行是一个观测(Observatiosn go in rows) 同样的数据可以有不同的表现形式，但只有满足整洁数据的三个条件的数据集才是最容易使用的。以下的 3 个数据集背后的均来自1999年和2000年世界卫生组织在阿富汗、巴西和中国的一次肺结核病例调查，都有 country、year、cases 和 population四个变量，但采用了不同的组织方式: table1 #&gt; # A tibble: 6 x 4 #&gt; country year cases population #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afghanistan 1999 745 19987071 #&gt; 2 Afghanistan 2000 2666 20595360 #&gt; 3 Brazil 1999 37737 172006362 #&gt; 4 Brazil 2000 80488 174504898 #&gt; 5 China 1999 212258 1272915272 #&gt; 6 China 2000 213766 1280428583 table2 #&gt; # A tibble: 12 x 4 #&gt; country year type count #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afghanistan 1999 cases 745 #&gt; 2 Afghanistan 1999 population 19987071 #&gt; 3 Afghanistan 2000 cases 2666 #&gt; 4 Afghanistan 2000 population 20595360 #&gt; 5 Brazil 1999 cases 37737 #&gt; 6 Brazil 1999 population 172006362 #&gt; # ... with 6 more rows table3 #&gt; # A tibble: 6 x 3 #&gt; country year rate #&gt; * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 Afghanistan 1999 745/19987071 #&gt; 2 Afghanistan 2000 2666/20595360 #&gt; 3 Brazil 1999 37737/172006362 #&gt; 4 Brazil 2000 80488/174504898 #&gt; 5 China 1999 212258/1272915272 #&gt; 6 China 2000 213766/1280428583 table4a 和 table4b分别是以 cases 和 population 为值的数据透视表： table4a #&gt; # A tibble: 3 x 3 #&gt; country `1999` `2000` #&gt; * &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afghanistan 745 2666 #&gt; 2 Brazil 37737 80488 #&gt; 3 China 212258 213766 table4b #&gt; # A tibble: 3 x 3 #&gt; country `1999` `2000` #&gt; * &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afghanistan 19987071 20595360 #&gt; 2 Brazil 172006362 174504898 #&gt; 3 China 1272915272 1280428583 在上面的例子中，只有table1 符合清洁数据的标准。在table2 中，type不是一个变量，它的值 cases 和 population 才是变量，进而导致了每一行不是一个完整的观测。在 table3 中，rate 同样不是一个变量，cases 和 population 的值被挤在了一个单元里。至于 table4a 和table4b，1999 和 2000不是变量，而是一个表示年份的变量的值。 为什么要为获得清洁的数据如此大费周折呢？主要有两个优点： 清洁数据的规则使得我们可以遵从一个一致、明确的结构存储数据。学习处理这些数据的工具变得很容易，因为你的对象在底层是一致的。 把变量存储在列中可以把 R 的向量化函数优势发挥到极致。例如 mutate() 和 summarize() ，许多内置的 R 函数都是对向量进行操作的。只要有了清洁的数据，后面的数据变换工作就很容易： # Compute rate per 10,000 table1 %&gt;% mutate(rate = cases / population * 10000) #&gt; mutate: new variable &#39;rate&#39; with 6 unique values and 0% NA #&gt; # A tibble: 6 x 5 #&gt; country year cases population rate #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan 1999 745 19987071 0.373 #&gt; 2 Afghanistan 2000 2666 20595360 1.29 #&gt; 3 Brazil 1999 37737 172006362 2.19 #&gt; 4 Brazil 2000 80488 174504898 4.61 #&gt; 5 China 1999 212258 1272915272 1.67 #&gt; 6 China 2000 213766 1280428583 1.67 # Compute cases per year table1 %&gt;% group_by(year) %&gt;% summarize(cases = sum(cases)) #&gt; group_by: one grouping variable (year) #&gt; summarize: now 2 rows and 2 columns, ungrouped #&gt; # A tibble: 2 x 2 #&gt; year cases #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1999 250740 #&gt; 2 2000 296920 # 或者： table1 %&gt;% count(year, wt = cases) #&gt; count: now 2 rows and 2 columns, ungrouped #&gt; # A tibble: 2 x 2 #&gt; year n #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1999 250740 #&gt; 2 2000 296920 # Visualise changes over time library(ggplot2) ggplot(table1, aes(year, cases)) + geom_line(aes(group = country), colour = &quot;grey50&quot;) + geom_point(aes(colour = country))+ scale_x_continuous(breaks = c(1999,2000),labels = c(&quot;1999&quot;,&quot;2000&quot;)) 6.1.1 Exercises Exercise 6.1 用 table2 计算发病率 (rate = cases / population), 需要进行以下四步操作： * 得到每个国家每年的cases * 得到每个国家每年的population * 计算 rate = cases / population * 把算好的数据存储到正确的位置 首先，分别对 cases 和population建立一张表，并且确保两张表的排列顺序相同： table2 #&gt; # A tibble: 12 x 4 #&gt; country year type count #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afghanistan 1999 cases 745 #&gt; 2 Afghanistan 1999 population 19987071 #&gt; 3 Afghanistan 2000 cases 2666 #&gt; 4 Afghanistan 2000 population 20595360 #&gt; 5 Brazil 1999 cases 37737 #&gt; 6 Brazil 1999 population 172006362 #&gt; # ... with 6 more rows (t2_cases &lt;- filter(table2, type == &quot;cases&quot;) %&gt;% rename(cases = count) %&gt;% arrange(country, year)) #&gt; filter: removed 6 rows (50%), 6 rows remaining #&gt; rename: renamed one variable (cases) #&gt; # A tibble: 6 x 4 #&gt; country year type cases #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afghanistan 1999 cases 745 #&gt; 2 Afghanistan 2000 cases 2666 #&gt; 3 Brazil 1999 cases 37737 #&gt; 4 Brazil 2000 cases 80488 #&gt; 5 China 1999 cases 212258 #&gt; 6 China 2000 cases 213766 (t2_population &lt;- filter(table2, type == &quot;population&quot;) %&gt;% rename(population = count) %&gt;% arrange(country, year)) #&gt; filter: removed 6 rows (50%), 6 rows remaining #&gt; rename: renamed one variable (population) #&gt; # A tibble: 6 x 4 #&gt; country year type population #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afghanistan 1999 population 19987071 #&gt; 2 Afghanistan 2000 population 20595360 #&gt; 3 Brazil 1999 population 172006362 #&gt; 4 Brazil 2000 population 174504898 #&gt; 5 China 1999 population 1272915272 #&gt; 6 China 2000 population 1280428583 计算 rate t2_cases_per_cap &lt;- tibble( t2_cases$country, t2_cases$year, cases = t2_cases$cases, population = t2_population$population ) t2_cases_per_cap #&gt; # A tibble: 6 x 4 #&gt; `t2_cases$country` `t2_cases$year` cases population #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afghanistan 1999 745 19987071 #&gt; 2 Afghanistan 2000 2666 20595360 #&gt; 3 Brazil 1999 37737 172006362 #&gt; 4 Brazil 2000 80488 174504898 #&gt; 5 China 1999 212258 1272915272 #&gt; 6 China 2000 213766 1280428583 t2_cases_per_cap %&gt;% mutate(rate = cases/population) %&gt;% select(1,2,5) %&gt;% # 改变列名 mutate( country = t2_cases$country, year = t2_cases$year ) %&gt;% select(country, year, rate) #&gt; mutate: new variable &#39;rate&#39; with 6 unique values and 0% NA #&gt; select: dropped 2 variables (cases, population) #&gt; mutate: new variable &#39;country&#39; with 3 unique values and 0% NA #&gt; new variable &#39;year&#39; with 2 unique values and 0% NA #&gt; select: dropped 2 variables (t2_cases$country, t2_cases$year) #&gt; # A tibble: 6 x 3 #&gt; country year rate #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan 1999 0.0000373 #&gt; 2 Afghanistan 2000 0.000129 #&gt; 3 Brazil 1999 0.000219 #&gt; 4 Brazil 2000 0.000461 #&gt; 5 China 1999 0.000167 #&gt; 6 China 2000 0.000167 "],
["pivoting.html", "6.2 Pivoting", " 6.2 Pivoting Figure 6.1: Taken from https://www.garrickadenbuie.com/project/tidyexplain/#spread-and-gather 细看两表，不难发现它们实质上相同的数据(相对于第二张表，第一张是以 id 为行字段，key 为列字段，val 为值的数据透视表)。第一种称为宽数据 (wide data,Cartesian data,笛卡尔型数据)，需要看行和列的交叉点来找到对应的值。而第二种形式称为长数据(long data,indexed data，指标型数据)，在长数据（指标型）数据汇总，你需要看指标来找到需要变量的数值（变量x，y，z的值）。。很难简单地说哪一种格式更优，因为两种形式都有可能是整洁的，这取决于值“A”、“B”、“C”、“D”的含义。 数据整理常需要化宽为长，但偶尔也需要化长为宽, tidyr 分别提供了cpivot_longer() 和 pivot_wider() 来实现以上两种形式的转换操作(统称为 pivoting)。在 tidyr 1.0.0 及更早的版本中，gather() 和 spread() 分别承担相同的工作，实际效果与 pivot_ 函数一样，但后者有更易理解的命名和 api。 pivot_longer(data, cols, names_to = &quot;name&quot;, values_to = &quot;value&quot;,) pivot_wider(data, names_from = name, values_from = value) names_to 和 values_to 参数相当于原来 gather() 中的 key 和 value，其中 “键” 列的默认名称变为 “name” 同理, names_from 和 values_from 相当于原来 spread() 中的 key 和 value 6.2.1 pivot_longer() tidyr::relig_income 是一个典型的宽数据，除第一列以外的所有列表示收入的不同水平，值表示对应的人数： relig_income #&gt; # A tibble: 18 x 11 #&gt; religion `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` `$75-100k` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Agnostic 27 34 60 81 76 137 122 #&gt; 2 Atheist 12 27 37 52 35 70 73 #&gt; 3 Buddhist 27 21 30 34 33 58 62 #&gt; 4 Catholic 418 617 732 670 638 1116 949 #&gt; 5 Don’t k~ 15 14 15 11 10 35 21 #&gt; 6 Evangel~ 575 869 1064 982 881 1486 949 #&gt; # ... with 12 more rows, and 3 more variables: `$100-150k` &lt;dbl&gt;, #&gt; # `&gt;150k` &lt;dbl&gt;, `Don&#39;t know/refused` &lt;dbl&gt; relig_income %&gt;% pivot_longer(-religion, names_to = &quot;income&quot;, values_to = &quot;population&quot;) #&gt; pivot_longer: reorganized (&lt;$10k, $10-20k, $20-30k, $30-40k, $40-50k, …) into (income, population) [was 18x11, now 180x3] #&gt; # A tibble: 180 x 3 #&gt; religion income population #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Agnostic &lt;$10k 27 #&gt; 2 Agnostic $10-20k 34 #&gt; 3 Agnostic $20-30k 60 #&gt; 4 Agnostic $30-40k 81 #&gt; 5 Agnostic $40-50k 76 #&gt; 6 Agnostic $50-75k 137 #&gt; # ... with 174 more rows 另一个例子：美国劳工市场的月度数据，首先创建一个 messy data： ec2 &lt;- economics %&gt;% as_tibble() %&gt;% transmute(year = year(date), month = month(date), rate = unemploy) %&gt;% filter(year &gt; 2005) %&gt;% pivot_wider(names_from = &quot;month&quot;, values_from = &quot;rate&quot;) #&gt; transmute: dropped 6 variables (date, pce, pop, psavert, uempmed, …) #&gt; new variable &#39;year&#39; with 49 unique values and 0% NA #&gt; new variable &#39;month&#39; with 12 unique values and 0% NA #&gt; new variable &#39;rate&#39; with 550 unique values and 0% NA #&gt; filter: removed 462 rows (80%), 112 rows remaining #&gt; pivot_wider: reorganized (month, rate) into (1, 2, 3, 4, 5, …) [was 112x3, now 10x13] ec2 #&gt; # A tibble: 10 x 13 #&gt; year `1` `2` `3` `4` `5` `6` `7` `8` `9` `10` `11` `12` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2006 7064 7184 7072 7120 6980 7001 7175 7091 6847 6727 6872 6762 #&gt; 2 2007 7116 6927 6731 6850 6766 6979 7149 7067 7170 7237 7240 7645 #&gt; 3 2008 7685 7497 7822 7637 8395 8575 8937 9438 9494 10074 10538 11286 #&gt; 4 2009 12058 12898 13426 13853 14499 14707 14601 14814 15009 15352 15219 15098 #&gt; 5 2010 15046 15113 15202 15325 14849 14474 14512 14648 14579 14516 15081 14348 #&gt; 6 2011 14013 13820 13737 13957 13855 13962 13763 13818 13948 13594 13302 13093 #&gt; # ... with 4 more rows 化宽为长： ec2 %&gt;% pivot_longer(-year, names_to = &quot;month&quot;, values_to = &quot;value&quot;) #&gt; pivot_longer: reorganized (1, 2, 3, 4, 5, …) into (month, value) [was 10x13, now 120x3] #&gt; # A tibble: 120 x 3 #&gt; year month value #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 2006 1 7064 #&gt; 2 2006 2 7184 #&gt; 3 2006 3 7072 #&gt; 4 2006 4 7120 #&gt; 5 2006 5 6980 #&gt; 6 2006 6 7001 #&gt; # ... with 114 more rows 以上就是 pviot_longer 的基本用法，下面来处理一些更复杂的情况。 6.2.1.1 Numeric data in column names pivot_longer() 提供了 names_ptype 和 values_ptypes 调整数据集变长后键列和值列的数据类型。看一下 billboard 数据集： billboard #&gt; # A tibble: 317 x 79 #&gt; artist track date.entered wk1 wk2 wk3 wk4 wk5 wk6 wk7 wk8 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2 Pac Baby~ 2000-02-26 87 82 72 77 87 94 99 NA #&gt; 2 2Ge+h~ The ~ 2000-09-02 91 87 92 NA NA NA NA NA #&gt; 3 3 Doo~ Kryp~ 2000-04-08 81 70 68 67 66 57 54 53 #&gt; 4 3 Doo~ Loser 2000-10-21 76 76 72 69 67 65 55 59 #&gt; 5 504 B~ Wobb~ 2000-04-15 57 34 25 17 17 31 36 49 #&gt; 6 98^0 Give~ 2000-08-19 51 39 34 26 26 19 2 2 #&gt; # ... with 311 more rows, and 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, #&gt; # wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;, wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, #&gt; # wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;, wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, #&gt; # wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;, wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, #&gt; # wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;, wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, #&gt; # wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;, wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, #&gt; # wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, wk43 &lt;dbl&gt;, wk44 &lt;dbl&gt;, wk45 &lt;dbl&gt;, wk46 &lt;dbl&gt;, #&gt; # wk47 &lt;dbl&gt;, wk48 &lt;dbl&gt;, wk49 &lt;dbl&gt;, wk50 &lt;dbl&gt;, wk51 &lt;dbl&gt;, wk52 &lt;dbl&gt;, #&gt; # wk53 &lt;dbl&gt;, wk54 &lt;dbl&gt;, wk55 &lt;dbl&gt;, wk56 &lt;dbl&gt;, wk57 &lt;dbl&gt;, wk58 &lt;dbl&gt;, #&gt; # wk59 &lt;dbl&gt;, wk60 &lt;dbl&gt;, wk61 &lt;dbl&gt;, wk62 &lt;dbl&gt;, wk63 &lt;dbl&gt;, wk64 &lt;dbl&gt;, #&gt; # wk65 &lt;dbl&gt;, wk66 &lt;lgl&gt;, wk67 &lt;lgl&gt;, wk68 &lt;lgl&gt;, wk69 &lt;lgl&gt;, wk70 &lt;lgl&gt;, #&gt; # wk71 &lt;lgl&gt;, wk72 &lt;lgl&gt;, wk73 &lt;lgl&gt;, wk74 &lt;lgl&gt;, wk75 &lt;lgl&gt;, wk76 &lt;lgl&gt; 显然，我们希望将所有以 \"wk\"开头的列聚合以得到整洁数据，键列和值列分别命名为 “week” 和 “rank”。另外要考虑的一点是，我们很可能之后想计算歌曲保持在榜单上的周数，故需要将 “week” 列转换为数值类型： billboard_tidy &lt;- billboard %&gt;% pivot_longer(cols = starts_with(&quot;wk&quot;), names_to = &quot;week&quot;, values_to = &quot;rank&quot;, names_prefix = &quot;wk&quot;, names_ptypes = list(week = integer()), values_drop_na = T) #&gt; pivot_longer: reorganized (wk1, wk2, wk3, wk4, wk5, …) into (week, rank) [was 317x79, now 5307x5] billboard_tidy #&gt; # A tibble: 5,307 x 5 #&gt; artist track date.entered week rank #&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 1 87 #&gt; 2 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 2 82 #&gt; 3 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 3 72 #&gt; 4 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 4 77 #&gt; 5 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 5 87 #&gt; 6 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 6 94 #&gt; # ... with 5,301 more rows names_prefix 去除前缀 “wk”(否则无法从字符串转换为数值)，names_ptype 以列表的形式转换键列的数据类型。同理, values_ptype 可以转换值列的数据类型。 ## 计算保持周数 billboard_tidy %&gt;% group_by(track) %&gt;% summarise(stay = max(week) - min(week) + 1) %&gt;% arrange(desc(stay)) #&gt; group_by: one grouping variable (track) #&gt; summarise: now 316 rows and 2 columns, ungrouped #&gt; # A tibble: 316 x 2 #&gt; track stay #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Higher 65 #&gt; 2 Amazed 64 #&gt; 3 Breathe 53 #&gt; 4 Kryptonite 53 #&gt; 5 With Arms Wide Open 47 #&gt; 6 I Wanna Know 44 #&gt; # ... with 310 more rows 6.2.1.2 Many variables in column names 在 tidyr 1.0.0 之前，当进行一定处理后发现多个变量被糅合到一列中时，可能会考虑使用 separate() 或者 extract(): who #&gt; # A tibble: 7,240 x 60 #&gt; country iso2 iso3 year new_sp_m014 new_sp_m1524 new_sp_m2534 new_sp_m3544 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afghan~ AF AFG 1980 NA NA NA NA #&gt; 2 Afghan~ AF AFG 1981 NA NA NA NA #&gt; 3 Afghan~ AF AFG 1982 NA NA NA NA #&gt; 4 Afghan~ AF AFG 1983 NA NA NA NA #&gt; 5 Afghan~ AF AFG 1984 NA NA NA NA #&gt; 6 Afghan~ AF AFG 1985 NA NA NA NA #&gt; # ... with 7,234 more rows, and 52 more variables: new_sp_m4554 &lt;int&gt;, #&gt; # new_sp_m5564 &lt;int&gt;, new_sp_m65 &lt;int&gt;, new_sp_f014 &lt;int&gt;, #&gt; # new_sp_f1524 &lt;int&gt;, new_sp_f2534 &lt;int&gt;, new_sp_f3544 &lt;int&gt;, #&gt; # new_sp_f4554 &lt;int&gt;, new_sp_f5564 &lt;int&gt;, new_sp_f65 &lt;int&gt;, #&gt; # new_sn_m014 &lt;int&gt;, new_sn_m1524 &lt;int&gt;, new_sn_m2534 &lt;int&gt;, #&gt; # new_sn_m3544 &lt;int&gt;, new_sn_m4554 &lt;int&gt;, new_sn_m5564 &lt;int&gt;, #&gt; # new_sn_m65 &lt;int&gt;, new_sn_f014 &lt;int&gt;, new_sn_f1524 &lt;int&gt;, #&gt; # new_sn_f2534 &lt;int&gt;, new_sn_f3544 &lt;int&gt;, new_sn_f4554 &lt;int&gt;, #&gt; # new_sn_f5564 &lt;int&gt;, new_sn_f65 &lt;int&gt;, new_ep_m014 &lt;int&gt;, #&gt; # new_ep_m1524 &lt;int&gt;, new_ep_m2534 &lt;int&gt;, new_ep_m3544 &lt;int&gt;, #&gt; # new_ep_m4554 &lt;int&gt;, new_ep_m5564 &lt;int&gt;, new_ep_m65 &lt;int&gt;, #&gt; # new_ep_f014 &lt;int&gt;, new_ep_f1524 &lt;int&gt;, new_ep_f2534 &lt;int&gt;, #&gt; # new_ep_f3544 &lt;int&gt;, new_ep_f4554 &lt;int&gt;, new_ep_f5564 &lt;int&gt;, #&gt; # new_ep_f65 &lt;int&gt;, newrel_m014 &lt;int&gt;, newrel_m1524 &lt;int&gt;, #&gt; # newrel_m2534 &lt;int&gt;, newrel_m3544 &lt;int&gt;, newrel_m4554 &lt;int&gt;, #&gt; # newrel_m5564 &lt;int&gt;, newrel_m65 &lt;int&gt;, newrel_f014 &lt;int&gt;, #&gt; # newrel_f1524 &lt;int&gt;, newrel_f2534 &lt;int&gt;, newrel_f3544 &lt;int&gt;, #&gt; # newrel_f4554 &lt;int&gt;, newrel_f5564 &lt;int&gt;, newrel_f65 &lt;int&gt; ## 原书 tidyr 一章中使用的方法 who %&gt;% gather(starts_with(&quot;new&quot;), key = key, value = value, na.rm = T) %&gt;% extract(key, into = c(&quot;diagnosis&quot;, &quot;gender&quot;, &quot;age&quot;), regex = &quot;new_?(.*)_(.)(.*)&quot;) #&gt; gather: reorganized (new_sp_m014, new_sp_m1524, new_sp_m2534, new_sp_m3544, new_sp_m4554, …) into (key, value) [was 7240x60, now 76046x6] #&gt; # A tibble: 76,046 x 8 #&gt; country iso2 iso3 year diagnosis gender age value #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afghanistan AF AFG 1997 sp m 014 0 #&gt; 2 Afghanistan AF AFG 1998 sp m 014 30 #&gt; 3 Afghanistan AF AFG 1999 sp m 014 8 #&gt; 4 Afghanistan AF AFG 2000 sp m 014 52 #&gt; 5 Afghanistan AF AFG 2001 sp m 014 129 #&gt; 6 Afghanistan AF AFG 2002 sp m 014 90 #&gt; # ... with 76,040 more rows 现在，pivot_longer() 现在可以在化宽为长的下一步直接完成拆分任务，可以直接在 names_to 中传入一个向量表示分裂后的各个键列，并在 names_sep(分隔符) 或者 names_pattern 中(正则表达式)指定分裂的模式： who %&gt;% pivot_longer(starts_with(&quot;new&quot;), names_to = c(&quot;diagonosis&quot;, &quot;gender&quot;, &quot;age&quot;), names_pattern = &quot;new_?(.*)_(.)(.*)&quot;, values_to = &quot;count&quot;, values_drop_na = T) #&gt; pivot_longer: reorganized (new_sp_m014, new_sp_m1524, new_sp_m2534, new_sp_m3544, new_sp_m4554, …) into (diagonosis, gender, age, count) [was 7240x60, now 76046x8] #&gt; # A tibble: 76,046 x 8 #&gt; country iso2 iso3 year diagonosis gender age count #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afghanistan AF AFG 1997 sp m 014 0 #&gt; 2 Afghanistan AF AFG 1997 sp m 1524 10 #&gt; 3 Afghanistan AF AFG 1997 sp m 2534 6 #&gt; 4 Afghanistan AF AFG 1997 sp m 3544 3 #&gt; 5 Afghanistan AF AFG 1997 sp m 4554 5 #&gt; 6 Afghanistan AF AFG 1997 sp m 5564 2 #&gt; # ... with 76,040 more rows 更进一步，顺便设定好整理后 gender 和 age 的类型： who %&gt;% pivot_longer(cols = starts_with(&quot;new&quot;), names_to = c(&quot;diagonosis&quot;, &quot;gender&quot;, &quot;age&quot;), names_pattern = &quot;new_?(.*)_(.)(.*)&quot;, names_ptypes = list( gender = factor(levels = c(&quot;f&quot;, &quot;m&quot;)), age = factor( levels = c(&quot;014&quot;, &quot;1524&quot;, &quot;2534&quot;, &quot;3544&quot;, &quot;4554&quot;, &quot;5564&quot;, &quot;65&quot;), ordered = TRUE) ), values_to = &quot;count&quot;, values_drop_na = T) #&gt; pivot_longer: reorganized (new_sp_m014, new_sp_m1524, new_sp_m2534, new_sp_m3544, new_sp_m4554, …) into (diagonosis, gender, age, count) [was 7240x60, now 76046x8] #&gt; # A tibble: 76,046 x 8 #&gt; country iso2 iso3 year diagonosis gender age count #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;ord&gt; &lt;int&gt; #&gt; 1 Afghanistan AF AFG 1997 sp m 014 0 #&gt; 2 Afghanistan AF AFG 1997 sp m 1524 10 #&gt; 3 Afghanistan AF AFG 1997 sp m 2534 6 #&gt; 4 Afghanistan AF AFG 1997 sp m 3544 3 #&gt; 5 Afghanistan AF AFG 1997 sp m 4554 5 #&gt; 6 Afghanistan AF AFG 1997 sp m 5564 2 #&gt; # ... with 76,040 more rows 6.2.1.3 Multiple observations per row (多个值列) So far, we have been working with data frames that have one observation per row, but many important pivotting problems involve multiple observations per row. You can usually recognise this case because name of the column that you want to appear in the output is part of the column name in the input. In this section, you’ll learn how to pivot this sort of data. family &lt;- tribble( ~family, ~dob_child1, ~dob_child2, ~gender_child1, ~gender_child2, 1L, &quot;1998-11-26&quot;, &quot;2000-01-29&quot;, 1L, 2L, 2L, &quot;1996-06-22&quot;, NA, 2L, NA, 3L, &quot;2002-07-11&quot;, &quot;2004-04-05&quot;, 2L, 2L, 4L, &quot;2004-10-10&quot;, &quot;2009-08-27&quot;, 1L, 1L, 5L, &quot;2000-12-05&quot;, &quot;2005-02-28&quot;, 2L, 1L, ) family &lt;- family %&gt;% mutate_at(vars(starts_with(&quot;dob&quot;)), ymd) #&gt; mutate_at: converted &#39;dob_child1&#39; from character to Date (0 new NA) #&gt; converted &#39;dob_child2&#39; from character to Date (0 new NA) family #&gt; # A tibble: 5 x 5 #&gt; family dob_child1 dob_child2 gender_child1 gender_child2 #&gt; &lt;int&gt; &lt;date&gt; &lt;date&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 1998-11-26 2000-01-29 1 2 #&gt; 2 2 1996-06-22 NA 2 NA #&gt; 3 3 2002-07-11 2004-04-05 2 2 #&gt; 4 4 2004-10-10 2009-08-27 1 1 #&gt; 5 5 2000-12-05 2005-02-28 2 1 理想中的数据格式(两个值列) family child dob gender 1 1 1998-11-26 1 1 2 2000-01-29 2 2 1 1996-06-22 2 3 1 2002-07-11 2 3 2 2004-04-05 2 4 1 2004-10-10 1 4 2 2009-08-27 1 5 1 2000-12-05 2 5 2 2005-02-28 1 Note that we have two pieces of information (or values) for each child: their gender and their dob (date of birth). These need to go into separate columns in the result. Again we supply multiple variables to names_to, using names_sep to split up each variable name. Note the special name .value: this tells pivot_longer() that that part of the column name specifies the “value” being measured (which will become a variable in the output) .value 在这里指代 dob 和 gender 两个值列 family %&gt;% pivot_longer( -family, names_to = c(&quot;.value&quot;, &quot;child&quot;), ## child 为每个 family 中的标识变量 names_sep = &quot;_&quot;, values_drop_na = TRUE ) #&gt; pivot_longer: reorganized (dob_child1, dob_child2, gender_child1, gender_child2) into (child, dob, gender) [was 5x5, now 9x4] #&gt; # A tibble: 9 x 4 #&gt; family child dob gender #&gt; &lt;int&gt; &lt;chr&gt; &lt;date&gt; &lt;int&gt; #&gt; 1 1 child1 1998-11-26 1 #&gt; 2 1 child2 2000-01-29 2 #&gt; 3 2 child1 1996-06-22 2 #&gt; 4 3 child1 2002-07-11 2 #&gt; 5 3 child2 2004-04-05 2 #&gt; 6 4 child1 2004-10-10 1 #&gt; # ... with 3 more rows 在这里，dob_child1、dob_child2、gender_child1、gender_child2四个列名的后半部分被当做键列的值。例如，可以认为对于 family == 1的观测，首先生成了如下的结构： family child dob dob gender gender 1 child1 1998-11-16 2000-01-29 1 2 2 child2 而后名称相同的值列合并： family child dob gender 1 child1 1998-11-26 1 1 child2 2000-01-29 2 另一个例子： anscombe #&gt; x1 x2 x3 x4 y1 y2 y3 y4 #&gt; 1 10 10 10 8 8.04 9.14 7.46 6.58 #&gt; 2 8 8 8 8 6.95 8.14 6.77 5.76 #&gt; 3 13 13 13 8 7.58 8.74 12.74 7.71 #&gt; 4 9 9 9 8 8.81 8.77 7.11 8.84 #&gt; 5 11 11 11 8 8.33 9.26 7.81 8.47 #&gt; 6 14 14 14 8 9.96 8.10 8.84 7.04 #&gt; 7 6 6 6 8 7.24 6.13 6.08 5.25 #&gt; 8 4 4 4 19 4.26 3.10 5.39 12.50 #&gt; 9 12 12 12 8 10.84 9.13 8.15 5.56 #&gt; 10 7 7 7 8 4.82 7.26 6.42 7.91 #&gt; 11 5 5 5 8 5.68 4.74 5.73 6.89 anscombe %&gt;% pivot_longer(everything(), names_to = c(&quot;.value&quot;, &quot;set&quot;), names_pattern = &quot;([xy])([1234])&quot;) #&gt; pivot_longer: reorganized (x1, x2, x3, x4, y1, …) into (set, x, y) [was 11x8, now 44x3] #&gt; # A tibble: 44 x 3 #&gt; set x y #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 10 8.04 #&gt; 2 2 10 9.14 #&gt; 3 3 10 7.46 #&gt; 4 4 8 6.58 #&gt; 5 1 8 6.95 #&gt; 6 2 8 8.14 #&gt; # ... with 38 more rows 叕一个例子： pnl &lt;- tibble( x = 1:4, a = c(1, 1,0, 0), b = c(0, 1, 1, 1), y1 = rnorm(4), y2 = rnorm(4), z1 = rep(3, 4), z2 = rep(-2, 4), ) pnl #&gt; # A tibble: 4 x 7 #&gt; x a b y1 y2 z1 z2 #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 0 0.788 -1.59 3 -2 #&gt; 2 2 1 1 -0.422 0.597 3 -2 #&gt; 3 3 0 1 0.0569 1.22 3 -2 #&gt; 4 4 0 1 0.711 -0.312 3 -2 pnl %&gt;% pivot_longer(-(x:b), names_to = c(&quot;.value&quot;, &quot;time&quot;), names_pattern = &quot;([yz])([12])&quot;) #&gt; pivot_longer: reorganized (y1, y2, z1, z2) into (time, y, z) [was 4x7, now 8x6] #&gt; # A tibble: 8 x 6 #&gt; x a b time y z #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 0 1 0.788 3 #&gt; 2 1 1 0 2 -1.59 -2 #&gt; 3 2 1 1 1 -0.422 3 #&gt; 4 2 1 1 2 0.597 -2 #&gt; 5 3 0 1 1 0.0569 3 #&gt; 6 3 0 1 2 1.22 -2 #&gt; # ... with 2 more rows 6.2.1.4 Duplicated column names 如果某个数据框中各列有重复的名字，用 gather() 聚合这些变量所在的列时会返回一条错误： Error: Can&#39;t bind data because some arguments have the same name 这是因为被聚合的列名被当做 key 列的值，又因这些值是重复的，故不能唯一标识一条记录。pivot_longer() 针对这一点做了优化，尝试聚合这些列时，会自动生成一个标识列： # To create a tibble with duplicated names # you have to explicitly opt out of the name repair # that usually prevents you from creating such a dataset: (df &lt;- tibble(x = 1:3, y = 4:6, y = 5:7, y = 7:9, .name_repair = &quot;minimal&quot;)) #&gt; # A tibble: 3 x 4 #&gt; x y y y #&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 4 5 7 #&gt; 2 2 5 6 8 #&gt; 3 3 6 7 9 df %&gt;% pivot_longer(-x, names_to = &quot;y&quot;) #&gt; pivot_longer: reorganized () into (.copy, value) [was 3x4, now 9x4] #&gt; # A tibble: 9 x 4 #&gt; x y .copy value #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 y 1 4 #&gt; 2 1 y 2 5 #&gt; 3 1 y 3 7 #&gt; 4 2 y 1 5 #&gt; 5 2 y 2 6 #&gt; 6 2 y 3 8 #&gt; # ... with 3 more rows 6.2.2 pivot_wider() pivot_wider() 是 pivot_longer() 的逆操作，虽然在获得 tidy data 上，前者没有后者常用，但它经常被用来创建一些 summary table。 fish_encounters 数据记录了一些沿河观测站对一批鱼群的观测情况(seen 为发现次数)： fish_encounters #&gt; # A tibble: 114 x 3 #&gt; fish station seen #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 4842 Release 1 #&gt; 2 4842 I80_1 1 #&gt; 3 4842 Lisbon 1 #&gt; 4 4842 Rstr 1 #&gt; 5 4842 Base_TD 1 #&gt; 6 4842 BCE 1 #&gt; # ... with 108 more rows 很多后续分析工具需要每个观测站的观测情况单独成一列，使用 pivot_wider(): # 数据中已有的列不需要引号便可引用 fish_encounters %&gt;% pivot_wider(names_from = station, values_from = seen) #&gt; pivot_wider: reorganized (station, seen) into (Release, I80_1, Lisbon, Rstr, Base_TD, …) [was 114x3, now 19x12] #&gt; # A tibble: 19 x 12 #&gt; fish Release I80_1 Lisbon Rstr Base_TD BCE BCW BCE2 BCW2 MAE MAW #&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 4842 1 1 1 1 1 1 1 1 1 1 1 #&gt; 2 4843 1 1 1 1 1 1 1 1 1 1 1 #&gt; 3 4844 1 1 1 1 1 1 1 1 1 1 1 #&gt; 4 4845 1 1 1 1 1 NA NA NA NA NA NA #&gt; 5 4847 1 1 1 NA NA NA NA NA NA NA NA #&gt; 6 4848 1 1 1 1 NA NA NA NA NA NA NA #&gt; # ... with 13 more rows 关于 pivot_wider()，很重要的一点是它会暴露出数据中的隐式缺失值(implicit missing value)。这些没有出现在原数据中的 NA 值不是源自于记录错误或者遗失，只是没有对应的观测而已（观测站只能记录发生了的观测）。参数 values_fill 可以以一个列表填充 pivot_wider() 结果中的 NA， 当然如何处理这些隐式缺失值要按具体情境而定，在鱼群的例子里，用 0 填充是合适的： fish_encounters %&gt;% pivot_wider(names_from = station, values_from = seen, values_fill = list(seen = 0)) #&gt; pivot_wider: reorganized (station, seen) into (Release, I80_1, Lisbon, Rstr, Base_TD, …) [was 114x3, now 19x12] #&gt; # A tibble: 19 x 12 #&gt; fish Release I80_1 Lisbon Rstr Base_TD BCE BCW BCE2 BCW2 MAE MAW #&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 4842 1 1 1 1 1 1 1 1 1 1 1 #&gt; 2 4843 1 1 1 1 1 1 1 1 1 1 1 #&gt; 3 4844 1 1 1 1 1 1 1 1 1 1 1 #&gt; 4 4845 1 1 1 1 1 0 0 0 0 0 0 #&gt; 5 4847 1 1 1 0 0 0 0 0 0 0 0 #&gt; 6 4848 1 1 1 1 0 0 0 0 0 0 0 #&gt; # ... with 13 more rows fish_encoutners 的贡献者 Myfanwy Johnston 在个人网站上有一篇相关的文章 6.2.2.1 Aggregation pivot_wider() 可以用来执行一些简单的聚合操作。warpbreaks 是一个关于经纱强度的控制试验，每个处理 (wool, tension) 上进行了 9 次试验： (warpbreaks &lt;- warpbreaks %&gt;% as_tibble() %&gt;% select(wool, tension, breaks)) #&gt; select: columns reordered (wool, tension, breaks) #&gt; # A tibble: 54 x 3 #&gt; wool tension breaks #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 A L 26 #&gt; 2 A L 30 #&gt; 3 A L 54 #&gt; 4 A L 25 #&gt; 5 A L 70 #&gt; 6 A L 52 #&gt; # ... with 48 more rows warpbreaks %&gt;% count(wool, tension) #&gt; count: now 6 rows and 3 columns, ungrouped #&gt; # A tibble: 6 x 3 #&gt; wool tension n #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 A L 9 #&gt; 2 A M 9 #&gt; 3 A H 9 #&gt; 4 B L 9 #&gt; 5 B M 9 #&gt; 6 B H 9 现在想知道每个处理下的平均断头次数，只需展开 wool 或 tension 中的任意一个： warpbreaks %&gt;% pivot_wider(names_from = wool, values_from = breaks) #&gt; Warning: Values in `breaks` are not uniquely identified; output will contain list-cols. #&gt; * Use `values_fn = list(breaks = list)` to suppress this warning. #&gt; * Use `values_fn = list(breaks = length)` to identify where the duplicates arise #&gt; * Use `values_fn = list(breaks = summary_fun)` to summarise duplicates #&gt; pivot_wider: reorganized (wool, breaks) into (A, B) [was 54x3, now 3x3] #&gt; # A tibble: 3 x 3 #&gt; tension A B #&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 L &lt;dbl [9]&gt; &lt;dbl [9]&gt; #&gt; 2 M &lt;dbl [9]&gt; &lt;dbl [9]&gt; #&gt; 3 H &lt;dbl [9]&gt; &lt;dbl [9]&gt; 由于 (wool, tension) 不能唯一确认一个观测，多个观测被压缩至一个列表中，values_fn(breaks = mean) 求得平均值： warpbreaks %&gt;% pivot_wider(names_from = wool, values_from = breaks, values_fn = list(breaks = mean)) #&gt; pivot_wider: reorganized (wool, breaks) into (A, B) [was 54x3, now 3x3] #&gt; # A tibble: 3 x 3 #&gt; tension A B #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 L 44.6 28.2 #&gt; 2 M 24 28.8 #&gt; 3 H 24.6 18.8 For more complex summary operations, I recommend summarising before reshaping, but for simple cases it’s often convenient to summarise within pivot_wider() 6.2.2.2 Generate column name from multiple variables 现有一个数据集存储了关于产品、生产国家、生产年份和产量的水平组合： production &lt;- expand_grid( product = c(&quot;A&quot;, &quot;B&quot;), country = c(&quot;AI&quot;, &quot;EI&quot;), year = 2000:2014 ) %&gt;% filter((product == &quot;A&quot; &amp; country == &quot;AI&quot;) | product == &quot;B&quot;) %&gt;% mutate(production = rnorm(nrow(.))) #&gt; filter: removed 15 rows (25%), 45 rows remaining #&gt; mutate: new variable &#39;production&#39; with 45 unique values and 0% NA production #&gt; # A tibble: 45 x 4 #&gt; product country year production #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 A AI 2000 -0.209 #&gt; 2 A AI 2001 -0.369 #&gt; 3 A AI 2002 0.330 #&gt; 4 A AI 2003 1.88 #&gt; 5 A AI 2004 -0.482 #&gt; 6 A AI 2005 1.74 #&gt; # ... with 39 more rows 假设现在希望对于每个 product 和 country 的组合均创建一列，关键是在 names_from 中传入一个向量： production %&gt;% pivot_wider(names_from = c(product, country), values_from = production) #&gt; pivot_wider: reorganized (product, country, production) into (A_AI, B_AI, B_EI) [was 45x4, now 15x4] #&gt; # A tibble: 15 x 4 #&gt; year A_AI B_AI B_EI #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2000 -0.209 1.02 -1.14 #&gt; 2 2001 -0.369 0.598 0.143 #&gt; 3 2002 0.330 1.38 -0.0472 #&gt; 4 2003 1.88 -1.06 -1.26 #&gt; 5 2004 -0.482 0.197 3.39 #&gt; 6 2005 1.74 1.37 0.120 #&gt; # ... with 9 more rows names_sep 可以指定除了 _ 以外的分隔符。names_prefix 为展开后的各列添加前缀(as opposed to removing in pivot_longer()) 6.2.2.3 Multiple value columns The us_rent_income dataset contains information about median income and rent for each state in the US for 2017 (from the American Community Survey, retrieved with the tidycensus package). us_rent_income #&gt; # A tibble: 104 x 5 #&gt; GEOID NAME variable estimate moe #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 01 Alabama income 24476 136 #&gt; 2 01 Alabama rent 747 3 #&gt; 3 02 Alaska income 32940 508 #&gt; 4 02 Alaska rent 1200 13 #&gt; 5 04 Arizona income 27517 148 #&gt; 6 04 Arizona rent 972 4 #&gt; # ... with 98 more rows Here both estimate and moe are values columns, so we can supply them to values_from: us_rent_income %&gt;% pivot_wider(names_from = variable, values_from = c(estimate, moe)) #&gt; pivot_wider: reorganized (variable, estimate, moe) into (estimate_income, estimate_rent, moe_income, moe_rent) [was 104x5, now 52x6] #&gt; # A tibble: 52 x 6 #&gt; GEOID NAME estimate_income estimate_rent moe_income moe_rent #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 01 Alabama 24476 747 136 3 #&gt; 2 02 Alaska 32940 1200 508 13 #&gt; 3 04 Arizona 27517 972 148 4 #&gt; 4 05 Arkansas 23789 709 165 5 #&gt; 5 06 California 29454 1358 109 3 #&gt; 6 08 Colorado 32401 1125 109 5 #&gt; # ... with 46 more rows Note that the name of the variable is automatically appended to the output columns. 6.2.2.4 When there is no identifying variable A final challenge is inspired by Jiena Gu. Imagine you have a contact list that you’ve copied and pasted from a website: contacts &lt;- tribble( ~field, ~value, &quot;name&quot;, &quot;Jiena McLellan&quot;, &quot;company&quot;, &quot;Toyota&quot;, &quot;name&quot;, &quot;John Smith&quot;, &quot;company&quot;, &quot;google&quot;, &quot;email&quot;, &quot;john@google.com&quot;, &quot;name&quot;, &quot;Huxley Ratcliffe&quot; ) contacts #&gt; # A tibble: 6 x 2 #&gt; field value #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 name Jiena McLellan #&gt; 2 company Toyota #&gt; 3 name John Smith #&gt; 4 company google #&gt; 5 email john@google.com #&gt; 6 name Huxley Ratcliffe This is challenging because there’s no variable that identifies which observations belong together. 直接化宽时，出现列表列（没有第三个标识变量） contacts %&gt;% pivot_wider(names_from = field, values_from = value) #&gt; pivot_wider: reorganized (field, value) into (name, company, email) [was 6x2, now 1x3] #&gt; # A tibble: 1 x 3 #&gt; name company email #&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 &lt;chr [3]&gt; &lt;chr [2]&gt; &lt;chr [1]&gt; We can fix this by noting that every contact starts with a name, so we can create a unique id by counting every time we see “name” as the field: (contacts &lt;- contacts %&gt;% mutate( person_id = cumsum(field == &quot;name&quot;) )) #&gt; mutate: new variable &#39;person_id&#39; with 3 unique values and 0% NA #&gt; # A tibble: 6 x 3 #&gt; field value person_id #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 name Jiena McLellan 1 #&gt; 2 company Toyota 1 #&gt; 3 name John Smith 2 #&gt; 4 company google 2 #&gt; 5 email john@google.com 2 #&gt; 6 name Huxley Ratcliffe 3 contacts %&gt;% pivot_wider(names_from = field, values_from = value) #&gt; pivot_wider: reorganized (field, value) into (name, company, email) [was 6x3, now 3x4] #&gt; # A tibble: 3 x 4 #&gt; person_id name company email #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1 Jiena McLellan Toyota &lt;NA&gt; #&gt; 2 2 John Smith google john@google.com #&gt; 3 3 Huxley Ratcliffe &lt;NA&gt; &lt;NA&gt; 6.2.3 Combining pivot_longer() and pivot_wider() Some problems can’t be solved by pivotting in a single direction. The examples in this section show how you might combine pivot_longer() and pivot_wider() to solve more complex problems. 6.2.3.1 world bank data world_bank_pop contains data from the World Bank about population per country from 2000 to 2018. world_bank_pop #&gt; # A tibble: 1,056 x 20 #&gt; country indicator `2000` `2001` `2002` `2003` `2004` `2005` `2006` #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 ABW SP.URB.T~ 4.24e4 4.30e4 4.37e4 4.42e4 4.47e+4 4.49e+4 4.49e+4 #&gt; 2 ABW SP.URB.G~ 1.18e0 1.41e0 1.43e0 1.31e0 9.51e-1 4.91e-1 -1.78e-2 #&gt; 3 ABW SP.POP.T~ 9.09e4 9.29e4 9.50e4 9.70e4 9.87e+4 1.00e+5 1.01e+5 #&gt; 4 ABW SP.POP.G~ 2.06e0 2.23e0 2.23e0 2.11e0 1.76e+0 1.30e+0 7.98e-1 #&gt; 5 AFG SP.URB.T~ 4.44e6 4.65e6 4.89e6 5.16e6 5.43e+6 5.69e+6 5.93e+6 #&gt; 6 AFG SP.URB.G~ 3.91e0 4.66e0 5.13e0 5.23e0 5.12e+0 4.77e+0 4.12e+0 #&gt; # ... with 1,050 more rows, and 11 more variables: `2007` &lt;dbl&gt;, `2008` &lt;dbl&gt;, #&gt; # `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;, #&gt; # `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;, `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt; It’s not obvious exactly what steps are needed yet, but I’ll start with the most obvious problem: year is spread across multiple columns. pop2 &lt;- world_bank_pop %&gt;% pivot_longer(`2000`:`2017`, names_to = &quot;year&quot;, values_to = &quot;value&quot;) #&gt; pivot_longer: reorganized (2000, 2001, 2002, 2003, 2004, …) into (year, value) [was 1056x20, now 19008x4] pop2 #&gt; # A tibble: 19,008 x 4 #&gt; country indicator year value #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 ABW SP.URB.TOTL 2000 42444 #&gt; 2 ABW SP.URB.TOTL 2001 43048 #&gt; 3 ABW SP.URB.TOTL 2002 43670 #&gt; 4 ABW SP.URB.TOTL 2003 44246 #&gt; 5 ABW SP.URB.TOTL 2004 44669 #&gt; 6 ABW SP.URB.TOTL 2005 44889 #&gt; # ... with 19,002 more rows Next we need to consider the indicator variable: world_bank_pop %&gt;% count(indicator) #&gt; count: now 4 rows and 2 columns, ungrouped #&gt; # A tibble: 4 x 2 #&gt; indicator n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 SP.POP.GROW 264 #&gt; 2 SP.POP.TOTL 264 #&gt; 3 SP.URB.GROW 264 #&gt; 4 SP.URB.TOTL 264 Here SP.POP.GROW is population growth, SP.POP.TOTL is total population, and SP.URB.* are the same but only for urban areas. Let’s split this up into two variables: area (total or urban) and the actual variable (population or growth): # Use NA to omit the variable in the output. pop3 &lt;- pop2 %&gt;% separate(indicator, c(NA, &quot;area&quot;, &quot;variable&quot;), sep = &quot;\\\\.&quot;) # sep takes a regex pop3 #&gt; # A tibble: 19,008 x 5 #&gt; country area variable year value #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 ABW URB TOTL 2000 42444 #&gt; 2 ABW URB TOTL 2001 43048 #&gt; 3 ABW URB TOTL 2002 43670 #&gt; 4 ABW URB TOTL 2003 44246 #&gt; 5 ABW URB TOTL 2004 44669 #&gt; 6 ABW URB TOTL 2005 44889 #&gt; # ... with 19,002 more rows Now we can complete the tidying by pivoting variable and value to make TOTL and GROW columns: pop3 %&gt;% pivot_wider(names_from = variable, values_from = value) #&gt; pivot_wider: reorganized (variable, value) into (TOTL, GROW) [was 19008x5, now 9504x5] #&gt; # A tibble: 9,504 x 5 #&gt; country area year TOTL GROW #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 ABW URB 2000 42444 1.18 #&gt; 2 ABW URB 2001 43048 1.41 #&gt; 3 ABW URB 2002 43670 1.43 #&gt; 4 ABW URB 2003 44246 1.31 #&gt; 5 ABW URB 2004 44669 0.951 #&gt; 6 ABW URB 2005 44889 0.491 #&gt; # ... with 9,498 more rows 6.2.3.2 mutli choice data Based on a suggestion by Maxime Wack, https://github.com/tidyverse/tidyr/issues/384), the final example shows how to deal with a common way of recording multiple choice data. Often you will get such data as follows: (multi &lt;- tribble( ~id, ~choice1, ~choice2, ~choice3, 1, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, 2, &quot;C&quot;, &quot;B&quot;, NA, 3, &quot;D&quot;, NA, NA, 4, &quot;B&quot;, &quot;D&quot;, NA )) #&gt; # A tibble: 4 x 4 #&gt; id choice1 choice2 choice3 #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1 A B C #&gt; 2 2 C B &lt;NA&gt; #&gt; 3 3 D &lt;NA&gt; &lt;NA&gt; #&gt; 4 4 B D &lt;NA&gt; But the actual order isn’t important, and you’d prefer to have the individual questions in the columns. You can achieve the desired transformation in two steps. First, you make the data longer, eliminating the explcit NAs, and adding a column to indicate that this choice was chosen: multi2 &lt;- multi %&gt;% pivot_longer(-id, values_drop_na = TRUE) %&gt;% mutate(checked = TRUE) #&gt; pivot_longer: reorganized (choice1, choice2, choice3) into (name, value) [was 4x4, now 8x3] #&gt; mutate: new variable &#39;checked&#39; with one unique value and 0% NA multi2 #&gt; # A tibble: 8 x 4 #&gt; id name value checked #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; #&gt; 1 1 choice1 A TRUE #&gt; 2 1 choice2 B TRUE #&gt; 3 1 choice3 C TRUE #&gt; 4 2 choice1 C TRUE #&gt; 5 2 choice2 B TRUE #&gt; 6 3 choice1 D TRUE #&gt; # ... with 2 more rows Then you make the data wider, filling in the missing observations with FALSE; note the use of id_cols = id here, this eliminated the name column and combines mutilples rows per person into one row, since we don’t need name in identifying an observation: multi2 %&gt;% pivot_wider(id_cols = id, names_from = value, values_from = checked, values_fill = list(checked = FALSE)) #&gt; pivot_wider: reorganized (name, value, checked) into (A, B, C, D) [was 8x4, now 4x5] #&gt; # A tibble: 4 x 5 #&gt; id A B C D #&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; #&gt; 1 1 TRUE TRUE TRUE FALSE #&gt; 2 2 FALSE TRUE TRUE FALSE #&gt; 3 3 FALSE FALSE FALSE TRUE #&gt; 4 4 FALSE TRUE FALSE TRUE 6.2.4 Exercises Exercise 6.2 在下面的例子中，研究为什么 pivot_longer() 和 pivot_wider() 不是完美对称的 (stocks &lt;- tibble( year = c(2015, 2015, 2016, 2016), half = c(1, 2, 1, 2), return = c(1.88, 0.59, 0.92, 0.17) )) #&gt; # A tibble: 4 x 3 #&gt; year half return #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2015 1 1.88 #&gt; 2 2015 2 0.59 #&gt; 3 2016 1 0.92 #&gt; 4 2016 2 0.17 stocks %&gt;% pivot_wider(names_from = year, values_from = return) %&gt;% pivot_longer(-half, names_to = &quot;year&quot;, values_to = &quot;return&quot;) #&gt; pivot_wider: reorganized (year, return) into (2015, 2016) [was 4x3, now 2x3] #&gt; pivot_longer: reorganized (2015, 2016) into (year, return) [was 2x3, now 4x3] #&gt; # A tibble: 4 x 3 #&gt; half year return #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 2015 1.88 #&gt; 2 1 2016 0.92 #&gt; 3 2 2015 0.59 #&gt; 4 2 2016 0.17 先后使用 pivot_wider() 和 pivot_longer()无法得到一个相同的数据集（除了列的顺序）是因为，数据整理有时会丢失列的类型信息。当 pivot_wider() 将变量 year 的值 2015 和 2016 用作列的名字时，它们自然被转化为了字符串\"2015\"和\"2016\"；随后 pivot_longer() 把列名用作键列year的值，从而year自然变成了一个字符向量，可以用 names_ptypes 避免这一点 。 stocks %&gt;% pivot_wider(names_from = year, values_from = return) %&gt;% pivot_longer(-half, names_to = &quot;year&quot;, values_to = &quot;return&quot;, names_ptypes = list(year = double())) #&gt; pivot_wider: reorganized (year, return) into (2015, 2016) [was 4x3, now 2x3] #&gt; pivot_longer: reorganized (2015, 2016) into (year, return) [was 2x3, now 4x3] #&gt; # A tibble: 4 x 3 #&gt; half year return #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 2015 1.88 #&gt; 2 1 2016 0.92 #&gt; 3 2 2015 0.59 #&gt; 4 2 2016 0.17 Exercise 6.3 为什么下面的数据框不能应用 pivot_wider()？可以添加一列解决这个问题吗？ (people &lt;- tribble( ~name, ~key, ~value, &quot;Phillip Woods&quot;, &quot;age&quot;, 45, &quot;Phillip Woods&quot;, &quot;height&quot;, 186, &quot;Phillip Woods&quot;, &quot;age&quot;, 50, &quot;Jessica Cordero&quot;, &quot;age&quot;, 37, &quot;Jessica Cordero&quot;, &quot;height&quot;, 156 )) #&gt; # A tibble: 5 x 3 #&gt; name key value #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Phillip Woods age 45 #&gt; 2 Phillip Woods height 186 #&gt; 3 Phillip Woods age 50 #&gt; 4 Jessica Cordero age 37 #&gt; 5 Jessica Cordero height 156 people %&gt;% pivot_wider(names_from = key, values_from = value) #&gt; # A tibble: 2 x 3 #&gt; name age height #&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 Phillip Woods &lt;dbl [2]&gt; &lt;dbl [1]&gt; #&gt; 2 Jessica Cordero &lt;dbl [1]&gt; &lt;dbl [1]&gt; 这个例子和 6.2.2.4 中的 contact 很类似，虽然这里有第三列 name，但仍不足以唯一标识任意观测 因为这个数据集里有两个对于 “Phillip Woods” 在 age 上年龄的观测，pivot_wider() 就要把由(Phillips Woods, age)确定的单元格里“塞进两个值”。本质上因为 name 和 key 这两个变量上的值不能唯一确定一行，所以我们只要添加一列，让name、key和新列可以唯一确定一行即可： people %&gt;% mutate(id = row_number()) %&gt;% pivot_wider(names_from = key, values_from = value) #&gt; mutate: new variable &#39;id&#39; with 5 unique values and 0% NA #&gt; pivot_wider: reorganized (key, value) into (age, height) [was 5x4, now 5x4] #&gt; # A tibble: 5 x 4 #&gt; name id age height #&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Phillip Woods 1 45 NA #&gt; 2 Phillip Woods 2 NA 186 #&gt; 3 Phillip Woods 3 50 NA #&gt; 4 Jessica Cordero 4 37 NA #&gt; 5 Jessica Cordero 5 NA 156 "],
["nesting.html", "6.3 Nesting", " 6.3 Nesting Nesting creates a list-column of data frames; unnesting flattens it back out into regular columns. Nesting is a implicitly summarising operation: you get one row for each group defined by the non-nested columns. This is useful in conjunction with other summaries that work with whole datasets, most notably models. Since a nested data frame is no more than a data frame where one (or more) list-columns of data frames. You can create simple nested data frames by hand: # df1 is a nested data frame (df1 &lt;- tibble( g = c(1, 2, 3), data = list( tibble(x = 1, y = 2), tibble(x = 4:5, y = 6:7), tibble(x = 10) ) )) #&gt; # A tibble: 3 x 2 #&gt; g data #&gt; &lt;dbl&gt; &lt;list&gt; #&gt; 1 1 &lt;tibble [1 x 2]&gt; #&gt; 2 2 &lt;tibble [2 x 2]&gt; #&gt; 3 3 &lt;tibble [1 x 1]&gt; Or more commonly, we can create nested data frames using tidyr::nest(). df %&gt;% nest(x, y) specifies the columns to be nested; i.e. the columns that will appear in the inner data frame. Alternatively, you can nest() a grouped data frame created by dplyr::group_by(). The grouping variables remain in the outer data frame and the others are nested. The result preserves the grouping of the input. Variables supplied to nest() will override grouping variables so that df %&gt;% group_by(x, y) %&gt;% nest(z) will be equivalent to df %&gt;% nest(z). df2 &lt;- tribble( ~g, ~x, ~y, 1, 1, 2, 2, 4, 6, 2, 5, 7, 3, 10, NA ) df2 %&gt;% nest(data = c(x, y)) #&gt; # A tibble: 3 x 2 #&gt; g data #&gt; &lt;dbl&gt; &lt;list&gt; #&gt; 1 1 &lt;tibble [1 x 2]&gt; #&gt; 2 2 &lt;tibble [2 x 2]&gt; #&gt; 3 3 &lt;tibble [1 x 2]&gt; # grouped nesting df2 %&gt;% group_by(g) %&gt;% nest() #&gt; group_by: one grouping variable (g) #&gt; # A tibble: 3 x 2 #&gt; # Groups: g [3] #&gt; g data #&gt; &lt;dbl&gt; &lt;list&gt; #&gt; 1 1 &lt;tibble [1 x 2]&gt; #&gt; 2 2 &lt;tibble [2 x 2]&gt; #&gt; 3 3 &lt;tibble [1 x 2]&gt; # equal to df2 %&gt;% group_nest(g) #&gt; # A tibble: 3 x 2 #&gt; g data #&gt; &lt;dbl&gt; &lt;list&gt; #&gt; 1 1 &lt;tibble [1 x 2]&gt; #&gt; 2 2 &lt;tibble [2 x 2]&gt; #&gt; 3 3 &lt;tibble [1 x 2]&gt; Nesting is easiest to understand in connection to grouped data: each row in the output corresponds to one group in the input. We’ll see shortly this is particularly convenient when you have other per-group objects. The opposite of nest() is unnest(). You give it the name of a list-column containing data frames, and it row-binds the data frames together, repeating the outer columns the right number of times to line up. df1 %&gt;% unnest(data) #&gt; # A tibble: 4 x 3 #&gt; g x y #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 2 #&gt; 2 2 4 6 #&gt; 3 2 5 7 #&gt; 4 3 10 NA dplyr::group_split() put each nested tibble in a list, similar to base::split(): df2 %&gt;% group_split(g) #&gt; [[1]] #&gt; # A tibble: 1 x 3 #&gt; g x y #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1 2 #&gt; #&gt; [[2]] #&gt; # A tibble: 2 x 3 #&gt; g x y #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2 4 6 #&gt; 2 2 5 7 #&gt; #&gt; [[3]] #&gt; # A tibble: 1 x 3 #&gt; g x y #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 3 10 NA #&gt; #&gt; attr(,&quot;ptype&quot;) #&gt; # A tibble: 0 x 3 #&gt; # ... with 3 variables: g &lt;dbl&gt;, x &lt;dbl&gt;, y &lt;dbl&gt; 6.3.1 Example: Managing multiple models Nested data is a great fit for problems where you have one of something for each group. A common place this arises is when you’re fitting multiple models. gapminder &lt;- gapminder::gapminder gapminder_nest &lt;- gapminder %&gt;% mutate(year1950 = year - 1950) %&gt;% group_nest(continent, country) #&gt; mutate: new variable &#39;year1950&#39; with 12 unique values and 0% NA gapminder_nest #&gt; # A tibble: 142 x 3 #&gt; continent country data #&gt; &lt;fct&gt; &lt;fct&gt; &lt;list&gt; #&gt; 1 Africa Algeria &lt;tibble [12 x 5]&gt; #&gt; 2 Africa Angola &lt;tibble [12 x 5]&gt; #&gt; 3 Africa Benin &lt;tibble [12 x 5]&gt; #&gt; 4 Africa Botswana &lt;tibble [12 x 5]&gt; #&gt; 5 Africa Burkina Faso &lt;tibble [12 x 5]&gt; #&gt; 6 Africa Burundi &lt;tibble [12 x 5]&gt; #&gt; # ... with 136 more rows Now gapminder_nest is a tibble with 142 rows representing 142 countries with their respective time series data from 1952 - 2007 stored in the list column data. Then we can combine mutate() and map to create a new column to fit a linear model for each country: mod_fit &lt;- function(data) { lm(lifeExp ~ year1950, data = data) } gapminder_model &lt;- gapminder_nest %&gt;% mutate(model = map(data, mod_fit)) #&gt; mutate: new variable &#39;model&#39; with 142 unique values and 0% NA gapminder_model #&gt; # A tibble: 142 x 4 #&gt; continent country data model #&gt; &lt;fct&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 Africa Algeria &lt;tibble [12 x 5]&gt; &lt;lm&gt; #&gt; 2 Africa Angola &lt;tibble [12 x 5]&gt; &lt;lm&gt; #&gt; 3 Africa Benin &lt;tibble [12 x 5]&gt; &lt;lm&gt; #&gt; 4 Africa Botswana &lt;tibble [12 x 5]&gt; &lt;lm&gt; #&gt; 5 Africa Burkina Faso &lt;tibble [12 x 5]&gt; &lt;lm&gt; #&gt; 6 Africa Burundi &lt;tibble [12 x 5]&gt; &lt;lm&gt; #&gt; # ... with 136 more rows Then use broom functions to generate “tidy” model summaries: gapminder_summary &lt;- gapminder_model %&gt;% mutate( glance = map(model, broom::glance), tidy = map(model, broom::tidy), augment = map(model, broom::augment) ) #&gt; mutate: new variable &#39;glance&#39; with 142 unique values and 0% NA #&gt; new variable &#39;tidy&#39; with 142 unique values and 0% NA #&gt; new variable &#39;augment&#39; with 142 unique values and 0% NA gapminder_summary #&gt; # A tibble: 142 x 7 #&gt; continent country data model glance tidy augment #&gt; &lt;fct&gt; &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 Africa Algeria &lt;tibble [12~ &lt;lm&gt; &lt;tibble [1 ~ &lt;tibble [2~ &lt;tibble [12~ #&gt; 2 Africa Angola &lt;tibble [12~ &lt;lm&gt; &lt;tibble [1 ~ &lt;tibble [2~ &lt;tibble [12~ #&gt; 3 Africa Benin &lt;tibble [12~ &lt;lm&gt; &lt;tibble [1 ~ &lt;tibble [2~ &lt;tibble [12~ #&gt; 4 Africa Botswana &lt;tibble [12~ &lt;lm&gt; &lt;tibble [1 ~ &lt;tibble [2~ &lt;tibble [12~ #&gt; 5 Africa Burkina F~ &lt;tibble [12~ &lt;lm&gt; &lt;tibble [1 ~ &lt;tibble [2~ &lt;tibble [12~ #&gt; 6 Africa Burundi &lt;tibble [12~ &lt;lm&gt; &lt;tibble [1 ~ &lt;tibble [2~ &lt;tibble [12~ #&gt; # ... with 136 more rows unnest() each column: # which country has the best fit gapminder_summary %&gt;% unnest(glance) %&gt;% arrange(desc(r.squared)) #&gt; # A tibble: 142 x 17 #&gt; continent country data model r.squared adj.r.squared sigma statistic p.value #&gt; &lt;fct&gt; &lt;fct&gt; &lt;lis&gt; &lt;lis&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Americas Brazil &lt;tib~ &lt;lm&gt; 0.998 0.998 0.326 5111. 6.99e-15 #&gt; 2 Africa Maurit~ &lt;tib~ &lt;lm&gt; 0.998 0.997 0.408 4290. 1.68e-14 #&gt; 3 Europe France &lt;tib~ &lt;lm&gt; 0.998 0.997 0.220 4200. 1.86e-14 #&gt; 4 Europe Switze~ &lt;tib~ &lt;lm&gt; 0.997 0.997 0.215 3823. 2.98e-14 #&gt; 5 Asia Pakist~ &lt;tib~ &lt;lm&gt; 0.997 0.997 0.403 3626. 3.88e-14 #&gt; 6 Asia Indone~ &lt;tib~ &lt;lm&gt; 0.997 0.997 0.646 3455. 4.93e-14 #&gt; # ... with 136 more rows, and 8 more variables: df &lt;int&gt;, logLik &lt;dbl&gt;, #&gt; # AIC &lt;dbl&gt;, BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt;, tidy &lt;list&gt;, #&gt; # augment &lt;list&gt; gapminder_summary %&gt;% unnest(tidy) #&gt; # A tibble: 284 x 11 #&gt; continent country data model glance term estimate std.error statistic #&gt; &lt;fct&gt; &lt;fct&gt; &lt;lis&gt; &lt;lis&gt; &lt;list&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Africa Algeria &lt;tib~ &lt;lm&gt; &lt;tibb~ (Int~ 42.2 0.756 55.8 #&gt; 2 Africa Algeria &lt;tib~ &lt;lm&gt; &lt;tibb~ year~ 0.569 0.0221 25.7 #&gt; 3 Africa Angola &lt;tib~ &lt;lm&gt; &lt;tibb~ (Int~ 31.7 0.804 39.4 #&gt; 4 Africa Angola &lt;tib~ &lt;lm&gt; &lt;tibb~ year~ 0.209 0.0235 8.90 #&gt; 5 Africa Benin &lt;tib~ &lt;lm&gt; &lt;tibb~ (Int~ 38.9 0.671 58.0 #&gt; 6 Africa Benin &lt;tib~ &lt;lm&gt; &lt;tibb~ year~ 0.334 0.0196 17.0 #&gt; # ... with 278 more rows, and 2 more variables: p.value &lt;dbl&gt;, augment &lt;list&gt; gapminder_summary %&gt;% unnest(augment) #&gt; # A tibble: 1,704 x 15 #&gt; continent country data model glance tidy lifeExp year1950 .fitted .se.fit #&gt; &lt;fct&gt; &lt;fct&gt; &lt;lis&gt; &lt;lis&gt; &lt;list&gt; &lt;lis&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Africa Algeria &lt;tib~ &lt;lm&gt; &lt;tibb~ &lt;tib~ 43.1 2 43.4 0.718 #&gt; 2 Africa Algeria &lt;tib~ &lt;lm&gt; &lt;tibb~ &lt;tib~ 45.7 7 46.2 0.627 #&gt; 3 Africa Algeria &lt;tib~ &lt;lm&gt; &lt;tibb~ &lt;tib~ 48.3 12 49.1 0.544 #&gt; 4 Africa Algeria &lt;tib~ &lt;lm&gt; &lt;tibb~ &lt;tib~ 51.4 17 51.9 0.472 #&gt; 5 Africa Algeria &lt;tib~ &lt;lm&gt; &lt;tibb~ &lt;tib~ 54.5 22 54.8 0.416 #&gt; 6 Africa Algeria &lt;tib~ &lt;lm&gt; &lt;tibb~ &lt;tib~ 58.0 27 57.6 0.386 #&gt; # ... with 1,698 more rows, and 5 more variables: .resid &lt;dbl&gt;, .hat &lt;dbl&gt;, #&gt; # .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt; A similar case can be found at 9.1 6.3.2 Example: Multicple hoice data multiple_choice &lt;- tibble(method = c( &quot;CNNs&quot;, &quot;Bayesian, Logistic Regression&quot;, &quot;Data Visualization, Decision Trees&quot;, &quot;Linear Regression, A/B Testing&quot;, &quot;Data Visualization, Text Analytics&quot; )) multiple_choice %&gt;% mutate(method = str_split(method, &quot;,&quot;)) %&gt;% unnest(method) #&gt; mutate: converted &#39;method&#39; from character to list (0 new NA) #&gt; # A tibble: 9 x 1 #&gt; method #&gt; &lt;chr&gt; #&gt; 1 &quot;CNNs&quot; #&gt; 2 &quot;Bayesian&quot; #&gt; 3 &quot; Logistic Regression&quot; #&gt; 4 &quot;Data Visualization&quot; #&gt; 5 &quot; Decision Trees&quot; #&gt; 6 &quot;Linear Regression&quot; #&gt; # ... with 3 more rows The trick here is that str_split() creates a list column, and then unnest() can unnest the column. A more general function separate_rows() in this case can be found at 6.5.1 Then we can do count() and plot the most frequent methods mentioned. "],
["rectangling.html", "6.4 Rectangling", " 6.4 Rectangling Rectangling is the art and craft of taking a deeply nested list (often sourced from wild caught JSON or XML) and taming it into a tidy data set of rows and columns. There are three functions from tidyr that are particularly useful for rectangling: unnest_longer() takes each element of a list-column and makes a new row. unnest_wider() takes each element of a list-column and makes a new column. unnest_auto() guesses whether you want unnest_longer() or unnest_wider(). hoist() is similar to unnest_wider() but only plucks out selected components, and can reach down multiple levels. A very large number of data rectangling problems can be solved by combining these functions with a splash of dplyr (largely eliminating prior approaches that combined mutate() with multiple purrr::map()s). To illustrate these techniques, we’ll use the repurrrsive package, which provides a number deeply nested lists originally mostly captured from web APIs. library(repurrrsive) 6.4.1 Github users We’ll start with gh_users, a list which contains information about six GitHub users. listviewer::jsonedit(gh_users) Each user is a named list, where each element represents a column: To begin, we put the gh_users list into a data frame: (users &lt;- tibble(user = gh_users)) #&gt; # A tibble: 6 x 1 #&gt; user #&gt; &lt;list&gt; #&gt; 1 &lt;named list [30]&gt; #&gt; 2 &lt;named list [30]&gt; #&gt; 3 &lt;named list [30]&gt; #&gt; 4 &lt;named list [30]&gt; #&gt; 5 &lt;named list [30]&gt; #&gt; 6 &lt;named list [30]&gt; Each element of column user is yet another list, where each element represents a column. names(users$user[[1]]) #&gt; [1] &quot;login&quot; &quot;id&quot; &quot;avatar_url&quot; #&gt; [4] &quot;gravatar_id&quot; &quot;url&quot; &quot;html_url&quot; #&gt; [7] &quot;followers_url&quot; &quot;following_url&quot; &quot;gists_url&quot; #&gt; [10] &quot;starred_url&quot; &quot;subscriptions_url&quot; &quot;organizations_url&quot; #&gt; [13] &quot;repos_url&quot; &quot;events_url&quot; &quot;received_events_url&quot; #&gt; [16] &quot;type&quot; &quot;site_admin&quot; &quot;name&quot; #&gt; [19] &quot;company&quot; &quot;blog&quot; &quot;location&quot; #&gt; [22] &quot;email&quot; &quot;hireable&quot; &quot;bio&quot; #&gt; [25] &quot;public_repos&quot; &quot;public_gists&quot; &quot;followers&quot; #&gt; [28] &quot;following&quot; &quot;created_at&quot; &quot;updated_at&quot; Obviously we could use unnest_wider() to turn the list components into columns: users %&gt;% unnest_wider(user) #&gt; # A tibble: 6 x 30 #&gt; login id avatar_url gravatar_id url html_url followers_url following_url #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 gabo~ 6.60e5 https://a~ &quot;&quot; http~ https:/~ https://api.~ https://api.~ #&gt; 2 jenn~ 5.99e5 https://a~ &quot;&quot; http~ https:/~ https://api.~ https://api.~ #&gt; 3 jtle~ 1.57e6 https://a~ &quot;&quot; http~ https:/~ https://api.~ https://api.~ #&gt; 4 juli~ 1.25e7 https://a~ &quot;&quot; http~ https:/~ https://api.~ https://api.~ #&gt; 5 leep~ 3.51e6 https://a~ &quot;&quot; http~ https:/~ https://api.~ https://api.~ #&gt; 6 masa~ 8.36e6 https://a~ &quot;&quot; http~ https:/~ https://api.~ https://api.~ #&gt; # ... with 22 more variables: gists_url &lt;chr&gt;, starred_url &lt;chr&gt;, #&gt; # subscriptions_url &lt;chr&gt;, organizations_url &lt;chr&gt;, repos_url &lt;chr&gt;, #&gt; # events_url &lt;chr&gt;, received_events_url &lt;chr&gt;, type &lt;chr&gt;, site_admin &lt;lgl&gt;, #&gt; # name &lt;chr&gt;, company &lt;chr&gt;, blog &lt;chr&gt;, location &lt;chr&gt;, email &lt;chr&gt;, #&gt; # public_repos &lt;int&gt;, public_gists &lt;int&gt;, followers &lt;int&gt;, following &lt;int&gt;, #&gt; # created_at &lt;chr&gt;, updated_at &lt;chr&gt;, bio &lt;chr&gt;, hireable &lt;lgl&gt; But in this case, there are many components and we don’t need most of them so we can instead use hoist(). hoist() allows us to pull out selected components using the same syntax as purrr::pluck(): users %&gt;% hoist(user, followers = &quot;followers&quot;, login = &quot;login&quot;, url = &quot;html_url&quot;) #&gt; # A tibble: 6 x 4 #&gt; followers login url user #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; #&gt; 1 303 gaborcsardi https://github.com/gaborcsardi &lt;named list [27]&gt; #&gt; 2 780 jennybc https://github.com/jennybc &lt;named list [27]&gt; #&gt; 3 3958 jtleek https://github.com/jtleek &lt;named list [27]&gt; #&gt; 4 115 juliasilge https://github.com/juliasilge &lt;named list [27]&gt; #&gt; 5 213 leeper https://github.com/leeper &lt;named list [27]&gt; #&gt; 6 34 masalmon https://github.com/masalmon &lt;named list [27]&gt; hoist() 从列表列中提取出指明的元素作为新变量，保留余下的元素 hoist() removes the named components from the user list-column, so you can think of it as moving components out of the inner list into the top-level data frame 6.4.2 Github repos We start off gh_repos similarly, by putting it in a tibble: repos &lt;- tibble(repo = gh_repos) repos #&gt; # A tibble: 6 x 1 #&gt; repo #&gt; &lt;list&gt; #&gt; 1 &lt;list [30]&gt; #&gt; 2 &lt;list [30]&gt; #&gt; 3 &lt;list [30]&gt; #&gt; 4 &lt;list [26]&gt; #&gt; 5 &lt;list [30]&gt; #&gt; 6 &lt;list [30]&gt; By comparison, gh_repos is more nested than gh_users, with elements in the 2nd hierarchy being repositorys that gh_users own, and thus requires one more level of information to record each repo. listviewer::jsonedit(gh_repos) This time the elements of user are a list of repositories that belong to that user. These are observations, so should become new rows, so we use unnest_longer() rather than unnest_wider(): repos &lt;- repos %&gt;% unnest_longer(repo) repos #&gt; # A tibble: 176 x 1 #&gt; repo #&gt; &lt;list&gt; #&gt; 1 &lt;named list [68]&gt; #&gt; 2 &lt;named list [68]&gt; #&gt; 3 &lt;named list [68]&gt; #&gt; 4 &lt;named list [68]&gt; #&gt; 5 &lt;named list [68]&gt; #&gt; 6 &lt;named list [68]&gt; #&gt; # ... with 170 more rows Now each rwo representes a repository, then we can use unnest_wider() or hoist(): repos %&gt;% hoist(repo, login = list(&quot;owner&quot;, &quot;login&quot;), name = &quot;name&quot;, homepage = &quot;homepage&quot;, watchers = &quot;watchers_count&quot; ) #&gt; # A tibble: 176 x 5 #&gt; login name homepage watchers repo #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;list&gt; #&gt; 1 gaborcsardi after &lt;NA&gt; 5 &lt;named list [65]&gt; #&gt; 2 gaborcsardi argufy &lt;NA&gt; 19 &lt;named list [65]&gt; #&gt; 3 gaborcsardi ask &lt;NA&gt; 5 &lt;named list [65]&gt; #&gt; 4 gaborcsardi baseimports &lt;NA&gt; 0 &lt;named list [65]&gt; #&gt; 5 gaborcsardi citest &lt;NA&gt; 0 &lt;named list [65]&gt; #&gt; 6 gaborcsardi clisymbols &quot;&quot; 18 &lt;named list [65]&gt; #&gt; # ... with 170 more rows Note the use of list(\"owner\", \"login\"): this allows us to reach two levels deep inside of a list using the same syntax as purrr::pluck(). An alternative approach would be to pull out just owner and then put each element of it in a column: repos %&gt;% hoist(repo, owner = &quot;owner&quot;) %&gt;% unnest_wider(owner) #&gt; # A tibble: 176 x 18 #&gt; login id avatar_url gravatar_id url html_url followers_url following_url #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 gabo~ 660288 https://a~ &quot;&quot; http~ https:/~ https://api.~ https://api.~ #&gt; 2 gabo~ 660288 https://a~ &quot;&quot; http~ https:/~ https://api.~ https://api.~ #&gt; 3 gabo~ 660288 https://a~ &quot;&quot; http~ https:/~ https://api.~ https://api.~ #&gt; 4 gabo~ 660288 https://a~ &quot;&quot; http~ https:/~ https://api.~ https://api.~ #&gt; 5 gabo~ 660288 https://a~ &quot;&quot; http~ https:/~ https://api.~ https://api.~ #&gt; 6 gabo~ 660288 https://a~ &quot;&quot; http~ https:/~ https://api.~ https://api.~ #&gt; # ... with 170 more rows, and 10 more variables: gists_url &lt;chr&gt;, #&gt; # starred_url &lt;chr&gt;, subscriptions_url &lt;chr&gt;, organizations_url &lt;chr&gt;, #&gt; # repos_url &lt;chr&gt;, events_url &lt;chr&gt;, received_events_url &lt;chr&gt;, type &lt;chr&gt;, #&gt; # site_admin &lt;lgl&gt;, repo &lt;list&gt; Instead of looking at the list and carefully thinking about whether it needs to become rows or columns, you can use unnest_auto(). It uses a handful of heuristics to figure out whether unnest_longer() or unnest_wider() is appropriate, and tells you about its reasoning. tibble(repo = gh_repos) %&gt;% unnest_auto(repo) %&gt;% unnest_auto(repo) #&gt; Using `unnest_longer(repo)`; no element has names #&gt; Using `unnest_wider(repo)`; elements have 68 names in common #&gt; # A tibble: 176 x 67 #&gt; id name full_name owner private html_url description fork url #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;lis&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; #&gt; 1 6.12e7 after gaborcsa~ &lt;nam~ FALSE https:/~ Run Code i~ FALSE http~ #&gt; 2 4.05e7 argu~ gaborcsa~ &lt;nam~ FALSE https:/~ Declarativ~ FALSE http~ #&gt; 3 3.64e7 ask gaborcsa~ &lt;nam~ FALSE https:/~ Friendly C~ FALSE http~ #&gt; 4 3.49e7 base~ gaborcsa~ &lt;nam~ FALSE https:/~ Do we get ~ FALSE http~ #&gt; 5 6.16e7 cite~ gaborcsa~ &lt;nam~ FALSE https:/~ Test R pac~ TRUE http~ #&gt; 6 3.39e7 clis~ gaborcsa~ &lt;nam~ FALSE https:/~ Unicode sy~ FALSE http~ #&gt; # ... with 170 more rows, and 58 more variables: forks_url &lt;chr&gt;, #&gt; # keys_url &lt;chr&gt;, collaborators_url &lt;chr&gt;, teams_url &lt;chr&gt;, hooks_url &lt;chr&gt;, #&gt; # issue_events_url &lt;chr&gt;, events_url &lt;chr&gt;, assignees_url &lt;chr&gt;, #&gt; # branches_url &lt;chr&gt;, tags_url &lt;chr&gt;, blobs_url &lt;chr&gt;, git_tags_url &lt;chr&gt;, #&gt; # git_refs_url &lt;chr&gt;, trees_url &lt;chr&gt;, statuses_url &lt;chr&gt;, #&gt; # languages_url &lt;chr&gt;, stargazers_url &lt;chr&gt;, contributors_url &lt;chr&gt;, #&gt; # subscribers_url &lt;chr&gt;, subscription_url &lt;chr&gt;, commits_url &lt;chr&gt;, #&gt; # git_commits_url &lt;chr&gt;, comments_url &lt;chr&gt;, issue_comment_url &lt;chr&gt;, #&gt; # contents_url &lt;chr&gt;, compare_url &lt;chr&gt;, merges_url &lt;chr&gt;, archive_url &lt;chr&gt;, #&gt; # downloads_url &lt;chr&gt;, issues_url &lt;chr&gt;, pulls_url &lt;chr&gt;, #&gt; # milestones_url &lt;chr&gt;, notifications_url &lt;chr&gt;, labels_url &lt;chr&gt;, #&gt; # releases_url &lt;chr&gt;, deployments_url &lt;chr&gt;, created_at &lt;chr&gt;, #&gt; # updated_at &lt;chr&gt;, pushed_at &lt;chr&gt;, git_url &lt;chr&gt;, ssh_url &lt;chr&gt;, #&gt; # clone_url &lt;chr&gt;, svn_url &lt;chr&gt;, size &lt;int&gt;, stargazers_count &lt;int&gt;, #&gt; # watchers_count &lt;int&gt;, language &lt;chr&gt;, has_issues &lt;lgl&gt;, #&gt; # has_downloads &lt;lgl&gt;, has_wiki &lt;lgl&gt;, has_pages &lt;lgl&gt;, forks_count &lt;int&gt;, #&gt; # open_issues_count &lt;int&gt;, forks &lt;int&gt;, open_issues &lt;int&gt;, watchers &lt;int&gt;, #&gt; # default_branch &lt;chr&gt;, homepage &lt;chr&gt; 6.4.3 Game of Throne characters got_chars has a similar structure to gh_users: it’s a list of named lists, where each element of the inner list describes some attribute of a GoT character. listviewer::jsonedit(got_chars) We start in the same way, first by creating a data frame and then by unnesting each component into a column: chars &lt;- tibble(char = got_chars) chars #&gt; # A tibble: 30 x 1 #&gt; char #&gt; &lt;list&gt; #&gt; 1 &lt;named list [18]&gt; #&gt; 2 &lt;named list [18]&gt; #&gt; 3 &lt;named list [18]&gt; #&gt; 4 &lt;named list [18]&gt; #&gt; 5 &lt;named list [18]&gt; #&gt; 6 &lt;named list [18]&gt; #&gt; # ... with 24 more rows chars2 &lt;- chars %&gt;% unnest_wider(char) chars2 #&gt; # A tibble: 30 x 18 #&gt; url id name gender culture born died alive titles aliases father #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;list&gt; &lt;list&gt; &lt;chr&gt; #&gt; 1 http~ 1022 Theo~ Male &quot;Ironb~ &quot;In ~ &quot;&quot; TRUE &lt;chr ~ &lt;chr [~ &quot;&quot; #&gt; 2 http~ 1052 Tyri~ Male &quot;&quot; &quot;In ~ &quot;&quot; TRUE &lt;chr ~ &lt;chr [~ &quot;&quot; #&gt; 3 http~ 1074 Vict~ Male &quot;Ironb~ &quot;In ~ &quot;&quot; TRUE &lt;chr ~ &lt;chr [~ &quot;&quot; #&gt; 4 http~ 1109 Will Male &quot;&quot; &quot;&quot; &quot;In ~ FALSE &lt;chr ~ &lt;chr [~ &quot;&quot; #&gt; 5 http~ 1166 Areo~ Male &quot;Norvo~ &quot;In ~ &quot;&quot; TRUE &lt;chr ~ &lt;chr [~ &quot;&quot; #&gt; 6 http~ 1267 Chett Male &quot;&quot; &quot;At ~ &quot;In ~ FALSE &lt;chr ~ &lt;chr [~ &quot;&quot; #&gt; # ... with 24 more rows, and 7 more variables: mother &lt;chr&gt;, spouse &lt;chr&gt;, #&gt; # allegiances &lt;list&gt;, books &lt;list&gt;, povBooks &lt;list&gt;, tvSeries &lt;list&gt;, #&gt; # playedBy &lt;list&gt; This is more complex than gh_users because some component of char are themselves a list, giving us a collection of list-columns: chars2 %&gt;% select_if(is.list) #&gt; select_if: dropped 11 variables (url, id, name, gender, culture, …) #&gt; # A tibble: 30 x 7 #&gt; titles aliases allegiances books povBooks tvSeries playedBy #&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 &lt;chr [3]&gt; &lt;chr [4]&gt; &lt;chr [1]&gt; &lt;chr [3]&gt; &lt;chr [2]&gt; &lt;chr [6]&gt; &lt;chr [1]&gt; #&gt; 2 &lt;chr [2]&gt; &lt;chr [11]&gt; &lt;chr [1]&gt; &lt;chr [2]&gt; &lt;chr [4]&gt; &lt;chr [6]&gt; &lt;chr [1]&gt; #&gt; 3 &lt;chr [2]&gt; &lt;chr [1]&gt; &lt;chr [1]&gt; &lt;chr [3]&gt; &lt;chr [2]&gt; &lt;chr [1]&gt; &lt;chr [1]&gt; #&gt; 4 &lt;chr [1]&gt; &lt;chr [1]&gt; &lt;??? [1]&gt; &lt;chr [1]&gt; &lt;chr [1]&gt; &lt;chr [1]&gt; &lt;chr [1]&gt; #&gt; 5 &lt;chr [1]&gt; &lt;chr [1]&gt; &lt;chr [1]&gt; &lt;chr [3]&gt; &lt;chr [2]&gt; &lt;chr [2]&gt; &lt;chr [1]&gt; #&gt; 6 &lt;chr [1]&gt; &lt;chr [1]&gt; &lt;??? [1]&gt; &lt;chr [2]&gt; &lt;chr [1]&gt; &lt;chr [1]&gt; &lt;chr [1]&gt; #&gt; # ... with 24 more rows What you do next will depend on the purposes of the analysis. Maybe you want a row for every book and TV series that the character appears in: chars2 %&gt;% select(name, books, tvSeries) %&gt;% pivot_longer(c(books, tvSeries), names_to = &quot;media&quot;, values_to = &quot;value&quot;) %&gt;% unnest_longer(value) #&gt; select: dropped 15 variables (url, id, gender, culture, born, …) #&gt; pivot_longer: reorganized (books, tvSeries) into (media, value) [was 30x3, now 60x3] #&gt; # A tibble: 180 x 3 #&gt; name media value #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Theon Greyjoy books A Game of Thrones #&gt; 2 Theon Greyjoy books A Storm of Swords #&gt; 3 Theon Greyjoy books A Feast for Crows #&gt; 4 Theon Greyjoy tvSeries Season 1 #&gt; 5 Theon Greyjoy tvSeries Season 2 #&gt; 6 Theon Greyjoy tvSeries Season 3 #&gt; # ... with 174 more rows Or maybe you want to build a table that lets you match title to name: chars2 %&gt;% select(name, title = titles) %&gt;% unnest_longer(title) #&gt; select: renamed one variable (title) and dropped 16 variables #&gt; # A tibble: 60 x 2 #&gt; name title #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Theon Greyjoy Prince of Winterfell #&gt; 2 Theon Greyjoy Captain of Sea Bitch #&gt; 3 Theon Greyjoy Lord of the Iron Islands (by law of the green lands) #&gt; 4 Tyrion Lannister Acting Hand of the King (former) #&gt; 5 Tyrion Lannister Master of Coin (former) #&gt; 6 Victarion Greyjoy Lord Captain of the Iron Fleet #&gt; # ... with 54 more rows 6.4.4 Sharla Gelfand’s discography We’ll finish off with the most complex list, from Sharla Gelfand’s discography. We’ll start the usual way: putting the list into a single column data frame, and then widening so each component is a column. I also parse the date_added column into a real date-time: listviewer::jsonedit(discog) discs &lt;- tibble(disc = discog) %&gt;% unnest_wider(disc) %&gt;% mutate(date_added = as.POSIXct(strptime(date_added, &quot;%Y-%m-%dT%H:%M:%S&quot;))) #&gt; mutate: converted &#39;date_added&#39; from character to double (0 new NA) discs #&gt; # A tibble: 155 x 5 #&gt; instance_id date_added basic_information id rating #&gt; &lt;int&gt; &lt;dttm&gt; &lt;list&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 354823933 2019-02-16 17:48:59 &lt;named list [11]&gt; 7496378 0 #&gt; 2 354092601 2019-02-13 14:13:11 &lt;named list [11]&gt; 4490852 0 #&gt; 3 354091476 2019-02-13 14:07:23 &lt;named list [11]&gt; 9827276 0 #&gt; 4 351244906 2019-02-02 11:39:58 &lt;named list [11]&gt; 9769203 0 #&gt; 5 351244801 2019-02-02 11:39:37 &lt;named list [11]&gt; 7237138 0 #&gt; 6 351052065 2019-02-01 20:40:53 &lt;named list [11]&gt; 13117042 0 #&gt; # ... with 149 more rows At this level, we see information about when each disc was added to Sharla’s discography, not any information about the disc itself. To do that we need to widen the basic_information column: discs %&gt;% unnest_wider(basic_information) #&gt; Error: Column name `id` must not be duplicated. Unfortunately that fails because there’s an id column inside basic_information. We can quickly see what’s going on by setting names_repair = \"unique\"(default to \"check_unique\" which makes no name repair, but check they are unique): discs %&gt;% unnest_wider(basic_information, names_repair = &quot;unique&quot;) #&gt; New names: #&gt; * id -&gt; id...6 #&gt; * id -&gt; id...14 #&gt; # A tibble: 155 x 15 #&gt; instance_id date_added labels year artists id...6 thumb title #&gt; &lt;int&gt; &lt;dttm&gt; &lt;list&gt; &lt;int&gt; &lt;list&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 354823933 2019-02-16 17:48:59 &lt;list~ 2015 &lt;list ~ 7.50e6 http~ Demo #&gt; 2 354092601 2019-02-13 14:13:11 &lt;list~ 2013 &lt;list ~ 4.49e6 http~ Obse~ #&gt; 3 354091476 2019-02-13 14:07:23 &lt;list~ 2017 &lt;list ~ 9.83e6 http~ I #&gt; 4 351244906 2019-02-02 11:39:58 &lt;list~ 2017 &lt;list ~ 9.77e6 http~ Oído~ #&gt; 5 351244801 2019-02-02 11:39:37 &lt;list~ 2015 &lt;list ~ 7.24e6 http~ A Ca~ #&gt; 6 351052065 2019-02-01 20:40:53 &lt;list~ 2019 &lt;list ~ 1.31e7 http~ Tash~ #&gt; # ... with 149 more rows, and 7 more variables: formats &lt;list&gt;, #&gt; # cover_image &lt;chr&gt;, resource_url &lt;chr&gt;, master_id &lt;int&gt;, master_url &lt;chr&gt;, #&gt; # id...14 &lt;int&gt;, rating &lt;int&gt; The problem is that basic_information repeats the id column that’s also stored at the top-level, so we can just drop that: discs %&gt;% unnest_wider(basic_information, names_repair = &quot;unique&quot;) %&gt;% select(starts_with(&quot;id&quot;)) #&gt; New names: #&gt; * id -&gt; id...6 #&gt; * id -&gt; id...14 #&gt; select: dropped 13 variables (instance_id, date_added, labels, year, artists, …) #&gt; # A tibble: 155 x 2 #&gt; id...6 id...14 #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 7496378 7496378 #&gt; 2 4490852 4490852 #&gt; 3 9827276 9827276 #&gt; 4 9769203 9769203 #&gt; 5 7237138 7237138 #&gt; 6 13117042 13117042 #&gt; # ... with 149 more rows discs %&gt;% select(-id) %&gt;% unnest_wider(basic_information) #&gt; select: dropped one variable (id) #&gt; # A tibble: 155 x 14 #&gt; instance_id date_added labels year artists id thumb title #&gt; &lt;int&gt; &lt;dttm&gt; &lt;list&gt; &lt;int&gt; &lt;list&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 354823933 2019-02-16 17:48:59 &lt;list~ 2015 &lt;list ~ 7.50e6 http~ Demo #&gt; 2 354092601 2019-02-13 14:13:11 &lt;list~ 2013 &lt;list ~ 4.49e6 http~ Obse~ #&gt; 3 354091476 2019-02-13 14:07:23 &lt;list~ 2017 &lt;list ~ 9.83e6 http~ I #&gt; 4 351244906 2019-02-02 11:39:58 &lt;list~ 2017 &lt;list ~ 9.77e6 http~ Oído~ #&gt; 5 351244801 2019-02-02 11:39:37 &lt;list~ 2015 &lt;list ~ 7.24e6 http~ A Ca~ #&gt; 6 351052065 2019-02-01 20:40:53 &lt;list~ 2019 &lt;list ~ 1.31e7 http~ Tash~ #&gt; # ... with 149 more rows, and 6 more variables: formats &lt;list&gt;, #&gt; # cover_image &lt;chr&gt;, resource_url &lt;chr&gt;, master_id &lt;int&gt;, master_url &lt;chr&gt;, #&gt; # rating &lt;int&gt; Alternatively, we could use hoist() discs %&gt;% hoist(basic_information, title = &quot;title&quot;, year = &quot;year&quot;, label = list(&quot;labels&quot;, 1, &quot;name&quot;), artist = list(&quot;artists&quot;, 1, &quot;name&quot;) ) #&gt; # A tibble: 155 x 9 #&gt; instance_id date_added title year label artist basic_informati~ #&gt; &lt;int&gt; &lt;dttm&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; #&gt; 1 354823933 2019-02-16 17:48:59 Demo 2015 Tobi~ Mollot &lt;named list [9]&gt; #&gt; 2 354092601 2019-02-13 14:13:11 Obse~ 2013 La V~ Una B~ &lt;named list [9]&gt; #&gt; 3 354091476 2019-02-13 14:07:23 I 2017 La V~ S.H.I~ &lt;named list [9]&gt; #&gt; 4 351244906 2019-02-02 11:39:58 Oído~ 2017 La V~ Rata ~ &lt;named list [9]&gt; #&gt; 5 351244801 2019-02-02 11:39:37 A Ca~ 2015 Kato~ Ivy (~ &lt;named list [9]&gt; #&gt; 6 351052065 2019-02-01 20:40:53 Tash~ 2019 High~ Tashme &lt;named list [9]&gt; #&gt; # ... with 149 more rows, and 2 more variables: id &lt;int&gt;, rating &lt;int&gt; A more systematic approach would be to create separate tables for artist and label: # table for artist discs %&gt;% hoist(basic_information, artist = &quot;artists&quot;) %&gt;% select(disk_id = id, artist) %&gt;% unnest_longer(artist) %&gt;% unnest_wider(artist) #&gt; select: renamed one variable (disk_id) and dropped 4 variables #&gt; # A tibble: 167 x 8 #&gt; disk_id join name anv tracks role resource_url id #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 7496378 &quot;&quot; Mollot &quot;&quot; &quot;&quot; &quot;&quot; https://api.discogs.co~ 4.62e6 #&gt; 2 4490852 &quot;&quot; Una Bèstia I~ &quot;&quot; &quot;&quot; &quot;&quot; https://api.discogs.co~ 3.19e6 #&gt; 3 9827276 &quot;&quot; S.H.I.T. (3) &quot;&quot; &quot;&quot; &quot;&quot; https://api.discogs.co~ 2.77e6 #&gt; 4 9769203 &quot;&quot; Rata Negra &quot;&quot; &quot;&quot; &quot;&quot; https://api.discogs.co~ 4.28e6 #&gt; 5 7237138 &quot;&quot; Ivy (18) &quot;&quot; &quot;&quot; &quot;&quot; https://api.discogs.co~ 3.60e6 #&gt; 6 13117042 &quot;&quot; Tashme &quot;&quot; &quot;&quot; &quot;&quot; https://api.discogs.co~ 5.21e6 #&gt; # ... with 161 more rows # table for label discs %&gt;% hoist(basic_information, format = &quot;formats&quot;) %&gt;% select(disk_id = id, format) %&gt;% unnest_longer(format) %&gt;% unnest_wider(format) %&gt;% unnest_longer(descriptions) #&gt; select: renamed one variable (disk_id) and dropped 4 variables #&gt; # A tibble: 281 x 5 #&gt; disk_id descriptions text name qty #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 7496378 &quot;Numbered&quot; Black Cassette 1 #&gt; 2 4490852 &quot;LP&quot; &lt;NA&gt; Vinyl 1 #&gt; 3 9827276 &quot;7\\&quot;&quot; &lt;NA&gt; Vinyl 1 #&gt; 4 9827276 &quot;45 RPM&quot; &lt;NA&gt; Vinyl 1 #&gt; 5 9827276 &quot;EP&quot; &lt;NA&gt; Vinyl 1 #&gt; 6 9769203 &quot;LP&quot; &lt;NA&gt; Vinyl 1 #&gt; # ... with 275 more rows "],
["separate-and-untie.html", "6.5 separate() and untie()", " 6.5 separate() and untie() separate() 和 untie() 函数则是为了解决以下问题：多个变量挤在了同一列中，或者一个变量分散到了不同列中。 6.5.1 separate() table3 #&gt; # A tibble: 6 x 3 #&gt; country year rate #&gt; * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 Afghanistan 1999 745/19987071 #&gt; 2 Afghanistan 2000 2666/20595360 #&gt; 3 Brazil 1999 37737/172006362 #&gt; 4 Brazil 2000 80488/174504898 #&gt; 5 China 1999 212258/1272915272 #&gt; 6 China 2000 213766/1280428583 在 table3 中，rate 同时包含了 cases 和 population 两个变量，我们需要把它拆分(separate)为两列，separate() 函数可以将这一混杂的列拆分成多个变量，它包含以下四个主要参数： * data: 需要调整的数据框 * col: 需要进行拆分的列的列名 * into: 拆分后新生成变量的列名，格式为字符串向量 * sep: 对如何拆分原变量的描述，其可以是正则表达式，如 _ 表示通过下划线拆分，或 [^a-z] 表示通过任意非字符字母拆分，或一个指定位置的整数。默认情况下，sep将认定一个非字符字母进行划分 # 这个例子里，sep 不是必需的 table3 %&gt;% separate(col = rate,into = c(&quot;cases&quot;,&quot;population&quot;)) #&gt; # A tibble: 6 x 4 #&gt; country year cases population #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Afghanistan 1999 745 19987071 #&gt; 2 Afghanistan 2000 2666 20595360 #&gt; 3 Brazil 1999 37737 172006362 #&gt; 4 Brazil 2000 80488 174504898 #&gt; 5 China 1999 212258 1272915272 #&gt; 6 China 2000 213766 1280428583 整理的图示： 注意，以上输出的tibble中，cases 和 population被设定为字符串类型，使用convert = T将其转换为数值变量 table3 %&gt;% separate(rate, into = c(&quot;cases&quot;, &quot;population&quot;), convert = T) #&gt; # A tibble: 6 x 4 #&gt; country year cases population #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afghanistan 1999 745 19987071 #&gt; 2 Afghanistan 2000 2666 20595360 #&gt; 3 Brazil 1999 37737 172006362 #&gt; 4 Brazil 2000 80488 174504898 #&gt; 5 China 1999 212258 1272915272 #&gt; 6 China 2000 213766 1280428583 A seemingly similar function tidyr::separate_rows() separate existing columns based on sep, and then breaks each component into new rows, instead of columns: df &lt;- tibble( x = 1:3, y = c(&quot;a&quot;, &quot;d,e,f&quot;, &quot;g,h&quot;), z = c(&quot;1&quot;, &quot;2,3,4&quot;, &quot;5,6&quot;) ) df %&gt;% separate_rows(y, z, sep = &quot;,&quot;) #&gt; # A tibble: 6 x 3 #&gt; x y z #&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1 a 1 #&gt; 2 2 d 2 #&gt; 3 2 e 3 #&gt; 4 2 f 4 #&gt; 5 3 g 5 #&gt; 6 3 h 6 The multiple choice data mentioned in 6.3.2 can easily be solved when using separate_rows(): multiple_choice %&gt;% separate_rows(method, sep = &quot;,&quot;) #&gt; # A tibble: 9 x 1 #&gt; method #&gt; &lt;chr&gt; #&gt; 1 &quot;CNNs&quot; #&gt; 2 &quot;Bayesian&quot; #&gt; 3 &quot; Logistic Regression&quot; #&gt; 4 &quot;Data Visualization&quot; #&gt; 5 &quot; Decision Trees&quot; #&gt; 6 &quot;Linear Regression&quot; #&gt; # ... with 3 more rows 6.5.2 unite() unite() 是 separate() 的逆运算——它可以将多列合并为一列. 在 table5 中，原来的 year 变量被拆成了两个列，可以用 unite()，只需要指定要合并后的列名和要合并的列。默认情况下，新列中将用_分隔符 table5 #&gt; # A tibble: 6 x 4 #&gt; country century year rate #&gt; * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Afghanistan 19 99 745/19987071 #&gt; 2 Afghanistan 20 00 2666/20595360 #&gt; 3 Brazil 19 99 37737/172006362 #&gt; 4 Brazil 20 00 80488/174504898 #&gt; 5 China 19 99 212258/1272915272 #&gt; 6 China 20 00 213766/1280428583 unite(table5, col = year, century, year) #&gt; # A tibble: 6 x 3 #&gt; country year rate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Afghanistan 19_99 745/19987071 #&gt; 2 Afghanistan 20_00 2666/20595360 #&gt; 3 Brazil 19_99 37737/172006362 #&gt; 4 Brazil 20_00 80488/174504898 #&gt; 5 China 19_99 212258/1272915272 #&gt; 6 China 20_00 213766/1280428583 sep = \"\" 可以取消分隔符： unite(table5, col = year, century, year, sep=&quot;&quot;) #&gt; # A tibble: 6 x 3 #&gt; country year rate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Afghanistan 1999 745/19987071 #&gt; 2 Afghanistan 2000 2666/20595360 #&gt; 3 Brazil 1999 37737/172006362 #&gt; 4 Brazil 2000 80488/174504898 #&gt; 5 China 1999 212258/1272915272 #&gt; 6 China 2000 213766/1280428583 整理的图示： table6 &lt;- unite(table5,col = year,century,year,sep=&quot;&quot;) table5 #&gt; # A tibble: 6 x 4 #&gt; country century year rate #&gt; * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Afghanistan 19 99 745/19987071 #&gt; 2 Afghanistan 20 00 2666/20595360 #&gt; 3 Brazil 19 99 37737/172006362 #&gt; 4 Brazil 20 00 80488/174504898 #&gt; 5 China 19 99 212258/1272915272 #&gt; 6 China 20 00 213766/1280428583 6.5.3 Exercises Exercise 6.4 separate()中的extra和fill参数的作用是什么？用下面两个数据框进行实验： tibble(x = c(&quot;a,b,c&quot;, &quot;d,e,f,g&quot;, &quot;h,i,j&quot;)) %&gt;% separate(x, into = c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) #&gt; # A tibble: 3 x 3 #&gt; one two three #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 a b c #&gt; 2 d e f #&gt; 3 h i j tibble(x = c(&quot;a,b,c&quot;, &quot;d,e&quot;, &quot;f,g,i&quot;)) %&gt;% separate(x, into = c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) #&gt; # A tibble: 3 x 3 #&gt; one two three #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 a b c #&gt; 2 d e &lt;NA&gt; #&gt; 3 f g i extra 用来告诉 separate() 函数如何处理分列过程中多出来的元素(too many pieces，即 into 指定的列数小于原数据中某行可分的元素个数)，fill 负责如何处理元素不够的情况(not enough pieces，即into指定的列数大于原数据中某行可分的元素个数)。默认情况下，extra = \"drop\"，separate() 将丢弃多余的元素，并生成一条警告信息： tibble(x = c(&quot;a,b,c&quot;, &quot;d,e,f,g&quot;, &quot;h,i,j&quot;)) %&gt;% separate(x, into = c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;), extra = &quot;drop&quot;) #&gt; # A tibble: 3 x 3 #&gt; one two three #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 a b c #&gt; 2 d e f #&gt; 3 h i j extra = \"merge\"将把多余的元素和前一个元素当做一个整体： tibble(x = c(&quot;a, b, c&quot;, &quot;d, e, f, g&quot;, &quot;h, i, j&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;), extra = &quot;merge&quot;) #&gt; # A tibble: 3 x 3 #&gt; one two three #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 a b c #&gt; 2 d e f, g #&gt; 3 h i j 对于元素过少的情况，默认的fill = \"warn\"将会用NA进行填充，但会生成一条警告。fill = \"right\"会尽可能让靠左的列拥有可用的元素，用NA填充右边的列；fill = \"left\"正好相反。这两种手动设置都不会产生warning: tibble(x = c(&quot;a, b, c&quot;, &quot;d, e, &quot;, &quot;h, i, j&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;), fill = &quot;left&quot;) #&gt; # A tibble: 3 x 3 #&gt; one two three #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 a b &quot;c&quot; #&gt; 2 d e &quot;&quot; #&gt; 3 h i &quot;j&quot; tibble(x = c(&quot;a,b,c&quot;, &quot;d, e, &quot;, &quot;h, i, j&quot;)) %&gt;% separate(x, c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;),fill = &quot;right&quot;) #&gt; # A tibble: 3 x 3 #&gt; one two three #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 a b &quot;c&quot; #&gt; 2 d e &quot;&quot; #&gt; 3 h i &quot;j&quot; 2.unite()和separate()均有一个remove参数，它的作用是什么？ remove控制是否在unite()或separate()输出的数据框中保留原来的列，默认remove = T。如果想保留原来未合并/分离的格列，可以设置remove = F table5 #&gt; # A tibble: 6 x 4 #&gt; country century year rate #&gt; * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Afghanistan 19 99 745/19987071 #&gt; 2 Afghanistan 20 00 2666/20595360 #&gt; 3 Brazil 19 99 37737/172006362 #&gt; 4 Brazil 20 00 80488/174504898 #&gt; 5 China 19 99 212258/1272915272 #&gt; 6 China 20 00 213766/1280428583 table5 %&gt;% unite(col = year_unite,century,year,sep = &quot;&quot;,remove = F) #&gt; # A tibble: 6 x 5 #&gt; country year_unite century year rate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Afghanistan 1999 19 99 745/19987071 #&gt; 2 Afghanistan 2000 20 00 2666/20595360 #&gt; 3 Brazil 1999 19 99 37737/172006362 #&gt; 4 Brazil 2000 20 00 80488/174504898 #&gt; 5 China 1999 19 99 212258/1272915272 #&gt; 6 China 2000 20 00 213766/1280428583 Exercise 6.5 探究 tidyr 中一个与 separate() 类似的函数 extract() 的用法 separate()函数的分列操作是基于参数 sep 的，无论是给 sep 传入字符串指定分隔符，还是用数值指定分隔的位置，separate() 必须要有一个分隔符才能正常运作（可以把sep = n看做第 n 个和第 n+1 个元素之间的一个空白分隔符） extract()用一个正则表达式regex描述要分隔的列col中存在的模式，在正则表达式中的每个子表达式(用()定义)将被认为是into中的一个元素，因此，extract()比separate()使用起来更加广泛灵活。例如下面的数据集无法用separate()分列，因为无法用一个各行的分隔符(的位置)不一样，但用extract()中的正则表达式就很简单： tibble(x = c(&quot;X1&quot;, &quot;X20&quot;, &quot;AA11&quot;, &quot;AA2&quot;)) %&gt;% extract(x, c(&quot;variable&quot;, &quot;id&quot;), regex = &quot;([A-Z]+)([0-9]+)&quot;) #&gt; # A tibble: 4 x 2 #&gt; variable id #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 X 1 #&gt; 2 X 20 #&gt; 3 AA 11 #&gt; 4 AA 2 适当设计regex，实现的效果可以与设置sep完全一致： # example with separators tibble(x = c(&quot;X_1&quot;, &quot;X_2&quot;, &quot;AA_1&quot;, &quot;AA_2&quot;)) %&gt;% extract(x, c(&quot;variable&quot;, &quot;id&quot;), regex = &quot;([A-Z]+)_([0-9])&quot;) #&gt; # A tibble: 4 x 2 #&gt; variable id #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 X 1 #&gt; 2 X 2 #&gt; 3 AA 1 #&gt; 4 AA 2 # example with position tibble(x = c(&quot;X1&quot;, &quot;X2&quot;, &quot;Y1&quot;, &quot;Y2&quot;)) %&gt;% extract(x, c(&quot;variable&quot;, &quot;id&quot;), regex = &quot;([A-Z])([0-9])&quot;) #&gt; # A tibble: 4 x 2 #&gt; variable id #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 X 1 #&gt; 2 X 2 #&gt; 3 Y 1 #&gt; 4 Y 2 "],
["tidyr-missing.html", "6.6 Handling missing values", " 6.6 Handling missing values 数据整理改变了数据的呈现方式，随之而来的一个话题便是缺失值。通常当我们泛泛地使用“缺失值 (missing value)” 这个名词的时候，其实是指以下两种“缺失”方式中的某一种： 显式缺失(Explicitly missing): 在数据中用 NA 标识 隐式缺失(Implicitly missing): 未出现在数据中的值 R for Data Science 中对这两种缺失的概括： &gt; An explicit missing value is the presence of an absence; an implicit missing value is the absence of a presence. 通过一个简单的数据框区分两种数据缺失的方式： stocks &lt;- tibble( year = c(2015, 2015, 2015, 2015, 2016, 2016, 2016), qtr = c( 1, 2, 3, 4, 2, 3, 4), return = c(1.88, 0.59, 0.35, NA, 0.92, 0.17, 2.66) ) stocks #&gt; # A tibble: 7 x 3 #&gt; year qtr return #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2015 1 1.88 #&gt; 2 2015 2 0.59 #&gt; 3 2015 3 0.35 #&gt; 4 2015 4 NA #&gt; 5 2016 2 0.92 #&gt; 6 2016 3 0.17 #&gt; # ... with 1 more row 我们很容易找到 stocks 第四条观测在 return 上的一个 NA ，因为它是显式缺失的。另一个隐式缺失的值是 (year = 2016, qtr = 1) 对应的观测，它没有出现在数据集中。 数据呈现方式上的改变可以将隐式缺失值变成显式。比如，用 pivot_wider()函数构造以 year 为行字段，以 return 为值的透视表,这样就会产生一个属于水平 (year = 2016, qtr = 1)的单元格： stocks %&gt;% pivot_wider(names_from = year, values_from = return) #&gt; pivot_wider: reorganized (year, return) into (2015, 2016) [was 7x3, now 4x3] #&gt; # A tibble: 4 x 3 #&gt; qtr `2015` `2016` #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 1.88 NA #&gt; 2 2 0.59 0.92 #&gt; 3 3 0.35 0.17 #&gt; 4 4 NA 2.66 现在，再使用 pivot_longer() 不能得到原来的数据框，因为将比原来多出一行显示的缺失值 stocks %&gt;% pivot_wider(names_from = year, values_from = return) %&gt;% pivot_longer(-qtr, names_to = &quot;year&quot;, values_to = &quot;return&quot;) #&gt; pivot_wider: reorganized (year, return) into (2015, 2016) [was 7x3, now 4x3] #&gt; pivot_longer: reorganized (2015, 2016) into (year, return) [was 4x3, now 8x3] #&gt; # A tibble: 8 x 3 #&gt; qtr year return #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 2015 1.88 #&gt; 2 1 2016 NA #&gt; 3 2 2015 0.59 #&gt; 4 2 2016 0.92 #&gt; 5 3 2015 0.35 #&gt; 6 3 2016 0.17 #&gt; # ... with 2 more rows 如果研究者认为这些缺失值是无足轻重的,values_drop_na = TRUE 将在 pivot_longer() 生成的数据框中移除含有缺失值的行，这会同时移除显式和隐式缺失值： ## 现在输出数据框比原来少一行 stocks %&gt;% pivot_wider(names_from = year, values_from = return) %&gt;% pivot_longer(-qtr, names_to = &quot;year&quot;, values_to = &quot;return&quot;, values_drop_na = TRUE) #&gt; pivot_wider: reorganized (year, return) into (2015, 2016) [was 7x3, now 4x3] #&gt; pivot_longer: reorganized (2015, 2016) into (year, return) [was 4x3, now 6x3] #&gt; # A tibble: 6 x 3 #&gt; qtr year return #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 2015 1.88 #&gt; 2 2 2015 0.59 #&gt; 3 2 2016 0.92 #&gt; 4 3 2015 0.35 #&gt; 5 3 2016 0.17 #&gt; 6 4 2016 2.66 fill() 专门用来填充缺失值,它接受一些需要填充缺失值的列，并用最近的值调换 NA，.direction 参数控制用填充的方向：direction = “up\" 将由下往上填充，NA 将被替换为它下面那一列的值；direction = \"donw\" 反之 treatment &lt;- tribble( ~ person, ~ treatment, ~response, &quot;Derrick Whitmore&quot;, 1, 7, NA, 2, 10, NA, 3, 9, &quot;Katherine Burke&quot;, 1, 4 ) treatment %&gt;% fill(person, .direction = &quot;up&quot;) #&gt; fill: changed 2 values (50%) of &#39;person&#39; (2 fewer NA) #&gt; # A tibble: 4 x 3 #&gt; person treatment response #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Derrick Whitmore 1 7 #&gt; 2 Katherine Burke 2 10 #&gt; 3 Katherine Burke 3 9 #&gt; 4 Katherine Burke 1 4 treatment %&gt;% fill(person, .direction = &quot;down&quot;) #&gt; fill: changed 2 values (50%) of &#39;person&#39; (2 fewer NA) #&gt; # A tibble: 4 x 3 #&gt; person treatment response #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Derrick Whitmore 1 7 #&gt; 2 Derrick Whitmore 2 10 #&gt; 3 Derrick Whitmore 3 9 #&gt; 4 Katherine Burke 1 4 More useful methods dealing with missing values (in tidyr and other packages) are discussed in 17 "],
["tidyr-case.html", "6.7 Case Study", " 6.7 Case Study To finish off the chapter, let’s pull together everything you’ve learned to tackle a realistic data tidying problem. The tidyr::who dataset contains tuberculosis (TB) cases broken down by year, country, age, gender, and diagnosis method. The data comes from the 2014 World Health Organization Global Tuberculosis Report, available at http://www.who.int/tb/country/data/download/en/. There’s a wealth of epidemiological information in this dataset, but it’s challenging to work with the data in the form that it’s provided: who #&gt; # A tibble: 7,240 x 60 #&gt; country iso2 iso3 year new_sp_m014 new_sp_m1524 new_sp_m2534 new_sp_m3544 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Afghan~ AF AFG 1980 NA NA NA NA #&gt; 2 Afghan~ AF AFG 1981 NA NA NA NA #&gt; 3 Afghan~ AF AFG 1982 NA NA NA NA #&gt; 4 Afghan~ AF AFG 1983 NA NA NA NA #&gt; 5 Afghan~ AF AFG 1984 NA NA NA NA #&gt; 6 Afghan~ AF AFG 1985 NA NA NA NA #&gt; # ... with 7,234 more rows, and 52 more variables: new_sp_m4554 &lt;int&gt;, #&gt; # new_sp_m5564 &lt;int&gt;, new_sp_m65 &lt;int&gt;, new_sp_f014 &lt;int&gt;, #&gt; # new_sp_f1524 &lt;int&gt;, new_sp_f2534 &lt;int&gt;, new_sp_f3544 &lt;int&gt;, #&gt; # new_sp_f4554 &lt;int&gt;, new_sp_f5564 &lt;int&gt;, new_sp_f65 &lt;int&gt;, #&gt; # new_sn_m014 &lt;int&gt;, new_sn_m1524 &lt;int&gt;, new_sn_m2534 &lt;int&gt;, #&gt; # new_sn_m3544 &lt;int&gt;, new_sn_m4554 &lt;int&gt;, new_sn_m5564 &lt;int&gt;, #&gt; # new_sn_m65 &lt;int&gt;, new_sn_f014 &lt;int&gt;, new_sn_f1524 &lt;int&gt;, #&gt; # new_sn_f2534 &lt;int&gt;, new_sn_f3544 &lt;int&gt;, new_sn_f4554 &lt;int&gt;, #&gt; # new_sn_f5564 &lt;int&gt;, new_sn_f65 &lt;int&gt;, new_ep_m014 &lt;int&gt;, #&gt; # new_ep_m1524 &lt;int&gt;, new_ep_m2534 &lt;int&gt;, new_ep_m3544 &lt;int&gt;, #&gt; # new_ep_m4554 &lt;int&gt;, new_ep_m5564 &lt;int&gt;, new_ep_m65 &lt;int&gt;, #&gt; # new_ep_f014 &lt;int&gt;, new_ep_f1524 &lt;int&gt;, new_ep_f2534 &lt;int&gt;, #&gt; # new_ep_f3544 &lt;int&gt;, new_ep_f4554 &lt;int&gt;, new_ep_f5564 &lt;int&gt;, #&gt; # new_ep_f65 &lt;int&gt;, newrel_m014 &lt;int&gt;, newrel_m1524 &lt;int&gt;, #&gt; # newrel_m2534 &lt;int&gt;, newrel_m3544 &lt;int&gt;, newrel_m4554 &lt;int&gt;, #&gt; # newrel_m5564 &lt;int&gt;, newrel_m65 &lt;int&gt;, newrel_f014 &lt;int&gt;, #&gt; # newrel_f1524 &lt;int&gt;, newrel_f2534 &lt;int&gt;, newrel_f3544 &lt;int&gt;, #&gt; # newrel_f4554 &lt;int&gt;, newrel_f5564 &lt;int&gt;, newrel_f65 &lt;int&gt; This is a very typical real-life example dataset. It contains redundant columns, odd variable codes, and many missing values. In short, who is messy, and we’ll need multiple steps to tidy it. Like dplyr, tidyr is designed so that each function does one thing well. That means in real-life situations you’ll usually need to string together multiple verbs into a pipeline. The best place to start is almost always to gather together the columns that are not variables. Let’s have a look at what we’ve got: * It looks like country, iso2, and iso3 are three variables that redundantly specify the country. * year is also a variable * We don’t know what all the other columns are yet, but given the structure in the variable names (e.g. new_sp_m014, new_ep_m014, new_ep_f014) these are likely to be values, not variables. So we need to pivot all the columns from new_sp_m014 to newrel_f65. We don’t know what those values represent yet, so we’ll give them the generic name \"name\". We know the cells represent the count of cases, so we’ll use the variable cases. There are a lot of missing values in the current representation, so for now we’ll use values_drop_na = TRUE just so we can focus on the values that are present. who1 &lt;- who %&gt;% pivot_longer(-(1:4), names_to = &quot;name&quot;, values_to = &quot;cases&quot;, values_drop_na = TRUE) #&gt; pivot_longer: reorganized (new_sp_m014, new_sp_m1524, new_sp_m2534, new_sp_m3544, new_sp_m4554, …) into (name, cases) [was 7240x60, now 76046x6] who1 #&gt; # A tibble: 76,046 x 6 #&gt; country iso2 iso3 year name cases #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Afghanistan AF AFG 1997 new_sp_m014 0 #&gt; 2 Afghanistan AF AFG 1997 new_sp_m1524 10 #&gt; 3 Afghanistan AF AFG 1997 new_sp_m2534 6 #&gt; 4 Afghanistan AF AFG 1997 new_sp_m3544 3 #&gt; 5 Afghanistan AF AFG 1997 new_sp_m4554 5 #&gt; 6 Afghanistan AF AFG 1997 new_sp_m5564 2 #&gt; # ... with 76,040 more rows We can get some hint of the structure of the values in the new name column by counting them: who1 %&gt;% count(name) #&gt; count: now 56 rows and 2 columns, ungrouped #&gt; # A tibble: 56 x 2 #&gt; name n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 new_ep_f014 1032 #&gt; 2 new_ep_f1524 1021 #&gt; 3 new_ep_f2534 1021 #&gt; 4 new_ep_f3544 1021 #&gt; 5 new_ep_f4554 1017 #&gt; 6 new_ep_f5564 1017 #&gt; # ... with 50 more rows You might be able to parse this out by yourself with a little thought and some experimentation, but luckily we have the data dictionary handy. It tells us: 1. The first three letters of each column denote whether the column contains new or old cases of TB. In this dataset, each column contains new cases. 2. The next two letters describe the type of TB: * rel stands for cases of relapse * ep stands for cases of extrapulmonary TB * sn stands for cases of pulmonary TB that could not be diagnosed by a pulmonary smear (smear negative) * sp stands for cases of pulmonary TB that could be diagnosed be a pulmonary smear (smear positive) 3. The sixth letter gives the sex of TB patients. The dataset groups cases by males (m) and females (f). 4. The remaining numbers gives the age group. The dataset groups cases into seven age groups: * 014 = 0 – 14 years old * 1524 = 15 – 24 years old * 2534 = 25 – 34 years old * 3544 = 35 – 44 years old * 4554 = 45 – 54 years old * 5564 = 55 – 64 years old * 65 = 65 or older We need to make a minor fix to the format of the column names: unfortunately the names are slightly inconsistent because instead of new_rel we have newrel (it’s hard to spot this here but if you don’t fix it we’ll get errors in subsequent steps). You’ll learn about str_replace() in strings, but the basic idea is pretty simple: replace the characters “newrel” with “new_rel”. This makes all variable names consistent. who2 &lt;- who1 %&gt;% mutate(key = stringr::str_replace(name, &quot;newrel&quot;, &quot;new_rel&quot;)) #&gt; mutate: new variable &#39;key&#39; with 56 unique values and 0% NA who2 #&gt; # A tibble: 76,046 x 7 #&gt; country iso2 iso3 year name cases key #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 Afghanistan AF AFG 1997 new_sp_m014 0 new_sp_m014 #&gt; 2 Afghanistan AF AFG 1997 new_sp_m1524 10 new_sp_m1524 #&gt; 3 Afghanistan AF AFG 1997 new_sp_m2534 6 new_sp_m2534 #&gt; 4 Afghanistan AF AFG 1997 new_sp_m3544 3 new_sp_m3544 #&gt; 5 Afghanistan AF AFG 1997 new_sp_m4554 5 new_sp_m4554 #&gt; 6 Afghanistan AF AFG 1997 new_sp_m5564 2 new_sp_m5564 #&gt; # ... with 76,040 more rows We can separate the values in each code with two passes of extracct()(This can also be done in the previous pivot_longer() step by passing a vector to names_to). who3 &lt;- who2 %&gt;% extract(name, into = c(&quot;type&quot;, &quot;sex&quot;, &quot;age&quot;), regex = &quot;new_(\\\\w{2,3})_([fm])(\\\\d+)&quot;) who3 #&gt; # A tibble: 76,046 x 9 #&gt; country iso2 iso3 year type sex age cases key #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 Afghanistan AF AFG 1997 sp m 014 0 new_sp_m014 #&gt; 2 Afghanistan AF AFG 1997 sp m 1524 10 new_sp_m1524 #&gt; 3 Afghanistan AF AFG 1997 sp m 2534 6 new_sp_m2534 #&gt; 4 Afghanistan AF AFG 1997 sp m 3544 3 new_sp_m3544 #&gt; 5 Afghanistan AF AFG 1997 sp m 4554 5 new_sp_m4554 #&gt; 6 Afghanistan AF AFG 1997 sp m 5564 2 new_sp_m5564 #&gt; # ... with 76,040 more rows Then, let’s also drop iso2 and iso3 since they’re redundant (see Exercise 6.6 below for proof). who4 &lt;- who3 %&gt;% select(-iso2, -iso3) #&gt; select: dropped 2 variables (iso2, iso3) Now who4 is a tidy version of tidyr::who! "],
["miscellaneous-functions.html", "6.8 Miscellaneous Functions", " 6.8 Miscellaneous Functions There are several remaining useful functions in tidyr that cannot be easily categorized. 6.8.1 chop() and unchop() Chopping and unchopping preserve the width of a data frame, changing its length. chop() makes df shorter by converting rows within each group into list-columns. unchop() makes df longer by expanding list-columns so that each element of the list-column gets its own row in the output. Note that we get one row of output for each unique combination of non-chopped variables: chop() differs from nest() in section 6.3 in that it does not collpase columns into a tibble, but into a list: df &lt;- tibble(x = c(1, 1, 1, 2, 2, 3), y = 1:6, z = 6:1) df %&gt;% chop(cols = c(y, z)) #&gt; # A tibble: 3 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 1 &lt;int [3]&gt; &lt;int [3]&gt; #&gt; 2 2 &lt;int [2]&gt; &lt;int [2]&gt; #&gt; 3 3 &lt;int [1]&gt; &lt;int [1]&gt; df %&gt;% nest(data = c(y, z)) #&gt; # A tibble: 3 x 2 #&gt; x data #&gt; &lt;dbl&gt; &lt;list&gt; #&gt; 1 1 &lt;tibble [3 x 2]&gt; #&gt; 2 2 &lt;tibble [2 x 2]&gt; #&gt; 3 3 &lt;tibble [1 x 2]&gt; unchop(): df &lt;- tibble(x = 1:4, y = list(integer(), 1L, 1:2, 1:3)) df %&gt;% unchop(y) #&gt; # A tibble: 6 x 2 #&gt; x y #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 2 1 #&gt; 2 3 1 #&gt; 3 3 2 #&gt; 4 4 1 #&gt; 5 4 2 #&gt; 6 4 3 If there’s a size-0 element (like NULL or an empty data frame), that entire row will be dropped from the output. If you want to preserve all rows, use keep_empty = TRUE to replace size-0 elements with a single row of missing values. # equivalent to df %&gt;% unnest_longer(y) df %&gt;% unchop(y, keep_empty = TRUE) #&gt; # A tibble: 7 x 2 #&gt; x y #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 NA #&gt; 2 2 1 #&gt; 3 3 1 #&gt; 4 3 2 #&gt; 5 4 1 #&gt; 6 4 2 #&gt; # ... with 1 more row # Incompatible types ------------------------------------------------- # If the list-col contains types that can not be natively df &lt;- tibble(x = 1:2, y = list(&quot;1&quot;, 1:3)) try(df %&gt;% unchop(y)) #&gt; Error : No common type for `..1$y` &lt;character&gt; and `..2$y` &lt;integer&gt;. df %&gt;% unchop(y, ptype = tibble(y = integer())) #&gt; # A tibble: 4 x 2 #&gt; x y #&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 1 #&gt; 2 2 1 #&gt; 3 2 2 #&gt; 4 2 3 df %&gt;% unchop(y, ptype = tibble(y = character())) #&gt; # A tibble: 4 x 2 #&gt; x y #&gt; &lt;int&gt; &lt;chr&gt; #&gt; 1 1 1 #&gt; 2 2 1 #&gt; 3 2 2 #&gt; 4 2 3 df %&gt;% unchop(y, ptype = tibble(y = list())) #&gt; # A tibble: 4 x 2 #&gt; x y #&gt; &lt;int&gt; &lt;list&gt; #&gt; 1 1 &lt;chr [1]&gt; #&gt; 2 2 &lt;int [1]&gt; #&gt; 3 2 &lt;int [1]&gt; #&gt; 4 2 &lt;int [1]&gt; ptype: Optionally, supply a data frame prototype for the output cols, overriding the default that will be guessed from the combination of individual value 6.8.2 uncount() Performs the opposite operation to dplyr::count(), duplicating rows according to a weighting variable (or expression) df &lt;- tibble(x = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), n = c(1, 2, 3)) uncount(df, n) #&gt; uncount: now 6 rows and one column, ungrouped #&gt; # A tibble: 6 x 1 #&gt; x #&gt; &lt;chr&gt; #&gt; 1 a #&gt; 2 b #&gt; 3 b #&gt; 4 c #&gt; 5 c #&gt; 6 c we can supply a string .id to create a new variable which gives a unique identifier for each created row: uncount(df, n, .id = &quot;id&quot;) #&gt; uncount: now 6 rows and 2 columns, ungrouped #&gt; # A tibble: 6 x 2 #&gt; x id #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 a 1 #&gt; 2 b 1 #&gt; 3 b 2 #&gt; 4 c 1 #&gt; 5 c 2 #&gt; 6 c 3 uncount() can be helpful in convertnig frequency form data to case form data, e.g: fiber &lt;- read_csv(&quot;data/Fiber.csv&quot;) #&gt; Parsed with column specification: #&gt; cols( #&gt; fiber = col_character(), #&gt; bloat = col_character(), #&gt; count = col_double() #&gt; ) fiber #&gt; # A tibble: 16 x 3 #&gt; fiber bloat count #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 bran high 0 #&gt; 2 gum high 5 #&gt; 3 both high 2 #&gt; 4 none high 0 #&gt; 5 bran medium 1 #&gt; 6 gum medium 3 #&gt; # ... with 10 more rows fiber %&gt;% uncount(count) #&gt; uncount: now 48 rows and 2 columns, ungrouped #&gt; # A tibble: 48 x 2 #&gt; fiber bloat #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 gum high #&gt; 2 gum high #&gt; 3 gum high #&gt; 4 gum high #&gt; 5 gum high #&gt; 6 both high #&gt; # ... with 42 more rows Other way that can achieve this transformation: rep(): fiber[rep(1:nrow(fiber), fiber$count), -3] #&gt; # A tibble: 48 x 2 #&gt; fiber bloat #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 gum high #&gt; 2 gum high #&gt; 3 gum high #&gt; 4 gum high #&gt; 5 gum high #&gt; 6 both high #&gt; # ... with 42 more rows 6.8.3 Exercises Exercise 6.6 在清理 who 数据集时，我们说iso2和iso3是冗余的，证明这一点 如果 iso2 和 iso3 是冗余的，则在数据集中对于变量组合 (country, year) 的每个值，都能唯一确定一个观测(因为 (country, year) 本身可以被用作键)。 who %&gt;% count(country, year) %&gt;% filter(n &gt; 1) #&gt; count: now 7,240 rows and 3 columns, ungrouped #&gt; filter: removed all rows (100%) #&gt; # A tibble: 0 x 3 #&gt; # ... with 3 variables: country &lt;chr&gt;, year &lt;int&gt;, n &lt;int&gt; 另一个思路是 distinct() 函数，它将返回数据框中某些列出现的的全部不重复的水平组合（注意complete()是”制造出“全部可能的水平组合），和 unique() 类似，但速度更快： who %&gt;% distinct(country, iso2, iso3) %&gt;% group_by(country) %&gt;% summarize(n = n()) %&gt;% filter(n &gt; 1) #&gt; distinct: removed 7,021 rows (97%), 219 rows remaining #&gt; group_by: one grouping variable (country) #&gt; summarize: now 219 rows and 2 columns, ungrouped #&gt; filter: removed all rows (100%) #&gt; # A tibble: 0 x 2 #&gt; # ... with 2 variables: country &lt;chr&gt;, n &lt;int&gt; "],
["non-tidy.html", "6.9 None-tidy data", " 6.9 None-tidy data Before we continue on to other topics, it’s worth talking briefly about non-tidy data. Earlier in the chapter, I used the pejorative term “messy” to refer to non-tidy data. That’s an oversimplification: there are lots of useful and well-founded data structures that are not tidy data. There are two main reasons to use other data structures: Alternative representations may have substantial performance or space advantages. Specialised fields have evolved their own conventions for storing data that may be quite different to the conventions of tidy data. Either of these reasons means you’ll need something other than a tibble (or data frame). If your data does fit naturally into a rectangular structure composed of observations and variables, I think tidy data should be your default choice. But there are good reasons to use other structures; tidy data is not the only way. If you’d like to learn more about non-tidy data, I’d highly recommend this thoughtful blog post by Jeff Leek: http://simplystatistics.org/2016/02/17/non-tidy-data/ "],
["purrr-functional-programming.html", "7 purrr: Functional programming", " 7 purrr: Functional programming purrr tutorial : https://jennybc.github.io/purrr-tutorial/ "],
["map-family.html", "7.1 map() family", " 7.1 map() family mtcars %&gt;% map(mean) %&gt;% str() #&gt; List of 11 #&gt; $ mpg : num 20.1 #&gt; $ cyl : num 6.19 #&gt; $ disp: num 231 #&gt; $ hp : num 147 #&gt; $ drat: num 3.6 #&gt; $ wt : num 3.22 #&gt; $ qsec: num 17.8 #&gt; $ vs : num 0.438 #&gt; $ am : num 0.406 #&gt; $ gear: num 3.69 #&gt; $ carb: num 2.81 simple_map &lt;- function(x, fun, ...) { output &lt;- vector(&quot;list&quot;, length = length(x)) for (i in seq_along(x)) { output[[i]] &lt;- fun(x[[i]], ...) } output } "],
["producing-atomic-vectors.html", "7.2 Producing atomic vectors", " 7.2 Producing atomic vectors # map_chr() always returns a character vector mtcars %&gt;% map_chr(typeof) #&gt; mpg cyl disp hp drat wt qsec vs #&gt; &quot;double&quot; &quot;double&quot; &quot;double&quot; &quot;double&quot; &quot;double&quot; &quot;double&quot; &quot;double&quot; &quot;double&quot; #&gt; am gear carb #&gt; &quot;double&quot; &quot;double&quot; &quot;double&quot; # map_lgl() always returns a logical vector mtcars %&gt;% map_lgl(is.double) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE # map_int() always returns a integer vector mtcars %&gt;% map_int(function(x) length(unique(x))) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; 25 3 27 22 22 29 30 2 2 3 6 # map_dbl() always returns a double vector mtcars %&gt;% map_dbl(mean) #&gt; mpg cyl disp hp drat wt qsec vs am gear #&gt; 20.091 6.188 230.722 146.688 3.597 3.217 17.849 0.438 0.406 3.688 #&gt; carb #&gt; 2.812 pair &lt;- function(x) c(x, x) map_dbl(1:2, pair) #&gt; Error: Result 1 must be a single double, not an integer vector of length 2 1:2 %&gt;% map(pair) #&gt; [[1]] #&gt; [1] 1 1 #&gt; #&gt; [[2]] #&gt; [1] 2 2 1:2 %&gt;% map_dbl(as.character) #&gt; Error: Can&#39;t coerce element 1 from a character to a double 1:2 %&gt;% map_chr(as.character) #&gt; [1] &quot;1&quot; &quot;2&quot; 7.2.1 purrr-style anonymous functions mtcars %&gt;% map_dbl(function(x) length(unique(x))) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; 25 3 27 22 22 29 30 2 2 3 6 mtcars %&gt;% map_dbl(~ length(unique(.x))) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; 25 3 27 22 22 29 30 2 2 3 6 mtcars %&gt;% map_dbl(~ length(unique(.))) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; 25 3 27 22 22 29 30 2 2 3 6 This shortcut is particularly useful for generating random data: 1:5 %&gt;% map(~ rnorm(mean = .x, n = 5)) %&gt;% str() #&gt; List of 5 #&gt; $ : num [1:5] 1.788 0.578 1.057 1.711 -0.587 #&gt; $ : num [1:5] 2.6 3.22 1.69 1.79 1.63 #&gt; $ : num [1:5] 3.33 4.88 2.52 4.74 3.32 #&gt; $ : num [1:5] 3.9 3.22 4.1 3.95 3.73 #&gt; $ : num [1:5] 5.65 3.43 2.95 6.02 5.6 x &lt;- list( list(-1, x = 1, y = c(2), z = &quot;a&quot;), list(-2, x = 4, y = c(5, 6), z = &quot;b&quot;), list(-3, x = 8, y = c(9, 10, 11)) ) # select by position x %&gt;% map(2) #&gt; [[1]] #&gt; [1] 1 #&gt; #&gt; [[2]] #&gt; [1] 4 #&gt; #&gt; [[3]] #&gt; [1] 8 # select by name x %&gt;% map(&quot;x&quot;) #&gt; [[1]] #&gt; [1] 1 #&gt; #&gt; [[2]] #&gt; [1] 4 #&gt; #&gt; [[3]] #&gt; [1] 8 # select by both position and name x %&gt;% map(list(&quot;y&quot;, 2)) #&gt; [[1]] #&gt; NULL #&gt; #&gt; [[2]] #&gt; [1] 6 #&gt; #&gt; [[3]] #&gt; [1] 10 "],
["predicate-functions.html", "7.3 Predicate functions", " 7.3 Predicate functions 7.3.1 Basics df &lt;- data.frame(x = 1:3, y = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) df %&gt;% keep(is.numeric) #&gt; x #&gt; 1 1 #&gt; 2 2 #&gt; 3 3 df %&gt;% discard(is.numeric) #&gt; y #&gt; 1 a #&gt; 2 b #&gt; 3 c df %&gt;% mutate(new_col = LETTERS[1:3]) %&gt;% detect(is.factor) #&gt; [1] a b c #&gt; Levels: a b c df %&gt;% detect_index(is.factor) #&gt; [1] 2 7.3.2 Map variants df &lt;- data.frame( num1 = c(0, 10, 20), num2 = c(5, 6, 7), chr1 = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), stringsAsFactors = FALSE ) df %&gt;% map_if(is.numeric, mean, na.rm = T) %&gt;% str() #&gt; List of 3 #&gt; $ num1: num 10 #&gt; $ num2: num 6 #&gt; $ chr1: chr [1:3] &quot;a&quot; &quot;b&quot; &quot;c&quot; df %&gt;% modify_if(is.character, str_to_upper) %&gt;% str() #&gt; &#39;data.frame&#39;: 3 obs. of 3 variables: #&gt; $ num1: num 0 10 20 #&gt; $ num2: num 5 6 7 #&gt; $ chr1: chr &quot;A&quot; &quot;B&quot; &quot;C&quot; "],
["group-functions.html", "7.4 group functions", " 7.4 group functions 7.4.1 group_map、group_modify group_map(), group_modify() and group_walk() are purrr-style functions that can be used to iterate on grouped tibbles. iris %&gt;% group_by(Species) %&gt;% group_map(~ broom::tidy(lm(Sepal.Length ~ Sepal.Width, data = .x))) %&gt;% bind_rows() #&gt; # A tibble: 6 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 2.64 0.310 8.51 3.74e-11 #&gt; 2 Sepal.Width 0.690 0.0899 7.68 6.71e-10 #&gt; 3 (Intercept) 3.54 0.563 6.29 9.07e- 8 #&gt; 4 Sepal.Width 0.865 0.202 4.28 8.77e- 5 #&gt; 5 (Intercept) 3.91 0.757 5.16 4.66e- 6 #&gt; 6 Sepal.Width 0.902 0.253 3.56 8.43e- 4 iris %&gt;% group_by(Species) %&gt;% group_modify(~ broom::tidy(lm(Sepal.Length ~ Sepal.Width, data = .x))) #&gt; # A tibble: 6 x 6 #&gt; # Groups: Species [3] #&gt; Species term estimate std.error statistic p.value #&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 setosa (Intercept) 2.64 0.310 8.51 3.74e-11 #&gt; 2 setosa Sepal.Width 0.690 0.0899 7.68 6.71e-10 #&gt; 3 versicolor (Intercept) 3.54 0.563 6.29 9.07e- 8 #&gt; 4 versicolor Sepal.Width 0.865 0.202 4.28 8.77e- 5 #&gt; 5 virginica (Intercept) 3.91 0.757 5.16 4.66e- 6 #&gt; 6 virginica Sepal.Width 0.902 0.253 3.56 8.43e- 4 This is similar to split() and then map(): iris %&gt;% split(.$Species) %&gt;% map_dfr(~ broom::tidy(lm(Sepal.Length ~ Sepal.Length, data = .x))) #&gt; # A tibble: 3 x 5 #&gt; term estimate std.error statistic p.value #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 (Intercept) 5.01 0.0498 100. 2.11e-58 #&gt; 2 (Intercept) 5.94 0.0730 81.3 6.14e-54 #&gt; 3 (Intercept) 6.59 0.0899 73.3 9.80e-52 7.4.2 group_nest、group_split、group_keys、group_data group_nest() is similar to group_by() + tidyr::nest(): iris %&gt;% as_tibble() %&gt;% group_nest(Species) #&gt; # A tibble: 3 x 2 #&gt; Species data #&gt; &lt;fct&gt; &lt;list&gt; #&gt; 1 setosa &lt;tibble [50 x 4]&gt; #&gt; 2 versicolor &lt;tibble [50 x 4]&gt; #&gt; 3 virginica &lt;tibble [50 x 4]&gt; group_split() is a tidy version of base::split(). In particular, it respects a group_by()-like grouping specification, and refuses to name its result. iris %&gt;% as_tibble() %&gt;% group_split(Species) #&gt; [[1]] #&gt; # A tibble: 50 x 5 #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa #&gt; # ... with 44 more rows #&gt; #&gt; [[2]] #&gt; # A tibble: 50 x 5 #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 7 3.2 4.7 1.4 versicolor #&gt; 2 6.4 3.2 4.5 1.5 versicolor #&gt; 3 6.9 3.1 4.9 1.5 versicolor #&gt; 4 5.5 2.3 4 1.3 versicolor #&gt; 5 6.5 2.8 4.6 1.5 versicolor #&gt; 6 5.7 2.8 4.5 1.3 versicolor #&gt; # ... with 44 more rows #&gt; #&gt; [[3]] #&gt; # A tibble: 50 x 5 #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 6.3 3.3 6 2.5 virginica #&gt; 2 5.8 2.7 5.1 1.9 virginica #&gt; 3 7.1 3 5.9 2.1 virginica #&gt; 4 6.3 2.9 5.6 1.8 virginica #&gt; 5 6.5 3 5.8 2.2 virginica #&gt; 6 7.6 3 6.6 2.1 virginica #&gt; # ... with 44 more rows #&gt; #&gt; attr(,&quot;ptype&quot;) #&gt; # A tibble: 0 x 5 #&gt; # ... with 5 variables: Sepal.Length &lt;dbl&gt;, Sepal.Width &lt;dbl&gt;, #&gt; # Petal.Length &lt;dbl&gt;, Petal.Width &lt;dbl&gt;, Species &lt;fct&gt; iris %&gt;% as_tibble() %&gt;% group_by(Species) %&gt;% group_data() #&gt; # A tibble: 3 x 2 #&gt; Species .rows #&gt; &lt;fct&gt; &lt;list&gt; #&gt; 1 setosa &lt;int [50]&gt; #&gt; 2 versicolor &lt;int [50]&gt; #&gt; 3 virginica &lt;int [50]&gt; only grouping variables: iris %&gt;% as_tibble() %&gt;% group_keys(Species) #&gt; # A tibble: 3 x 1 #&gt; Species #&gt; &lt;fct&gt; #&gt; 1 setosa #&gt; 2 versicolor #&gt; 3 virginica only rows: iris %&gt;% as_tibble() %&gt;% group_by(Species) %&gt;% group_rows() #&gt; [[1]] #&gt; [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #&gt; [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 #&gt; #&gt; [[2]] #&gt; [1] 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 #&gt; [20] 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 #&gt; [39] 89 90 91 92 93 94 95 96 97 98 99 100 #&gt; #&gt; [[3]] #&gt; [1] 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 #&gt; [20] 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 #&gt; [39] 139 140 141 142 143 144 145 146 147 148 149 150 "],
["other-useful-tools.html", "7.5 Other useful tools", " 7.5 Other useful tools 7.5.1 imap() imap_xxx(x, ...) is short hand for map2(x, names(x), ...)if x has names, or map2(x, seq_along(x), ...) if it does not. This is useful if you need to compute on both the value and the position of an element. Note that when using the formula shortcut, .x is the value, and the .y is the position: x &lt;- c(&quot;sheldon&quot; = 150, &quot;leonard&quot; = 140, &quot;raj&quot; = 145, &quot;howard&quot; = 140) imap(x, str_c) #&gt; $sheldon #&gt; [1] &quot;150sheldon&quot; #&gt; #&gt; $leonard #&gt; [1] &quot;140leonard&quot; #&gt; #&gt; $raj #&gt; [1] &quot;145raj&quot; #&gt; #&gt; $howard #&gt; [1] &quot;140howard&quot; imap(x, ~ str_c(.y, .x)) #&gt; $sheldon #&gt; [1] &quot;sheldon150&quot; #&gt; #&gt; $leonard #&gt; [1] &quot;leonard140&quot; #&gt; #&gt; $raj #&gt; [1] &quot;raj145&quot; #&gt; #&gt; $howard #&gt; [1] &quot;howard140&quot; # on a tibble imap_chr(mtcars, ~ str_c(.y, &quot;median:&quot;, median(.x), sep = &quot; &quot;)) #&gt; mpg cyl disp #&gt; &quot;mpg median: 19.2&quot; &quot;cyl median: 6&quot; &quot;disp median: 196.3&quot; #&gt; hp drat wt #&gt; &quot;hp median: 123&quot; &quot;drat median: 3.695&quot; &quot;wt median: 3.325&quot; #&gt; qsec vs am #&gt; &quot;qsec median: 17.71&quot; &quot;vs median: 0&quot; &quot;am median: 0&quot; #&gt; gear carb #&gt; &quot;gear median: 4&quot; &quot;carb median: 2&quot; 7.5.2 adverbs partial wraps a function: mean(c(10, NA, 5, 7), na.rm = TRUE) #&gt; [1] 7.33 my_mean &lt;- partial(mean, na.rm = TRUE) my_mean(c(10, NA, 5, 7)) #&gt; [1] 7.33 negate() negates a predicate function: lst &lt;- list(&quot;a&quot;, 3, 22, NULL, &quot;q&quot;, NULL) map_lgl(lst, ~ !is.null(.)) #&gt; [1] TRUE TRUE TRUE FALSE TRUE FALSE is_not_null &lt;- negate(is.null) map_lgl(lst, is_not_null) #&gt; [1] TRUE TRUE TRUE FALSE TRUE FALSE safely() and possibly(): add_ten &lt;- function(x) { x + 10 } add_ten_safely &lt;- safely(add_ten) map(lst, add_ten_safely) #&gt; [[1]] #&gt; [[1]]$result #&gt; NULL #&gt; #&gt; [[1]]$error #&gt; &lt;simpleError in x + 10: non-numeric argument to binary operator&gt; #&gt; #&gt; #&gt; [[2]] #&gt; [[2]]$result #&gt; [1] 13 #&gt; #&gt; [[2]]$error #&gt; NULL #&gt; #&gt; #&gt; [[3]] #&gt; [[3]]$result #&gt; [1] 32 #&gt; #&gt; [[3]]$error #&gt; NULL #&gt; #&gt; #&gt; [[4]] #&gt; [[4]]$result #&gt; numeric(0) #&gt; #&gt; [[4]]$error #&gt; NULL #&gt; #&gt; #&gt; [[5]] #&gt; [[5]]$result #&gt; NULL #&gt; #&gt; [[5]]$error #&gt; &lt;simpleError in x + 10: non-numeric argument to binary operator&gt; #&gt; #&gt; #&gt; [[6]] #&gt; [[6]]$result #&gt; numeric(0) #&gt; #&gt; [[6]]$error #&gt; NULL # If you’re not interested in what the error is add_ten_possibly &lt;- possibly(add_ten, otherwise = &quot;not numeric&quot;) map(lst, add_ten_possibly) #&gt; [[1]] #&gt; [1] &quot;not numeric&quot; #&gt; #&gt; [[2]] #&gt; [1] 13 #&gt; #&gt; [[3]] #&gt; [1] 32 #&gt; #&gt; [[4]] #&gt; numeric(0) #&gt; #&gt; [[5]] #&gt; [1] &quot;not numeric&quot; #&gt; #&gt; [[6]] #&gt; numeric(0) compose() lets you string together multiple functions add_ten_log_and_round &lt;- compose(round, log, add_ten) c(1, 5, 100) %&gt;% add_ten_log_and_round() #&gt; [1] 2 3 5 # is equal to c(1, 5, 10) %&gt;% add_ten() %&gt;% log() %&gt;% round() #&gt; [1] 2 3 3 "],
["relational-data.html", "8 Relational data ", " 8 Relational data "],
["introduction-1.html", "8.1 Introduction", " 8.1 Introduction "],
["mutating-joins.html", "8.2 Mutating joins", " 8.2 Mutating joins "],
["filtering-join.html", "8.3 Filtering join", " 8.3 Filtering join "],
["broom-tidy-representation-of-models.html", "9 broom: Tidy representation of models", " 9 broom: Tidy representation of models library(broom) https://broom.tidyverse.org/index.html broom and updated dplyr https://broom.tidyverse.org/articles/broom_and_dplyr.html "],
["viz-many-models.html", "9.1 Visualizing many models", " 9.1 Visualizing many models gapminder &lt;- gapminder::gapminder gapminder %&gt;% group_by(continent) %&gt;% summarize(t_test = list(t.test(lifeExp))) %&gt;% mutate(tidied = map(t_test, broom::tidy)) %&gt;% unnest(tidied) %&gt;% ggplot() + geom_errorbarh(aes(xmin = conf.low, xmax = conf.high, y = continent)) "],
["examples.html", "9.2 Examples", " 9.2 Examples 9.2.1 PCA An excellent answer on tidyverse solutions to PCA: https://community.rstudio.com/t/tidyverse-solutions-for-factor-analysis-principal-component-analysis/4504 library(ggfortify) # for plotting pca iris_pca &lt;- iris %&gt;% nest() %&gt;% mutate(pca = map(data, ~ prcomp(.x %&gt;% select(-Species), center = TRUE, scale = TRUE)), pca_tidy = map2(pca, data, ~ broom::augment(.x, data = .y))) iris_pca #&gt; # A tibble: 1 x 3 #&gt; data pca pca_tidy #&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 &lt;tibble [150 x 5]&gt; &lt;prcomp&gt; &lt;tibble [150 x 10]&gt; iris_pca %&gt;% unnest(pca_tidy) %&gt;% summarize_at(.vars = vars(contains(&quot;.fittedPC&quot;)), list(variance = var)) %&gt;% pivot_longer(everything(), names_to = &quot;pc&quot;, values_to = &quot;variance&quot;) %&gt;% mutate(var_explained = variance / sum(variance)) #&gt; # A tibble: 4 x 3 #&gt; pc variance var_explained #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 .fittedPC1_variance 2.92 0.730 #&gt; 2 .fittedPC2_variance 0.914 0.229 #&gt; 3 .fittedPC3_variance 0.147 0.0367 #&gt; 4 .fittedPC4_variance 0.0207 0.00518 iris_pca %&gt;% mutate( pca_graph = map2( pca, data, ~ autoplot(.x, loadings = TRUE, loadings.label = TRUE, data = .y, colour = &quot;Species&quot;) ) ) %&gt;% pull(pca_graph) #&gt; [[1]] "],
["broomextra.html", "9.3 broomExtra", " 9.3 broomExtra # install.packages(&quot;broomExtra&quot;) "],
["ggfortify.html", "9.4 ggfortify", " 9.4 ggfortify lm(Petal.Width ~ Petal.Length, data = iris) %&gt;% autoplot(label.size = 3) "],
["vroom-fast-reading-of-delimited-files.html", "10 vroom: Fast reading of delimited files", " 10 vroom: Fast reading of delimited files vroom(Hester and Wickham 2019) https://vroom.r-lib.org/ library(vroom) file_path &lt;- vroom_example(&quot;mtcars.csv&quot;) vroom(file_path) #&gt; # A tibble: 32 x 12 #&gt; model mpg cyl disp hp drat wt qsec vs am gear carb #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 4 4 #&gt; 2 Mazda RX4 W~ 21 6 160 110 3.9 2.88 17.0 0 1 4 4 #&gt; 3 Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 #&gt; 4 Hornet 4 Dr~ 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 #&gt; 5 Hornet Spor~ 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 #&gt; 6 Valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 #&gt; # ... with 26 more rows spec(vroom(file_path)) #&gt; cols( #&gt; model = col_character(), #&gt; mpg = col_double(), #&gt; cyl = col_double(), #&gt; disp = col_double(), #&gt; hp = col_double(), #&gt; drat = col_double(), #&gt; wt = col_double(), #&gt; qsec = col_double(), #&gt; vs = col_double(), #&gt; am = col_double(), #&gt; gear = col_double(), #&gt; carb = col_double() #&gt; ) compressed &lt;- vroom_example(&quot;mtcars.csv.zip&quot;) vroom(compressed) #&gt; # A tibble: 32 x 12 #&gt; model mpg cyl disp hp drat wt qsec vs am gear carb #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 4 4 #&gt; 2 Mazda RX4 W~ 21 6 160 110 3.9 2.88 17.0 0 1 4 4 #&gt; 3 Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 #&gt; 4 Hornet 4 Dr~ 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 #&gt; 5 Hornet Spor~ 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 #&gt; 6 Valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 #&gt; # ... with 26 more rows vroom(compressed, col_select = c(model, cyl, gear)) #&gt; # A tibble: 32 x 3 #&gt; model cyl gear #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Mazda RX4 6 4 #&gt; 2 Mazda RX4 Wag 6 4 #&gt; 3 Datsun 710 4 4 #&gt; 4 Hornet 4 Drive 6 3 #&gt; 5 Hornet Sportabout 8 3 #&gt; 6 Valiant 6 3 #&gt; # ... with 26 more rows mtcars #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; Mazda RX4 21.0 6 160.0 110 3.90 2.62 16.5 0 1 4 4 #&gt; Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.88 17.0 0 1 4 4 #&gt; Datsun 710 22.8 4 108.0 93 3.85 2.32 18.6 1 1 4 1 #&gt; Hornet 4 Drive 21.4 6 258.0 110 3.08 3.21 19.4 1 0 3 1 #&gt; Hornet Sportabout 18.7 8 360.0 175 3.15 3.44 17.0 0 0 3 2 #&gt; Valiant 18.1 6 225.0 105 2.76 3.46 20.2 1 0 3 1 #&gt; Duster 360 14.3 8 360.0 245 3.21 3.57 15.8 0 0 3 4 #&gt; Merc 240D 24.4 4 146.7 62 3.69 3.19 20.0 1 0 4 2 #&gt; Merc 230 22.8 4 140.8 95 3.92 3.15 22.9 1 0 4 2 #&gt; Merc 280 19.2 6 167.6 123 3.92 3.44 18.3 1 0 4 4 #&gt; Merc 280C 17.8 6 167.6 123 3.92 3.44 18.9 1 0 4 4 #&gt; Merc 450SE 16.4 8 275.8 180 3.07 4.07 17.4 0 0 3 3 #&gt; Merc 450SL 17.3 8 275.8 180 3.07 3.73 17.6 0 0 3 3 #&gt; Merc 450SLC 15.2 8 275.8 180 3.07 3.78 18.0 0 0 3 3 #&gt; Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.25 18.0 0 0 3 4 #&gt; Lincoln Continental 10.4 8 460.0 215 3.00 5.42 17.8 0 0 3 4 #&gt; Chrysler Imperial 14.7 8 440.0 230 3.23 5.34 17.4 0 0 3 4 #&gt; Fiat 128 32.4 4 78.7 66 4.08 2.20 19.5 1 1 4 1 #&gt; Honda Civic 30.4 4 75.7 52 4.93 1.61 18.5 1 1 4 2 #&gt; Toyota Corolla 33.9 4 71.1 65 4.22 1.83 19.9 1 1 4 1 #&gt; Toyota Corona 21.5 4 120.1 97 3.70 2.46 20.0 1 0 3 1 #&gt; Dodge Challenger 15.5 8 318.0 150 2.76 3.52 16.9 0 0 3 2 #&gt; AMC Javelin 15.2 8 304.0 150 3.15 3.44 17.3 0 0 3 2 #&gt; Camaro Z28 13.3 8 350.0 245 3.73 3.84 15.4 0 0 3 4 #&gt; Pontiac Firebird 19.2 8 400.0 175 3.08 3.85 17.1 0 0 3 2 #&gt; Fiat X1-9 27.3 4 79.0 66 4.08 1.94 18.9 1 1 4 1 #&gt; Porsche 914-2 26.0 4 120.3 91 4.43 2.14 16.7 0 1 5 2 #&gt; Lotus Europa 30.4 4 95.1 113 3.77 1.51 16.9 1 1 5 2 #&gt; Ford Pantera L 15.8 8 351.0 264 4.22 3.17 14.5 0 1 5 4 #&gt; Ferrari Dino 19.7 6 145.0 175 3.62 2.77 15.5 0 1 5 6 #&gt; Maserati Bora 15.0 8 301.0 335 3.54 3.57 14.6 0 1 5 8 #&gt; Volvo 142E 21.4 4 121.0 109 4.11 2.78 18.6 1 1 4 2 "],
["reading-in-data-from-other-formats.html", "11 Reading in data from other formats", " 11 Reading in data from other formats Either of readr or vroom aimed at reading at reading in flat, deliminated files. This chapter is a introduction on how to extract and read into R data that comes as other formats or sources, such as pdfs, office documents, google sheets and images. "],
["pdf.html", "11.1 PDF", " 11.1 PDF It is often the case that data is trapped inside pdfs, but thankfully there are ways to extract it from the pdfs. A very nice package for this task is pdftools. library(pdftools) download.file(&quot;http://arxiv.org/pdf/1403.2805.pdf&quot;, &quot;data/1403.2805.pdf&quot;, mode = &quot;wb&quot;) Each string in the vector contains a plain text version of the text on that page. txt &lt;- pdf_text(&quot;data/1403.2805.pdf&quot;) # first page text cat(txt[1]) #&gt; The jsonlite Package: A Practical and Consistent Mapping #&gt; Between JSON Data and R Objects #&gt; Jeroen Ooms #&gt; arXiv:1403.2805v1 [stat.CO] 12 Mar 2014 #&gt; UCLA Department of Statistics #&gt; Abstract #&gt; A naive realization of JSON data in R maps JSON arrays to an unnamed list, and JSON objects to a #&gt; named list. However, in practice a list is an awkward, inefficient type to store and manipulate data. #&gt; Most statistical applications work with (homogeneous) vectors, matrices or data frames. Therefore JSON #&gt; packages in R typically define certain special cases of JSON structures which map to simpler R types. #&gt; Currently there exist no formal guidelines, or even consensus between implementations on how R data #&gt; should be represented in JSON. Furthermore, upon closer inspection, even the most basic data structures #&gt; in R actually do not perfectly map to their JSON counterparts and leave some ambiguity for edge cases. #&gt; These problems have resulted in different behavior between implementations and can lead to unexpected #&gt; output. This paper explicitly describes a mapping between R classes and JSON data, highlights potential #&gt; problems, and proposes conventions that generalize the mapping to cover all common structures. We #&gt; emphasize the importance of type consistency when using JSON to exchange dynamic data, and illustrate #&gt; using examples and anecdotes. The jsonlite R package is used throughout the paper as a reference #&gt; implementation. #&gt; 1 Introduction #&gt; JavaScript Object Notation (JSON) is a text format for the serialization of structured data (Crockford, 2006a). #&gt; It is derived from the object literals of JavaScript, as defined in the ECMAScript Programming Language #&gt; Standard, Third Edition (ECMA, 1999). Design of JSON is simple and concise in comparison with other #&gt; text based formats, and it was originally proposed by Douglas Crockford as a “fat-free alternative to XML” #&gt; (Crockford, 2006b). The syntax is easy for humans to read and write, easy for machines to parse and generate #&gt; and completely described in a single page at http://www.json.org. The character encoding of JSON text #&gt; is always Unicode, using UTF-8 by default (Crockford, 2006a), making it naturally compatible with non- #&gt; latin alphabets. Over the past years, JSON has become hugely popular on the internet as a general purpose #&gt; data interchange format. High quality parsing libraries are available for almost any programming language, #&gt; making it easy to implement systems and applications that exchange data over the network using JSON. For #&gt; R (R Core Team, 2013), several packages that assist the user in generating, parsing and validating JSON #&gt; are available through CRAN, including rjson (Couture-Beil, 2013), RJSONIO (Lang, 2013), and jsonlite #&gt; (Ooms et al., 2014). #&gt; The emphasis of this paper is not on discussing the JSON format or any particular implementation for using #&gt; 1 # second page text cat(txt[2]) #&gt; JSON with R. We refer to Nolan and Temple Lang (2014) for a comprehensive introduction, or one of the #&gt; many tutorials available on the web. Instead we take a high level view and discuss how R data structures are #&gt; most naturally represented in JSON. This is not a trivial problem, particulary for complex or relational data #&gt; as they frequently appear in statistical applications. Several R packages implement toJSON and fromJSON #&gt; functions which directly convert R objects into JSON and vice versa. However, the exact mapping between #&gt; the various R data classes JSON structures is not self evident. Currently, there are no formal guidelines, #&gt; or even consensus between implementations on how R data should be represented in JSON. Furthermore, #&gt; upon closer inspection, even the most basic data structures in R actually do not perfectly map to their #&gt; JSON counterparts, and leave some ambiguity for edge cases. These problems have resulted in different #&gt; behavior between implementations, and can lead to unexpected output for certain special cases. To further #&gt; complicate things, best practices of representing data in JSON have been established outside the R community. #&gt; Incorporating these conventions where possible is important to maximize interoperability. #&gt; 1.1 Parsing and type safety #&gt; The JSON format specifies 4 primitive types (string, number, boolean, null) and two universal structures: #&gt; • A JSON object : an unordered collection of zero or more name/value pairs, where a name is a string and #&gt; a value is a string, number, boolean, null, object, or array. #&gt; • A JSON array: an ordered sequence of zero or more values. #&gt; Both these structures are heterogeneous; i.e. they are allowed to contain elements of different types. There- #&gt; fore, the native R realization of these structures is a named list for JSON objects, and unnamed list for #&gt; JSON arrays. However, in practice a list is an awkward, inefficient type to store and manipulate data in R. #&gt; Most statistical applications work with (homogeneous) vectors, matrices or data frames. In order to give #&gt; these data structures a JSON representation, we can define certain special cases of JSON structures which get #&gt; parsed into other, more specific R types. For example, one convention which all current implementations #&gt; have in common is that a homogeneous array of primitives gets parsed into an atomic vector instead of a #&gt; list. The RJSONIO documentation uses the term “simplify” for this, and we adopt this jargon. #&gt; txt &lt;- &quot;[12, 3, 7]&quot; #&gt; x &lt;- fromJSON(txt) #&gt; is(x) #&gt; [1] &quot;numeric&quot; &quot;vector&quot; #&gt; print(x) #&gt; [1] 12 3 7 #&gt; This seems very reasonable and it is the only practical solution to represent vectors in JSON. However the #&gt; price we pay is that automatic simplification can compromise type-safety in the context of dynamic data. #&gt; For example, suppose an R package uses fromJSON to pull data from a JSON API on the web, similar to #&gt; the example above. However, for some particular combination of parameters, the result includes a null #&gt; value, e.g: [12, null, 7]. This is actually quite common, many APIs use null for missing values or unset #&gt; fields. This case makes the behavior of parsers ambiguous, because the JSON array is technically no longer #&gt; 2 The package has some utilities to extract other data from the PDF file. pdf_toc(&quot;data/1403.2805.pdf&quot;) %&gt;% str(max.level = 3) #&gt; List of 2 #&gt; $ title : chr &quot;&quot; #&gt; $ children:List of 6 #&gt; ..$ :List of 2 #&gt; .. ..$ title : chr &quot;1 Introduction&quot; #&gt; .. ..$ children:List of 4 #&gt; ..$ :List of 2 #&gt; .. ..$ title : chr &quot;2 Converting between JSON and R classes&quot; #&gt; .. ..$ children:List of 4 #&gt; ..$ :List of 2 #&gt; .. ..$ title : chr &quot;3 Structural consistency and type safety in dynamic data&quot; #&gt; .. ..$ children:List of 3 #&gt; ..$ :List of 2 #&gt; .. ..$ title : chr &quot;Appendices&quot; #&gt; .. ..$ children: list() #&gt; ..$ :List of 2 #&gt; .. ..$ title : chr &quot;A Public JSON APIs&quot; #&gt; .. ..$ children:List of 3 #&gt; ..$ :List of 2 #&gt; .. ..$ title : chr &quot;B Simple JSON RPC with OpenCPU&quot; #&gt; .. ..$ children: list() pdf_info(&quot;data/1403.2805.pdf&quot;) #&gt; $version #&gt; [1] &quot;1.4&quot; #&gt; #&gt; $pages #&gt; [1] 29 #&gt; #&gt; $encrypted #&gt; [1] FALSE #&gt; #&gt; $linearized #&gt; [1] FALSE #&gt; #&gt; $keys #&gt; $keys$Producer #&gt; [1] &quot;dvips + GPL Ghostscript GIT PRERELEASE 9.08&quot; #&gt; #&gt; $keys$Creator #&gt; [1] &quot;LaTeX with hyperref package&quot; #&gt; #&gt; $keys$Title #&gt; [1] &quot;&quot; #&gt; #&gt; $keys$Subject #&gt; [1] &quot;&quot; #&gt; #&gt; $keys$Author #&gt; [1] &quot;&quot; #&gt; #&gt; $keys$Keywords #&gt; [1] &quot;&quot; #&gt; #&gt; #&gt; $created #&gt; [1] &quot;2014-03-13 09:00:25 CST&quot; #&gt; #&gt; $modified #&gt; [1] &quot;2014-03-13 09:00:25 CST&quot; #&gt; #&gt; $metadata #&gt; [1] &quot;&lt;?xpacket begin=&#39;ï»¿&#39; id=&#39;W5M0MpCehiHzreSzNTczkc9d&#39;?&gt;\\n&lt;?adobe-xap-filters esc=\\&quot;CRLF\\&quot;?&gt;\\n&lt;x:xmpmeta xmlns:x=&#39;adobe:ns:meta/&#39; x:xmptk=&#39;XMP toolkit 2.9.1-13, framework 1.6&#39;&gt;\\n&lt;rdf:RDF xmlns:rdf=&#39;http://www.w3.org/1999/02/22-rdf-syntax-ns#&#39; xmlns:iX=&#39;http://ns.adobe.com/iX/1.0/&#39;&gt;\\n&lt;rdf:Description rdf:about=&#39;uuid:68dde9e4-e267-11ee-0000-a5c788a95450&#39; xmlns:pdf=&#39;http://ns.adobe.com/pdf/1.3/&#39;&gt;&lt;pdf:Producer&gt;dvips + GPL Ghostscript GIT PRERELEASE 9.08&lt;/pdf:Producer&gt;\\n&lt;pdf:Keywords&gt;()&lt;/pdf:Keywords&gt;\\n&lt;/rdf:Description&gt;\\n&lt;rdf:Description rdf:about=&#39;uuid:68dde9e4-e267-11ee-0000-a5c788a95450&#39; xmlns:xmp=&#39;http://ns.adobe.com/xap/1.0/&#39;&gt;&lt;xmp:ModifyDate&gt;2014-03-12T21:00:25-04:00&lt;/xmp:ModifyDate&gt;\\n&lt;xmp:CreateDate&gt;2014-03-12T21:00:25-04:00&lt;/xmp:CreateDate&gt;\\n&lt;xmp:CreatorTool&gt;LaTeX with hyperref package&lt;/xmp:CreatorTool&gt;&lt;/rdf:Description&gt;\\n&lt;rdf:Description rdf:about=&#39;uuid:68dde9e4-e267-11ee-0000-a5c788a95450&#39; xmlns:xapMM=&#39;http://ns.adobe.com/xap/1.0/mm/&#39; xapMM:DocumentID=&#39;uuid:68dde9e4-e267-11ee-0000-a5c788a95450&#39;/&gt;\\n&lt;rdf:Description rdf:about=&#39;uuid:68dde9e4-e267-11ee-0000-a5c788a95450&#39; xmlns:dc=&#39;http://purl.org/dc/elements/1.1/&#39; dc:format=&#39;application/pdf&#39;&gt;&lt;dc:title&gt;&lt;rdf:Alt&gt;&lt;rdf:li xml:lang=&#39;x-default&#39;&gt;()&lt;/rdf:li&gt;&lt;/rdf:Alt&gt;&lt;/dc:title&gt;&lt;dc:creator&gt;&lt;rdf:Seq&gt;&lt;rdf:li&gt;()&lt;/rdf:li&gt;&lt;/rdf:Seq&gt;&lt;/dc:creator&gt;&lt;dc:description&gt;&lt;rdf:Seq&gt;&lt;rdf:li&gt;()&lt;/rdf:li&gt;&lt;/rdf:Seq&gt;&lt;/dc:description&gt;&lt;/rdf:Description&gt;\\n&lt;/rdf:RDF&gt;\\n&lt;/x:xmpmeta&gt;\\n \\n \\n&lt;?xpacket end=&#39;w&#39;?&gt;&quot; #&gt; #&gt; $locked #&gt; [1] FALSE #&gt; #&gt; $attachments #&gt; [1] FALSE #&gt; #&gt; $layout #&gt; [1] &quot;no_layout&quot; pdf_fonts(&quot;data/1403.2805.pdf&quot;) #&gt; # A tibble: 18 x 4 #&gt; name type embedded file #&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; #&gt; 1 Times-Roman type1 FALSE &quot;C:\\\\windows\\\\Fonts\\\\arial.ttf&quot; #&gt; 2 UWJIZQ+CMTT10 type1c TRUE &quot;&quot; #&gt; 3 OYDUEZ+CMR10 type1c TRUE &quot;&quot; #&gt; 4 EBLJKS+CMTI10 type1c TRUE &quot;&quot; #&gt; 5 APBPVY+CMBX12 type1c TRUE &quot;&quot; #&gt; 6 FBFBNT+CMTI9 type1c TRUE &quot;&quot; #&gt; # ... with 12 more rows 11.1.1 Scraping pdf data https://www.brodrigues.co/blog/2018-06-10-scraping_pdfs/ As one can imagine, scraping pdf data is just a matter of text process after loading in pdf documents with pdf_text(). We start by downloading multiple pdf documents, then extracting and cleaning data stored in a table library(glue) country &lt;- c(&quot;chn&quot;, &quot;usa&quot;, &quot;gbr&quot;, &quot;jpn&quot;) url &lt;- &quot;http://www.who.int/diabetes/country-profiles/{country}_en.pdf?ua=1&quot; urls &lt;- glue(url) pdf_names &lt;- glue(here::here(&quot;data/report_{country}.pdf&quot;)) # download pdfs walk2(urls, pdf_names, download.file, mode = &quot;wb&quot;) raw_text &lt;- map(pdf_names, pdf_text) Because each pdf document only contain one page, raw_data has a simple 2-level structure, with each element being one report of a country: str(raw_text) #&gt; List of 4 #&gt; $ : chr &quot;China &quot;| __truncated__ #&gt; $ : chr &quot;United States Of America &quot;| __truncated__ #&gt; $ : chr &quot;United Kingdom &quot;| __truncated__ #&gt; $ : chr &quot;Japan &quot;| __truncated__ raw_text[[1]] #&gt; [1] &quot;China Total population: 1 376 000 000\\r\\n Income group: Upper middle\\r\\nMortality*\\r\\nNumber of diabetes deaths Number of deaths attributable to high blood glucose\\r\\n males females males females\\r\\nages 30–69 37 000 56 000 ages 30–69 139 100 130 900\\r\\nages 70+ 49 300 82 400 ages 70+ 198 800 268 400\\r\\nProportional mortality (% of total deaths, all ages)* Trends in age-standardized prevalence of diabetes\\r\\n Communicable,\\r\\n maternal, perinatal\\r\\n Injuries 35%\\r\\n 8%\\r\\n and nutritional\\r\\n conditions\\r\\n 5%\\r\\n 30%\\r\\n Other NCDs\\r\\n 6% Cardiovascular 25%\\r\\n diseases\\r\\n % of population\\r\\n Diabetes 45%\\r\\n 20%\\r\\n 2%\\r\\n No data available 15% No data available\\r\\n Respiratory\\r\\n diseases 10%\\r\\n 11%\\r\\n 5%\\r\\n 0%\\r\\n Cancers\\r\\n 23% males females\\r\\nPrevalence of diabetes and related risk factors\\r\\n males females total\\r\\nDiabetes 10.5% 8.3% 9.4%\\r\\nOverweight 37.2% 33.6% 35.4%\\r\\nObesity 6.2% 8.5% 7.3%\\r\\nPhysical inactivity 22.2% 25.4% 23.8%\\r\\nNational response to diabetes\\r\\nPolicies, guidelines and monitoring\\r\\nOperational policy/strategy/action plan for diabetes Yes\\r\\nOperational policy/strategy/action plan to reduce overweight and obesity No\\r\\nOperational policy/strategy/action plan to reduce physical inactivity Yes\\r\\nEvidence-based national diabetes guidelines/protocols/standards Available and fully implemented\\r\\nStandard criteria for referral of patients from primary care to higher level of care Available and partially implemented\\r\\nDiabetes registry Yes\\r\\nRecent national risk factor survey in which blood glucose was measured Yes\\r\\nAvailability of medicines, basic technologies and procedures in the public health sector\\r\\nMedicines in primary care facilities Basic technologies in primary care facilities\\r\\nInsulin &lt;U+25CF&gt; Blood glucose measurement &lt;U+25CF&gt;\\r\\nMetformin &lt;U+25CF&gt; Oral glucose tolerance test &lt;U+25CF&gt;\\r\\nSulphonylurea &lt;U+25CF&gt; HbA1c test &lt;U+25CF&gt;\\r\\nProcedures Dilated fundus examination &lt;U+25CF&gt;\\r\\nRetinal photocoagulation &lt;U+25CB&gt; Foot vibration perception by tuning fork &lt;U+25CF&gt;\\r\\nRenal replacement therapy by dialysis &lt;U+25CB&gt; Foot vascular status by Doppler &lt;U+25CF&gt;\\r\\nRenal replacement therapy by transplantation &lt;U+25CB&gt; Urine strips for glucose and ketone measurement &lt;U+25CF&gt;\\r\\n* The mortality estimates for this country have a high degree of uncertainty because they are not\\r\\n based on any national NCD mortality data (see Explanatory Notes).\\r\\n&lt;U+3007&gt; = not generally available &lt;U+25CF&gt; = generally available\\r\\nWorld Health Organization – Diabetes country profiles, 2016.\\r\\n&quot; Note that the table we want starts with “Prevalence of diabetes and related risk factors”, and ends with “National response to diabetes”. We define a function for extracting the table when looping over all 4 pdf documents. get_table &lt;- function(text) { # split the text into a one raw text matrix text_matrix &lt;- text %&gt;% str_split(&quot;\\\\n&quot;, simplify = TRUE) # extract country name country_name &lt;- text_matrix[1, 1] %&gt;% str_squish() %&gt;% str_extract(&quot;.+(?= Total population)&quot;) # locate the start and end of the table table_start &lt;- text_matrix %&gt;% str_which(&quot;Prevalence of diabetes and related risk factors&quot;) table_end &lt;- text_matrix %&gt;% str_which(&quot;National response to diabetes&quot;) # extract table text, replace space with &quot;|&quot; table_raw &lt;- text_matrix[1, (table_start + 1):(table_end - 1)] %&gt;% str_replace_all(&quot;\\\\s{2,}&quot;, &quot;|&quot;) # creat text connection so that the text can be read back with read.csv() table &lt;- table_raw %&gt;% textConnection() %&gt;% read.csv(sep = &quot;|&quot;, col.names = c(&quot;condition&quot;, &quot;males&quot;, &quot;females&quot;, &quot;total&quot;)) %&gt;% as_tibble() %&gt;% mutate(country = country_name) table } df &lt;- map_dfr(raw_text, get_table) df #&gt; # A tibble: 16 x 5 #&gt; condition males females total country #&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Diabetes 10.5% 8.3% 9.4% China #&gt; 2 Overweight 37.2% 33.6% 35.4% China #&gt; 3 Obesity 6.2% 8.5% 7.3% China #&gt; 4 Physical inactivity 22.2% 25.4% 23.8% China #&gt; 5 Diabetes 9.8% 8.3% 9.1% United States Of America #&gt; 6 Overweight 74.1% 65.3% 69.6% United States Of America #&gt; # ... with 10 more rows There is one problem left, we need to convert percentage (recoginized as character) back to numeric: df %&gt;% mutate_at(vars(males, females, total), ~ parse_number(.x) / 100) #&gt; # A tibble: 16 x 5 #&gt; condition males females total country #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 Diabetes 0.105 0.083 0.094 China #&gt; 2 Overweight 0.372 0.336 0.354 China #&gt; 3 Obesity 0.062 0.085 0.073 China #&gt; 4 Physical inactivity 0.222 0.254 0.238 China #&gt; 5 Diabetes 0.098 0.083 0.091 United States Of America #&gt; 6 Overweight 0.741 0.653 0.696 United States Of America #&gt; # ... with 10 more rows "],
["office-documents.html", "11.2 Office documents", " 11.2 Office documents "],
["google-sheet.html", "11.3 Google sheet", " 11.3 Google sheet "],
["images.html", "11.4 Images", " 11.4 Images https://cran.r-project.org/web/packages/tesseract/vignettes/intro.html#read_from_pdf_files "],
["useful-apis.html", "12 Useful APIs ", " 12 Useful APIs "],
["wdi.html", "12.1 WDI", " 12.1 WDI # install.packages(&quot;WDI&quot;) library(WDI) World Devevlopment Indicators(WDI)是世界银行提供的公开、高质量数据库。指标首先按照领域分类，除了一般常见的统计数据外，还包括农业、气候、贫穷、健康方面的数据。 Figure 12.1: Indicators divided by sectors 具体到某领域内，每个数据集都是各个国家在某个指标上的时间序列，时效性一般在两年之内。下面是气候变化分类中电力覆盖率指标(Access to electricity)的一个例子： knitr::include_graphics(&quot;images/20.png&quot;) 数据库中包含了超过 1600 个这样的时间序列，很多有效跨度超过了 50 年。WDI(Arel-Bundock 2019) 包提供了在 WDI 数据库中搜索、提取、格式化信息的接口。 12.1.1 WDIsearch() WDIserach() 用于在 WDI 数据库中搜索可用的指标，是之后用 WDI() 提取相关数据的基础。 WDIsearch(string = &quot;gdp&quot;, field = &quot;name&quot;, short = TRUE, cache = NULL) string = \"gdp\": 用于搜索的正则表达式 field= “name”: 搜索域，可选 “indicator”（编码）、“name”（名称）、“discription”（详细描述）、“sourceDatabase” 和 “sourceOrganization” （来源数据库或组织） short = TRUE：WDIsearch()默认只返回匹配指标的编码和名称，short = FSLE 同时返回详细描述和来源 如在名称域内中搜索与二氧化碳相关的指标： # the result of WDIsearch() is a 2D matrix WDIsearch(&quot;CO2&quot;, short = F) %&gt;% as_tibble() #&gt; # A tibble: 45 x 5 #&gt; indicator name description sourceDatabase sourceOrganization #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 EN.ATM.CO2~ &quot;CO2 emis~ &quot;Carbon dioxide ~ World Developm~ &quot;Carbon Dioxide Info~ #&gt; 2 EN.ATM.CO2~ &quot;CO2 emis~ &quot;Carbon dioxide ~ World Developm~ &quot;Carbon Dioxide Info~ #&gt; 3 EN.ATM.CO2~ &quot;CO2 emis~ &quot;Carbon dioxide ~ World Developm~ &quot;Carbon Dioxide Info~ #&gt; 4 EN.ATM.CO2~ &quot;CO2 emis~ &quot;Carbon dioxide ~ World Developm~ &quot;Carbon Dioxide Info~ #&gt; 5 EN.ATM.CO2~ &quot;CO2 emis~ &quot;Carbon dioxide ~ World Developm~ &quot;Carbon Dioxide Info~ #&gt; 6 EN.ATM.CO2~ &quot;CO2 emis~ &quot;&quot; WDI Database A~ &quot;&quot; #&gt; # ... with 39 more rows 改变搜索域： WDIsearch(string = &quot;mortality&quot;, field = &quot;description&quot;, short = F) %&gt;% as_tibble() #&gt; # A tibble: 383 x 5 #&gt; indicator name description sourceDatabase sourceOrganization #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 5.51.01.0~ Maternal he~ Births attended~ Statistical Ca~ World Development In~ #&gt; 2 5.51.01.0~ Immunization The proportion ~ Statistical Ca~ World Development In~ #&gt; 3 5.51.01.0~ Child morta~ Under-five mort~ Statistical Ca~ World Development In~ #&gt; 4 PRJ.POP.1~ Wittgenstei~ Total populatio~ Education Stat~ Wittgenstein Centre ~ #&gt; 5 PRJ.POP.1~ Wittgenstei~ Total populatio~ Education Stat~ Wittgenstein Centre ~ #&gt; 6 PRJ.POP.1~ Wittgenstei~ Total populatio~ Education Stat~ Wittgenstein Centre ~ #&gt; # ... with 377 more rows WDIsearch() 中的正则表达式背后用 base R 中的 grep() 实现，所以无视大小写。 搜索人均不变价 GDP ： WDIsearch(string = &#39;gdp.*capita.*constant&#39;) %&gt;% as_tibble() #&gt; # A tibble: 5 x 2 #&gt; indicator name #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 6.0.GDPpc_constant &quot;GDP per capita, PPP (constant 2011 international $) &quot; #&gt; 2 NY.GDP.PCAP.PP.KD.87 &quot;GDP per capita, PPP (constant 1987 international $)&quot; #&gt; 3 NY.GDP.PCAP.PP.KD &quot;GDP per capita, PPP (constant 2011 international $)&quot; #&gt; 4 NY.GDP.PCAP.KN &quot;GDP per capita (constant LCU)&quot; #&gt; 5 NY.GDP.PCAP.KD &quot;GDP per capita (constant 2010 US$)&quot; 12.1.2 WDI WDIsearch() 得到了指标的编码之后，就可以用 WDI() 下载相关数据： WDI(country = &quot;all&quot;, indicator = &quot;NY.GNS.ICTR.GN.ZS&quot;, start = NULL, end = NULL, extra = FALSE, cache = NULL) country: 筛选国家或地区。使用 “ISO 3166-1” 两位字母编码，具体可见 mapdata::iso3166。（中国 CN, 美国 US，德国 DE，日本 JP，英国 GB） indicator: 指标的编码。具名向量可以在重命名该指标 start 和 end：时间序列的起始和结束。默认为 1950 年和今年 extra = FALSE： 若 extra = RTUE，返回首都经纬度、地区、收入水平等更多信息 WDI(indicator = &quot;5.51.01.03.mortal&quot;, country = c(&quot;US&quot;, &quot;CN&quot;), start = 2005, end = 2015) %&gt;% as_tibble() #&gt; # A tibble: 11 x 4 #&gt; iso2c country `5.51.01.03.mortal` year #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 CN China 1 2015 #&gt; 2 CN China 1 2014 #&gt; 3 CN China 1 2013 #&gt; 4 CN China 1 2012 #&gt; 5 CN China 1 2011 #&gt; 6 CN China 1 2010 #&gt; # ... with 5 more rows # renaming and extra = TRUE WDI(indicator = c(child_motality = &quot;5.51.01.03.mortal&quot;), country = c(&quot;US&quot;, &quot;CN&quot;), start = 2005, end = 2015, extra = TRUE) %&gt;% as_tibble() #&gt; # A tibble: 11 x 11 #&gt; iso2c country child_motality year iso3c region capital longitude latitude #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; #&gt; 1 CN China 1 2015 CHN East ~ Beijing 116.286 40.0495 #&gt; 2 CN China 1 2014 CHN East ~ Beijing 116.286 40.0495 #&gt; 3 CN China 1 2013 CHN East ~ Beijing 116.286 40.0495 #&gt; 4 CN China 1 2012 CHN East ~ Beijing 116.286 40.0495 #&gt; 5 CN China 1 2011 CHN East ~ Beijing 116.286 40.0495 #&gt; 6 CN China 1 2010 CHN East ~ Beijing 116.286 40.0495 #&gt; # ... with 5 more rows, and 2 more variables: income &lt;fct&gt;, lending &lt;fct&gt; "],
["ipumsr.html", "12.2 ipumsr", " 12.2 ipumsr IPUMS provides census and survey data from around the world integrated across time and space. IPUMS integration and documentation makes it easy to study change, conduct comparative research, merge information across data types, and analyze individuals within family and community contexts. Data and services available free of charge. https://github.com/mnpopcenter/ipumsr # install.packages(&quot;ipumsr&quot;) "],
["data-summary.html", "13 Data summary ", " 13 Data summary "],
["skimr.html", "13.1 skimr", " 13.1 skimr skimr(Waring et al. 2019) 是由 rOpenSci project 开发的用于探索性数据分析的包，可以看作增强版的 summary()，根据不同的列类型返回整洁有用的统计量。如： library(skimr) skim(iris) #&gt; -- Data Summary ------------------------ #&gt; Values #&gt; Name iris #&gt; Number of rows 150 #&gt; Number of columns 5 #&gt; _______________________ #&gt; Column type frequency: #&gt; factor 1 #&gt; numeric 4 #&gt; ________________________ #&gt; Group variables None #&gt; #&gt; -- Variable type: factor ------------------------------------------------------- #&gt; # A tibble: 1 x 6 #&gt; skim_variable n_missing complete_rate ordered n_unique #&gt; * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;int&gt; #&gt; 1 Species 0 1 FALSE 3 #&gt; top_counts #&gt; * &lt;chr&gt; #&gt; 1 set: 50, ver: 50, vir: 50 #&gt; #&gt; -- Variable type: numeric ------------------------------------------------------ #&gt; # A tibble: 4 x 11 #&gt; skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 #&gt; * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Sepal.Length 0 1 5.84 0.828 4.3 5.1 5.8 6.4 #&gt; 2 Sepal.Width 0 1 3.06 0.436 2 2.8 3 3.3 #&gt; 3 Petal.Length 0 1 3.76 1.77 1 1.6 4.35 5.1 #&gt; 4 Petal.Width 0 1 1.20 0.762 0.1 0.3 1.3 1.8 #&gt; p100 hist #&gt; * &lt;dbl&gt; &lt;chr&gt; #&gt; 1 7.9 &lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2582&gt; #&gt; 2 4.4 &lt;U+2581&gt;&lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2582&gt;&lt;U+2581&gt; #&gt; 3 6.9 &lt;U+2587&gt;&lt;U+2581&gt;&lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2582&gt; #&gt; 4 2.5 &lt;U+2587&gt;&lt;U+2581&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2583&gt; 由于 skim() 的返回结果在 bookdown 里显示效果不太好，这里只给出一个最简单的例子，关于该包的具体使用可见 Introduction to skimr "],
["visdat.html", "13.2 visdat", " 13.2 visdat # install.packages(&quot;visdat&quot;) library(visdat) https://docs.ropensci.org/visdat/ vis_dat(ggplot2::diamonds) "],
["summarytools.html", "13.3 summarytools", " 13.3 summarytools 由于很多 summarytools 中的函数往往直接生成 markdown 代码，为了在 rmarkdown 正确美观的呈现它们需要同时设置 summarytools 中和 knitr 中的一些全局选项：（详细可见Recommendations for Using summarytools With Rmarkdown）。 library(summarytools) st_options(bootstrap.css = FALSE, # Already part of the theme so no need for it plain.ascii = FALSE, # One of the essential settings style = &quot;rmarkdown&quot;, # Idem. dfSummary.silent = TRUE, # Suppresses messages about temporary files footnote = NA, # Keeping the results minimalistic subtitle.emphasis = FALSE) # For the vignette theme, this gives # much better results. Your mileage may vary. st_css() # This is a must; without it, expect odd layout, especially with dfSummary() #&gt; &lt;style type=&quot;text/css&quot;&gt; #&gt; img { background-color: transparent; border: 0; } .st-table td, .st-table th { padding: 8px; } .st-table &gt; thead &gt; tr { background-color: #eeeeee; } .st-cross-table td { text-align: center; } .st-descr-table td { text-align: right; } table.st-table th { text-align: center; } table.st-table &gt; thead &gt; tr { background-color: #eeeeee; } table.st-table td span { display: block; } table.st-table &gt; tfoot &gt; tr &gt; td { border:none; } .st-container { width: 100%; padding-right: 15px; padding-left: 15px; margin-right: auto; margin-left: auto; margin-top: 15px; } .st-multiline { white-space: pre; } .st-table { width: auto; table-layout: auto; margin-top: 20px; margin-bottom: 20px; max-width: 100%; background-color: transparent; border-collapse: collapse; } .st-table &gt; thead &gt; tr &gt; th, .st-table &gt; tbody &gt; tr &gt; th, .st-table &gt; tfoot &gt; tr &gt; th, .st-table &gt; thead &gt; tr &gt; td, .st-table &gt; tbody &gt; tr &gt; td, .st-table &gt; tfoot &gt; tr &gt; td { vertical-align: middle; } .st-table-bordered { border: 1px solid #bbbbbb; } .st-table-bordered &gt; thead &gt; tr &gt; th, .st-table-bordered &gt; tbody &gt; tr &gt; th, .st-table-bordered &gt; thead &gt; tr &gt; td, .st-table-bordered &gt; tbody &gt; tr &gt; td { border: 1px solid #cccccc; } .st-table-bordered &gt; thead &gt; tr &gt; th, .st-table-bordered &gt; thead &gt; tr &gt; td, .st-table thead &gt; tr &gt; th { border-bottom: none; } .st-freq-table &gt; thead &gt; tr &gt; th, .st-freq-table &gt; tbody &gt; tr &gt; th, .st-freq-table &gt; tfoot &gt; tr &gt; th, .st-freq-table &gt; thead &gt; tr &gt; td, .st-freq-table &gt; tbody &gt; tr &gt; td, .st-freq-table &gt; tfoot &gt; tr &gt; td, .st-freq-table-nomiss &gt; thead &gt; tr &gt; th, .st-freq-table-nomiss &gt; tbody &gt; tr &gt; th, .st-freq-table-nomiss &gt; tfoot &gt; tr &gt; th, .st-freq-table-nomiss &gt; thead &gt; tr &gt; td, .st-freq-table-nomiss &gt; tbody &gt; tr &gt; td, .st-freq-table-nomiss &gt; tfoot &gt; tr &gt; td, .st-cross-table &gt; thead &gt; tr &gt; th, .st-cross-table &gt; tbody &gt; tr &gt; th, .st-cross-table &gt; tfoot &gt; tr &gt; th, .st-cross-table &gt; thead &gt; tr &gt; td, .st-cross-table &gt; tbody &gt; tr &gt; td, .st-cross-table &gt; tfoot &gt; tr &gt; td { padding-left: 20px; padding-right: 20px; } .st-table-bordered &gt; thead &gt; tr &gt; th, .st-table-bordered &gt; tbody &gt; tr &gt; th, .st-table-bordered &gt; thead &gt; tr &gt; td, .st-table-bordered &gt; tbody &gt; tr &gt; td { border: 1px solid #cccccc; } .st-table-striped &gt; tbody &gt; tr:nth-of-type(odd) { background-color: #ffffff; } .st-table-striped &gt; tbody &gt; tr:nth-of-type(even) { background-color: #f9f9f9; } .st-descr-table &gt; thead &gt; tr &gt; th, .st-descr-table &gt; tbody &gt; tr &gt; th, .st-descr-table &gt; thead &gt; tr &gt; td, .st-descr-table &gt; tbody &gt; tr &gt; td { padding-left: 24px; padding-right: 24px; word-wrap: break-word; } .st-freq-table, .st-freq-table-nomiss, .st-cross-table { border: medium none; } .st-freq-table &gt; thead &gt; tr:nth-child(1) &gt; th:nth-child(1), .st-cross-table &gt; thead &gt; tr:nth-child(1) &gt; th:nth-child(1), .st-cross-table &gt; thead &gt; tr:nth-child(1) &gt; th:nth-child(3) { border: none; background-color: #ffffff; text-align: center; } .st-protect-top-border { border-top: 1px solid #cccccc !important; } .st-ws-char { display: inline; color: #999999; letter-spacing: 0.2em; } /* Optionnal classes */ .st-small { font-size: 13px; } .st-small td, .st-small th { padding: 8px; } .st-small &gt; thead &gt; tr &gt; th, .st-small &gt; tbody &gt; tr &gt; th, .st-small &gt; thead &gt; tr &gt; td, .st-small &gt; tbody &gt; tr &gt; td { padding-left: 12px; padding-right: 12px; } &lt;/style&gt; library(knitr) opts_chunk$set(comment = NA, prompt = FALSE, results=&#39;asis&#39;, collapse = FALSE) 如果之前没有设置 collpase = TRUE, collapse = FALSE 不是必要的 13.3.1 freq freq(iris$Species, plain.ascii = FALSE, style = &quot;rmarkdown&quot;, headings = FALSE) Freq % Valid % Valid Cum. % Total % Total Cum. setosa 50 33.33 33.33 33.33 33.33 versicolor 50 33.33 66.67 33.33 66.67 virginica 50 33.33 100.00 33.33 100.00 &lt;NA&gt; 0 0.00 100.00 Total 150 100.00 100.00 100.00 100.00 freq(iris$Species, report.nas = FALSE, headings = FALSE) F req % % Cum. setosa 50 33.33 33.33 versicolor 50 33.33 66.67 virginica 50 33.33 100.00 Total 150 100.00 100.00 freq(iris$Species, report.nas = FALSE, totals = FALSE, cumul = FALSE, style = &quot;rmarkdown&quot;, headings = FALSE) Freq % setosa 50 33.33 versicolor 50 33.33 virginica 50 33.33 13.3.2 descr() descr(iris) #&gt; Descriptive Statistics #&gt; iris #&gt; N: 150 #&gt; #&gt; Petal.Length Petal.Width Sepal.Length Sepal.Width #&gt; ----------------- -------------- ------------- -------------- ------------- #&gt; Mean 3.76 1.20 5.84 3.06 #&gt; Std.Dev 1.77 0.76 0.83 0.44 #&gt; Min 1.00 0.10 4.30 2.00 #&gt; Q1 1.60 0.30 5.10 2.80 #&gt; Median 4.35 1.30 5.80 3.00 #&gt; Q3 5.10 1.80 6.40 3.30 #&gt; Max 6.90 2.50 7.90 4.40 #&gt; MAD 1.85 1.04 1.04 0.44 #&gt; IQR 3.50 1.50 1.30 0.50 #&gt; CV 0.47 0.64 0.14 0.14 #&gt; Skewness -0.27 -0.10 0.31 0.31 #&gt; SE.Skewness 0.20 0.20 0.20 0.20 #&gt; Kurtosis -1.42 -1.36 -0.61 0.14 #&gt; N.Valid 150.00 150.00 150.00 150.00 #&gt; Pct.Valid 100.00 100.00 100.00 100.00 "],
["gt-and-gtsummary.html", "13.4 gt and gtsummary", " 13.4 gt and gtsummary # devtools::install_github(&quot;rstudio/gt&quot;) # install.packages(&quot;gtsummary&quot;) library(gt) library(gtsummary) tbl_summary( data = trial[c(&quot;trt&quot;, &quot;age&quot;, &quot;grade&quot;, &quot;response&quot;)], by = trt ) %&gt;% add_p() #&gt; &lt;style&gt;html { #&gt; font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif; #&gt; } #&gt; #&gt; #yfzokgxden .gt_table { #&gt; display: table; #&gt; border-collapse: collapse; #&gt; margin-left: auto; #&gt; margin-right: auto; #&gt; color: #333333; #&gt; font-size: 16px; #&gt; background-color: #FFFFFF; #&gt; width: auto; #&gt; border-top-style: solid; #&gt; border-top-width: 2px; #&gt; border-top-color: #A8A8A8; #&gt; border-right-style: none; #&gt; border-right-width: 2px; #&gt; border-right-color: #D3D3D3; #&gt; border-bottom-style: solid; #&gt; border-bottom-width: 2px; #&gt; border-bottom-color: #A8A8A8; #&gt; border-left-style: none; #&gt; border-left-width: 2px; #&gt; border-left-color: #D3D3D3; #&gt; } #&gt; #&gt; #yfzokgxden .gt_heading { #&gt; background-color: #FFFFFF; #&gt; text-align: center; #&gt; border-bottom-color: #FFFFFF; #&gt; border-left-style: none; #&gt; border-left-width: 1px; #&gt; border-left-color: #D3D3D3; #&gt; border-right-style: none; #&gt; border-right-width: 1px; #&gt; border-right-color: #D3D3D3; #&gt; } #&gt; #&gt; #yfzokgxden .gt_title { #&gt; color: #333333; #&gt; font-size: 125%; #&gt; font-weight: initial; #&gt; padding-top: 4px; #&gt; padding-bottom: 4px; #&gt; border-bottom-color: #FFFFFF; #&gt; border-bottom-width: 0; #&gt; } #&gt; #&gt; #yfzokgxden .gt_subtitle { #&gt; color: #333333; #&gt; font-size: 85%; #&gt; font-weight: initial; #&gt; padding-top: 0; #&gt; padding-bottom: 4px; #&gt; border-top-color: #FFFFFF; #&gt; border-top-width: 0; #&gt; } #&gt; #&gt; #yfzokgxden .gt_bottom_border { #&gt; border-bottom-style: solid; #&gt; border-bottom-width: 2px; #&gt; border-bottom-color: #D3D3D3; #&gt; } #&gt; #&gt; #yfzokgxden .gt_col_headings { #&gt; border-top-style: solid; #&gt; border-top-width: 2px; #&gt; border-top-color: #D3D3D3; #&gt; border-bottom-style: solid; #&gt; border-bottom-width: 2px; #&gt; border-bottom-color: #D3D3D3; #&gt; border-left-style: none; #&gt; border-left-width: 1px; #&gt; border-left-color: #D3D3D3; #&gt; border-right-style: none; #&gt; border-right-width: 1px; #&gt; border-right-color: #D3D3D3; #&gt; } #&gt; #&gt; #yfzokgxden .gt_col_heading { #&gt; color: #333333; #&gt; background-color: #FFFFFF; #&gt; font-size: 100%; #&gt; font-weight: normal; #&gt; text-transform: inherit; #&gt; border-left-style: none; #&gt; border-left-width: 1px; #&gt; border-left-color: #D3D3D3; #&gt; border-right-style: none; #&gt; border-right-width: 1px; #&gt; border-right-color: #D3D3D3; #&gt; vertical-align: bottom; #&gt; padding-top: 5px; #&gt; padding-bottom: 6px; #&gt; padding-left: 5px; #&gt; padding-right: 5px; #&gt; overflow-x: hidden; #&gt; } #&gt; #&gt; #yfzokgxden .gt_column_spanner_outer { #&gt; color: #333333; #&gt; background-color: #FFFFFF; #&gt; font-size: 100%; #&gt; font-weight: normal; #&gt; text-transform: inherit; #&gt; padding-top: 0; #&gt; padding-bottom: 0; #&gt; padding-left: 4px; #&gt; padding-right: 4px; #&gt; } #&gt; #&gt; #yfzokgxden .gt_column_spanner_outer:first-child { #&gt; padding-left: 0; #&gt; } #&gt; #&gt; #yfzokgxden .gt_column_spanner_outer:last-child { #&gt; padding-right: 0; #&gt; } #&gt; #&gt; #yfzokgxden .gt_column_spanner { #&gt; border-bottom-style: solid; #&gt; border-bottom-width: 2px; #&gt; border-bottom-color: #D3D3D3; #&gt; vertical-align: bottom; #&gt; padding-top: 5px; #&gt; padding-bottom: 6px; #&gt; overflow-x: hidden; #&gt; display: inline-block; #&gt; width: 100%; #&gt; } #&gt; #&gt; #yfzokgxden .gt_group_heading { #&gt; padding: 8px; #&gt; color: #333333; #&gt; background-color: #FFFFFF; #&gt; font-size: 100%; #&gt; font-weight: initial; #&gt; text-transform: inherit; #&gt; border-top-style: solid; #&gt; border-top-width: 2px; #&gt; border-top-color: #D3D3D3; #&gt; border-bottom-style: solid; #&gt; border-bottom-width: 2px; #&gt; border-bottom-color: #D3D3D3; #&gt; border-left-style: none; #&gt; border-left-width: 1px; #&gt; border-left-color: #D3D3D3; #&gt; border-right-style: none; #&gt; border-right-width: 1px; #&gt; border-right-color: #D3D3D3; #&gt; vertical-align: middle; #&gt; } #&gt; #&gt; #yfzokgxden .gt_empty_group_heading { #&gt; padding: 0.5px; #&gt; color: #333333; #&gt; background-color: #FFFFFF; #&gt; font-size: 100%; #&gt; font-weight: initial; #&gt; border-top-style: solid; #&gt; border-top-width: 2px; #&gt; border-top-color: #D3D3D3; #&gt; border-bottom-style: solid; #&gt; border-bottom-width: 2px; #&gt; border-bottom-color: #D3D3D3; #&gt; vertical-align: middle; #&gt; } #&gt; #&gt; #yfzokgxden .gt_striped { #&gt; background-color: rgba(128, 128, 128, 0.05); #&gt; } #&gt; #&gt; #yfzokgxden .gt_from_md &gt; :first-child { #&gt; margin-top: 0; #&gt; } #&gt; #&gt; #yfzokgxden .gt_from_md &gt; :last-child { #&gt; margin-bottom: 0; #&gt; } #&gt; #&gt; #yfzokgxden .gt_row { #&gt; padding-top: 8px; #&gt; padding-bottom: 8px; #&gt; padding-left: 5px; #&gt; padding-right: 5px; #&gt; margin: 10px; #&gt; border-top-style: solid; #&gt; border-top-width: 1px; #&gt; border-top-color: #D3D3D3; #&gt; border-left-style: none; #&gt; border-left-width: 1px; #&gt; border-left-color: #D3D3D3; #&gt; border-right-style: none; #&gt; border-right-width: 1px; #&gt; border-right-color: #D3D3D3; #&gt; vertical-align: middle; #&gt; overflow-x: hidden; #&gt; } #&gt; #&gt; #yfzokgxden .gt_stub { #&gt; color: #333333; #&gt; background-color: #FFFFFF; #&gt; font-size: 100%; #&gt; font-weight: initial; #&gt; text-transform: inherit; #&gt; border-right-style: solid; #&gt; border-right-width: 2px; #&gt; border-right-color: #D3D3D3; #&gt; padding-left: 12px; #&gt; } #&gt; #&gt; #yfzokgxden .gt_summary_row { #&gt; color: #333333; #&gt; background-color: #FFFFFF; #&gt; text-transform: inherit; #&gt; padding-top: 8px; #&gt; padding-bottom: 8px; #&gt; padding-left: 5px; #&gt; padding-right: 5px; #&gt; } #&gt; #&gt; #yfzokgxden .gt_first_summary_row { #&gt; padding-top: 8px; #&gt; padding-bottom: 8px; #&gt; padding-left: 5px; #&gt; padding-right: 5px; #&gt; border-top-style: solid; #&gt; border-top-width: 2px; #&gt; border-top-color: #D3D3D3; #&gt; } #&gt; #&gt; #yfzokgxden .gt_grand_summary_row { #&gt; color: #333333; #&gt; background-color: #FFFFFF; #&gt; text-transform: inherit; #&gt; padding-top: 8px; #&gt; padding-bottom: 8px; #&gt; padding-left: 5px; #&gt; padding-right: 5px; #&gt; } #&gt; #&gt; #yfzokgxden .gt_first_grand_summary_row { #&gt; padding-top: 8px; #&gt; padding-bottom: 8px; #&gt; padding-left: 5px; #&gt; padding-right: 5px; #&gt; border-top-style: double; #&gt; border-top-width: 6px; #&gt; border-top-color: #D3D3D3; #&gt; } #&gt; #&gt; #yfzokgxden .gt_table_body { #&gt; border-top-style: solid; #&gt; border-top-width: 2px; #&gt; border-top-color: #D3D3D3; #&gt; border-bottom-style: solid; #&gt; border-bottom-width: 2px; #&gt; border-bottom-color: #D3D3D3; #&gt; } #&gt; #&gt; #yfzokgxden .gt_footnotes { #&gt; color: #333333; #&gt; background-color: #FFFFFF; #&gt; border-bottom-style: none; #&gt; border-bottom-width: 2px; #&gt; border-bottom-color: #D3D3D3; #&gt; border-left-style: none; #&gt; border-left-width: 2px; #&gt; border-left-color: #D3D3D3; #&gt; border-right-style: none; #&gt; border-right-width: 2px; #&gt; border-right-color: #D3D3D3; #&gt; } #&gt; #&gt; #yfzokgxden .gt_footnote { #&gt; margin: 0px; #&gt; font-size: 90%; #&gt; padding: 4px; #&gt; } #&gt; #&gt; #yfzokgxden .gt_sourcenotes { #&gt; color: #333333; #&gt; background-color: #FFFFFF; #&gt; border-bottom-style: none; #&gt; border-bottom-width: 2px; #&gt; border-bottom-color: #D3D3D3; #&gt; border-left-style: none; #&gt; border-left-width: 2px; #&gt; border-left-color: #D3D3D3; #&gt; border-right-style: none; #&gt; border-right-width: 2px; #&gt; border-right-color: #D3D3D3; #&gt; } #&gt; #&gt; #yfzokgxden .gt_sourcenote { #&gt; font-size: 90%; #&gt; padding: 4px; #&gt; } #&gt; #&gt; #yfzokgxden .gt_left { #&gt; text-align: left; #&gt; } #&gt; #&gt; #yfzokgxden .gt_center { #&gt; text-align: center; #&gt; } #&gt; #&gt; #yfzokgxden .gt_right { #&gt; text-align: right; #&gt; font-variant-numeric: tabular-nums; #&gt; } #&gt; #&gt; #yfzokgxden .gt_font_normal { #&gt; font-weight: normal; #&gt; } #&gt; #&gt; #yfzokgxden .gt_font_bold { #&gt; font-weight: bold; #&gt; } #&gt; #&gt; #yfzokgxden .gt_font_italic { #&gt; font-style: italic; #&gt; } #&gt; #&gt; #yfzokgxden .gt_super { #&gt; font-size: 65%; #&gt; } #&gt; #&gt; #yfzokgxden .gt_footnote_marks { #&gt; font-style: italic; #&gt; font-size: 65%; #&gt; } #&gt; &lt;/style&gt; #&gt; &lt;div id=&quot;yfzokgxden&quot; style=&quot;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&quot;&gt;&lt;table class=&quot;gt_table&quot;&gt; #&gt; #&gt; &lt;thead class=&quot;gt_col_headings&quot;&gt; #&gt; &lt;tr&gt; #&gt; &lt;th class=&quot;gt_col_heading gt_columns_bottom_border gt_left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;strong&gt;Characteristic&lt;/strong&gt;&lt;/th&gt; #&gt; &lt;th class=&quot;gt_col_heading gt_columns_bottom_border gt_center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;strong&gt;Drug A&lt;/strong&gt;, N = 98&lt;sup class=&quot;gt_footnote_marks&quot;&gt;1&lt;/sup&gt;&lt;/th&gt; #&gt; &lt;th class=&quot;gt_col_heading gt_columns_bottom_border gt_center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;strong&gt;Drug B&lt;/strong&gt;, N = 102&lt;sup class=&quot;gt_footnote_marks&quot;&gt;1&lt;/sup&gt;&lt;/th&gt; #&gt; &lt;th class=&quot;gt_col_heading gt_columns_bottom_border gt_center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;strong&gt;p-value&lt;/strong&gt;&lt;sup class=&quot;gt_footnote_marks&quot;&gt;2&lt;/sup&gt;&lt;/th&gt; #&gt; &lt;/tr&gt; #&gt; &lt;/thead&gt; #&gt; &lt;tbody class=&quot;gt_table_body&quot;&gt; #&gt; &lt;tr&gt; #&gt; &lt;td class=&quot;gt_row gt_left&quot;&gt;Age, yrs&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;46 (37, 59)&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;48 (39, 56)&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;0.7&lt;/td&gt; #&gt; &lt;/tr&gt; #&gt; &lt;tr&gt; #&gt; &lt;td class=&quot;gt_row gt_left&quot; style=&quot;text-align: left; text-indent: 10px;&quot;&gt;Unknown&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;7&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;4&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;&lt;/td&gt; #&gt; &lt;/tr&gt; #&gt; &lt;tr&gt; #&gt; &lt;td class=&quot;gt_row gt_left&quot;&gt;Grade&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;0.9&lt;/td&gt; #&gt; &lt;/tr&gt; #&gt; &lt;tr&gt; #&gt; &lt;td class=&quot;gt_row gt_left&quot; style=&quot;text-align: left; text-indent: 10px;&quot;&gt;I&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;35 (36%)&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;33 (32%)&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;&lt;/td&gt; #&gt; &lt;/tr&gt; #&gt; &lt;tr&gt; #&gt; &lt;td class=&quot;gt_row gt_left&quot; style=&quot;text-align: left; text-indent: 10px;&quot;&gt;II&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;32 (33%)&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;36 (35%)&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;&lt;/td&gt; #&gt; &lt;/tr&gt; #&gt; &lt;tr&gt; #&gt; &lt;td class=&quot;gt_row gt_left&quot; style=&quot;text-align: left; text-indent: 10px;&quot;&gt;III&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;31 (32%)&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;33 (32%)&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;&lt;/td&gt; #&gt; &lt;/tr&gt; #&gt; &lt;tr&gt; #&gt; &lt;td class=&quot;gt_row gt_left&quot;&gt;Tumor Response&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;28 (29%)&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;33 (34%)&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;0.6&lt;/td&gt; #&gt; &lt;/tr&gt; #&gt; &lt;tr&gt; #&gt; &lt;td class=&quot;gt_row gt_left&quot; style=&quot;text-align: left; text-indent: 10px;&quot;&gt;Unknown&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;3&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;4&lt;/td&gt; #&gt; &lt;td class=&quot;gt_row gt_center&quot;&gt;&lt;/td&gt; #&gt; &lt;/tr&gt; #&gt; &lt;/tbody&gt; #&gt; #&gt; &lt;tfoot&gt; #&gt; &lt;tr class=&quot;gt_footnotes&quot;&gt; #&gt; &lt;td colspan=&quot;4&quot;&gt; #&gt; &lt;p class=&quot;gt_footnote&quot;&gt; #&gt; &lt;sup class=&quot;gt_footnote_marks&quot;&gt; #&gt; &lt;em&gt;1&lt;/em&gt; #&gt; &lt;/sup&gt; #&gt; #&gt; Statistics presented: median (IQR); n (%) #&gt; &lt;br /&gt; #&gt; &lt;/p&gt; #&gt; &lt;p class=&quot;gt_footnote&quot;&gt; #&gt; &lt;sup class=&quot;gt_footnote_marks&quot;&gt; #&gt; &lt;em&gt;2&lt;/em&gt; #&gt; &lt;/sup&gt; #&gt; #&gt; Statistical tests performed: Wilcoxon rank-sum test; chi-square test of independence #&gt; &lt;br /&gt; #&gt; &lt;/p&gt; #&gt; &lt;/td&gt; #&gt; &lt;/tr&gt; #&gt; &lt;/tfoot&gt; #&gt; &lt;/table&gt;&lt;/div&gt; "],
["naniar.html", "13.5 naniar", " 13.5 naniar library(naniar) ggplot(data = airquality, aes(x = Ozone, y = Solar.R)) + geom_miss_point() "],
["janitor.html", "14 Janitor", " 14 Janitor blog: http://sfirke.github.io/janitor/articles/janitor.html janitor(Firke 2019) 提供了一些简单易用的函数方便数据清洗和探索流程。 library(janitor) "],
["janitor-cleaning.html", "14.1 cleaning", " 14.1 cleaning 14.1.1 clean_names clean_names() 将输入数据框的列名转换为整洁格式，与 readxl::read_excel() 和 readr::read_csv() 等不会擅自修改原列名的函数搭配使用效果最佳。 clean_names() 的 输入输出 都是数据框，这使它很适应和管道操作符 %&gt;% 和 tidyverse 中的其他函数一同工作。 列名的转换有以下几种主要情形： 统一字母的大小写，采用一致的命名方式（默认为蛇形命名法 snake_case） 自动为重复的列名编号，填充空的列名 删除空格和某些特殊字符，如括号， œ、oe “%” 转换至 “percent”, “#” 转换至 “number” # Create a data.frame with dirty names test_df &lt;- as.data.frame(matrix(ncol = 6)) names(test_df) &lt;- c(&quot;firstName&quot;, &quot;ábc@!*&quot;, &quot;% successful (2009)&quot;, &quot;REPEAT VALUE&quot;, &quot;REPEAT VALUE&quot;, &quot;&quot;) # Clean the variable names, returning a data.frame: test_df %&gt;% clean_names() #&gt; first_name abc percent_successful_2009 repeat_value repeat_value_2 x #&gt; 1 NA NA NA NA NA NA 与 Base R 中的 make.names() 对比 (注意这个函数是基于字符向量的)： names(test_df) %&gt;% make.names() #&gt; [1] &quot;firstName&quot; &quot;ábc...&quot; &quot;X..successful..2009.&quot; #&gt; [4] &quot;REPEAT.VALUE&quot; &quot;REPEAT.VALUE&quot; &quot;X&quot; 改变命名规范： # snake_case test_df %&gt;% clean_names() #&gt; first_name abc percent_successful_2009 repeat_value repeat_value_2 x #&gt; 1 NA NA NA NA NA NA # lower_camel and upper_camel test_df %&gt;% clean_names(case = &quot;lower_camel&quot;) #&gt; firstName abc percentSuccessful2009 repeatValue repeatValue_2 x #&gt; 1 NA NA NA NA NA NA test_df %&gt;% clean_names(case = &quot;upper_camel&quot;) #&gt; FirstName Abc PercentSuccessful2009 RepeatValue RepeatValue_2 X #&gt; 1 NA NA NA NA NA NA clean_names() 只能适用于于数据框 ，而 make_clean_names() 适用于向量，这使后者可以作为很多函数的参数，进行有效的 functional programming， 例如以下四种修改列名的方式是等效的： iris %&gt;% clean_names() #&gt; sepal_length sepal_width petal_length petal_width species #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3.0 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5.0 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa #&gt; 7 4.6 3.4 1.4 0.3 setosa #&gt; 8 5.0 3.4 1.5 0.2 setosa #&gt; 9 4.4 2.9 1.4 0.2 setosa #&gt; 10 4.9 3.1 1.5 0.1 setosa #&gt; 11 5.4 3.7 1.5 0.2 setosa #&gt; 12 4.8 3.4 1.6 0.2 setosa #&gt; 13 4.8 3.0 1.4 0.1 setosa #&gt; 14 4.3 3.0 1.1 0.1 setosa #&gt; 15 5.8 4.0 1.2 0.2 setosa #&gt; 16 5.7 4.4 1.5 0.4 setosa #&gt; 17 5.4 3.9 1.3 0.4 setosa #&gt; 18 5.1 3.5 1.4 0.3 setosa #&gt; 19 5.7 3.8 1.7 0.3 setosa #&gt; 20 5.1 3.8 1.5 0.3 setosa #&gt; 21 5.4 3.4 1.7 0.2 setosa #&gt; 22 5.1 3.7 1.5 0.4 setosa #&gt; 23 4.6 3.6 1.0 0.2 setosa #&gt; 24 5.1 3.3 1.7 0.5 setosa #&gt; 25 4.8 3.4 1.9 0.2 setosa #&gt; 26 5.0 3.0 1.6 0.2 setosa #&gt; 27 5.0 3.4 1.6 0.4 setosa #&gt; 28 5.2 3.5 1.5 0.2 setosa #&gt; 29 5.2 3.4 1.4 0.2 setosa #&gt; 30 4.7 3.2 1.6 0.2 setosa #&gt; 31 4.8 3.1 1.6 0.2 setosa #&gt; 32 5.4 3.4 1.5 0.4 setosa #&gt; 33 5.2 4.1 1.5 0.1 setosa #&gt; 34 5.5 4.2 1.4 0.2 setosa #&gt; 35 4.9 3.1 1.5 0.2 setosa #&gt; 36 5.0 3.2 1.2 0.2 setosa #&gt; 37 5.5 3.5 1.3 0.2 setosa #&gt; 38 4.9 3.6 1.4 0.1 setosa #&gt; 39 4.4 3.0 1.3 0.2 setosa #&gt; 40 5.1 3.4 1.5 0.2 setosa #&gt; 41 5.0 3.5 1.3 0.3 setosa #&gt; 42 4.5 2.3 1.3 0.3 setosa #&gt; 43 4.4 3.2 1.3 0.2 setosa #&gt; 44 5.0 3.5 1.6 0.6 setosa #&gt; 45 5.1 3.8 1.9 0.4 setosa #&gt; 46 4.8 3.0 1.4 0.3 setosa #&gt; 47 5.1 3.8 1.6 0.2 setosa #&gt; 48 4.6 3.2 1.4 0.2 setosa #&gt; 49 5.3 3.7 1.5 0.2 setosa #&gt; 50 5.0 3.3 1.4 0.2 setosa #&gt; 51 7.0 3.2 4.7 1.4 versicolor #&gt; 52 6.4 3.2 4.5 1.5 versicolor #&gt; 53 6.9 3.1 4.9 1.5 versicolor #&gt; 54 5.5 2.3 4.0 1.3 versicolor #&gt; 55 6.5 2.8 4.6 1.5 versicolor #&gt; 56 5.7 2.8 4.5 1.3 versicolor #&gt; 57 6.3 3.3 4.7 1.6 versicolor #&gt; 58 4.9 2.4 3.3 1.0 versicolor #&gt; 59 6.6 2.9 4.6 1.3 versicolor #&gt; 60 5.2 2.7 3.9 1.4 versicolor #&gt; 61 5.0 2.0 3.5 1.0 versicolor #&gt; 62 5.9 3.0 4.2 1.5 versicolor #&gt; 63 6.0 2.2 4.0 1.0 versicolor #&gt; 64 6.1 2.9 4.7 1.4 versicolor #&gt; 65 5.6 2.9 3.6 1.3 versicolor #&gt; 66 6.7 3.1 4.4 1.4 versicolor #&gt; 67 5.6 3.0 4.5 1.5 versicolor #&gt; 68 5.8 2.7 4.1 1.0 versicolor #&gt; 69 6.2 2.2 4.5 1.5 versicolor #&gt; 70 5.6 2.5 3.9 1.1 versicolor #&gt; 71 5.9 3.2 4.8 1.8 versicolor #&gt; 72 6.1 2.8 4.0 1.3 versicolor #&gt; 73 6.3 2.5 4.9 1.5 versicolor #&gt; 74 6.1 2.8 4.7 1.2 versicolor #&gt; 75 6.4 2.9 4.3 1.3 versicolor #&gt; 76 6.6 3.0 4.4 1.4 versicolor #&gt; 77 6.8 2.8 4.8 1.4 versicolor #&gt; 78 6.7 3.0 5.0 1.7 versicolor #&gt; 79 6.0 2.9 4.5 1.5 versicolor #&gt; 80 5.7 2.6 3.5 1.0 versicolor #&gt; 81 5.5 2.4 3.8 1.1 versicolor #&gt; 82 5.5 2.4 3.7 1.0 versicolor #&gt; 83 5.8 2.7 3.9 1.2 versicolor #&gt; 84 6.0 2.7 5.1 1.6 versicolor #&gt; 85 5.4 3.0 4.5 1.5 versicolor #&gt; 86 6.0 3.4 4.5 1.6 versicolor #&gt; 87 6.7 3.1 4.7 1.5 versicolor #&gt; 88 6.3 2.3 4.4 1.3 versicolor #&gt; 89 5.6 3.0 4.1 1.3 versicolor #&gt; 90 5.5 2.5 4.0 1.3 versicolor #&gt; 91 5.5 2.6 4.4 1.2 versicolor #&gt; 92 6.1 3.0 4.6 1.4 versicolor #&gt; 93 5.8 2.6 4.0 1.2 versicolor #&gt; 94 5.0 2.3 3.3 1.0 versicolor #&gt; 95 5.6 2.7 4.2 1.3 versicolor #&gt; 96 5.7 3.0 4.2 1.2 versicolor #&gt; 97 5.7 2.9 4.2 1.3 versicolor #&gt; 98 6.2 2.9 4.3 1.3 versicolor #&gt; 99 5.1 2.5 3.0 1.1 versicolor #&gt; 100 5.7 2.8 4.1 1.3 versicolor #&gt; 101 6.3 3.3 6.0 2.5 virginica #&gt; 102 5.8 2.7 5.1 1.9 virginica #&gt; 103 7.1 3.0 5.9 2.1 virginica #&gt; 104 6.3 2.9 5.6 1.8 virginica #&gt; 105 6.5 3.0 5.8 2.2 virginica #&gt; 106 7.6 3.0 6.6 2.1 virginica #&gt; 107 4.9 2.5 4.5 1.7 virginica #&gt; 108 7.3 2.9 6.3 1.8 virginica #&gt; 109 6.7 2.5 5.8 1.8 virginica #&gt; 110 7.2 3.6 6.1 2.5 virginica #&gt; 111 6.5 3.2 5.1 2.0 virginica #&gt; 112 6.4 2.7 5.3 1.9 virginica #&gt; 113 6.8 3.0 5.5 2.1 virginica #&gt; 114 5.7 2.5 5.0 2.0 virginica #&gt; 115 5.8 2.8 5.1 2.4 virginica #&gt; 116 6.4 3.2 5.3 2.3 virginica #&gt; 117 6.5 3.0 5.5 1.8 virginica #&gt; 118 7.7 3.8 6.7 2.2 virginica #&gt; 119 7.7 2.6 6.9 2.3 virginica #&gt; 120 6.0 2.2 5.0 1.5 virginica #&gt; 121 6.9 3.2 5.7 2.3 virginica #&gt; 122 5.6 2.8 4.9 2.0 virginica #&gt; 123 7.7 2.8 6.7 2.0 virginica #&gt; 124 6.3 2.7 4.9 1.8 virginica #&gt; 125 6.7 3.3 5.7 2.1 virginica #&gt; 126 7.2 3.2 6.0 1.8 virginica #&gt; 127 6.2 2.8 4.8 1.8 virginica #&gt; 128 6.1 3.0 4.9 1.8 virginica #&gt; 129 6.4 2.8 5.6 2.1 virginica #&gt; 130 7.2 3.0 5.8 1.6 virginica #&gt; 131 7.4 2.8 6.1 1.9 virginica #&gt; 132 7.9 3.8 6.4 2.0 virginica #&gt; 133 6.4 2.8 5.6 2.2 virginica #&gt; 134 6.3 2.8 5.1 1.5 virginica #&gt; 135 6.1 2.6 5.6 1.4 virginica #&gt; 136 7.7 3.0 6.1 2.3 virginica #&gt; 137 6.3 3.4 5.6 2.4 virginica #&gt; 138 6.4 3.1 5.5 1.8 virginica #&gt; 139 6.0 3.0 4.8 1.8 virginica #&gt; 140 6.9 3.1 5.4 2.1 virginica #&gt; 141 6.7 3.1 5.6 2.4 virginica #&gt; 142 6.9 3.1 5.1 2.3 virginica #&gt; 143 5.8 2.7 5.1 1.9 virginica #&gt; 144 6.8 3.2 5.9 2.3 virginica #&gt; 145 6.7 3.3 5.7 2.5 virginica #&gt; 146 6.7 3.0 5.2 2.3 virginica #&gt; 147 6.3 2.5 5.0 1.9 virginica #&gt; 148 6.5 3.0 5.2 2.0 virginica #&gt; 149 6.2 3.4 5.4 2.3 virginica #&gt; 150 5.9 3.0 5.1 1.8 virginica iris %&gt;% names() %&gt;% make_clean_names() #&gt; [1] &quot;sepal_length&quot; &quot;sepal_width&quot; &quot;petal_length&quot; &quot;petal_width&quot; &quot;species&quot; iris %&gt;% as_tibble(name_repair = make_clean_names) #&gt; # A tibble: 150 x 5 #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa #&gt; # ... with 144 more rows iris %&gt;% rename_all(make_clean_names) #&gt; sepal_length sepal_width petal_length petal_width species #&gt; 1 5.1 3.5 1.4 0.2 setosa #&gt; 2 4.9 3.0 1.4 0.2 setosa #&gt; 3 4.7 3.2 1.3 0.2 setosa #&gt; 4 4.6 3.1 1.5 0.2 setosa #&gt; 5 5.0 3.6 1.4 0.2 setosa #&gt; 6 5.4 3.9 1.7 0.4 setosa #&gt; 7 4.6 3.4 1.4 0.3 setosa #&gt; 8 5.0 3.4 1.5 0.2 setosa #&gt; 9 4.4 2.9 1.4 0.2 setosa #&gt; 10 4.9 3.1 1.5 0.1 setosa #&gt; 11 5.4 3.7 1.5 0.2 setosa #&gt; 12 4.8 3.4 1.6 0.2 setosa #&gt; 13 4.8 3.0 1.4 0.1 setosa #&gt; 14 4.3 3.0 1.1 0.1 setosa #&gt; 15 5.8 4.0 1.2 0.2 setosa #&gt; 16 5.7 4.4 1.5 0.4 setosa #&gt; 17 5.4 3.9 1.3 0.4 setosa #&gt; 18 5.1 3.5 1.4 0.3 setosa #&gt; 19 5.7 3.8 1.7 0.3 setosa #&gt; 20 5.1 3.8 1.5 0.3 setosa #&gt; 21 5.4 3.4 1.7 0.2 setosa #&gt; 22 5.1 3.7 1.5 0.4 setosa #&gt; 23 4.6 3.6 1.0 0.2 setosa #&gt; 24 5.1 3.3 1.7 0.5 setosa #&gt; 25 4.8 3.4 1.9 0.2 setosa #&gt; 26 5.0 3.0 1.6 0.2 setosa #&gt; 27 5.0 3.4 1.6 0.4 setosa #&gt; 28 5.2 3.5 1.5 0.2 setosa #&gt; 29 5.2 3.4 1.4 0.2 setosa #&gt; 30 4.7 3.2 1.6 0.2 setosa #&gt; 31 4.8 3.1 1.6 0.2 setosa #&gt; 32 5.4 3.4 1.5 0.4 setosa #&gt; 33 5.2 4.1 1.5 0.1 setosa #&gt; 34 5.5 4.2 1.4 0.2 setosa #&gt; 35 4.9 3.1 1.5 0.2 setosa #&gt; 36 5.0 3.2 1.2 0.2 setosa #&gt; 37 5.5 3.5 1.3 0.2 setosa #&gt; 38 4.9 3.6 1.4 0.1 setosa #&gt; 39 4.4 3.0 1.3 0.2 setosa #&gt; 40 5.1 3.4 1.5 0.2 setosa #&gt; 41 5.0 3.5 1.3 0.3 setosa #&gt; 42 4.5 2.3 1.3 0.3 setosa #&gt; 43 4.4 3.2 1.3 0.2 setosa #&gt; 44 5.0 3.5 1.6 0.6 setosa #&gt; 45 5.1 3.8 1.9 0.4 setosa #&gt; 46 4.8 3.0 1.4 0.3 setosa #&gt; 47 5.1 3.8 1.6 0.2 setosa #&gt; 48 4.6 3.2 1.4 0.2 setosa #&gt; 49 5.3 3.7 1.5 0.2 setosa #&gt; 50 5.0 3.3 1.4 0.2 setosa #&gt; 51 7.0 3.2 4.7 1.4 versicolor #&gt; 52 6.4 3.2 4.5 1.5 versicolor #&gt; 53 6.9 3.1 4.9 1.5 versicolor #&gt; 54 5.5 2.3 4.0 1.3 versicolor #&gt; 55 6.5 2.8 4.6 1.5 versicolor #&gt; 56 5.7 2.8 4.5 1.3 versicolor #&gt; 57 6.3 3.3 4.7 1.6 versicolor #&gt; 58 4.9 2.4 3.3 1.0 versicolor #&gt; 59 6.6 2.9 4.6 1.3 versicolor #&gt; 60 5.2 2.7 3.9 1.4 versicolor #&gt; 61 5.0 2.0 3.5 1.0 versicolor #&gt; 62 5.9 3.0 4.2 1.5 versicolor #&gt; 63 6.0 2.2 4.0 1.0 versicolor #&gt; 64 6.1 2.9 4.7 1.4 versicolor #&gt; 65 5.6 2.9 3.6 1.3 versicolor #&gt; 66 6.7 3.1 4.4 1.4 versicolor #&gt; 67 5.6 3.0 4.5 1.5 versicolor #&gt; 68 5.8 2.7 4.1 1.0 versicolor #&gt; 69 6.2 2.2 4.5 1.5 versicolor #&gt; 70 5.6 2.5 3.9 1.1 versicolor #&gt; 71 5.9 3.2 4.8 1.8 versicolor #&gt; 72 6.1 2.8 4.0 1.3 versicolor #&gt; 73 6.3 2.5 4.9 1.5 versicolor #&gt; 74 6.1 2.8 4.7 1.2 versicolor #&gt; 75 6.4 2.9 4.3 1.3 versicolor #&gt; 76 6.6 3.0 4.4 1.4 versicolor #&gt; 77 6.8 2.8 4.8 1.4 versicolor #&gt; 78 6.7 3.0 5.0 1.7 versicolor #&gt; 79 6.0 2.9 4.5 1.5 versicolor #&gt; 80 5.7 2.6 3.5 1.0 versicolor #&gt; 81 5.5 2.4 3.8 1.1 versicolor #&gt; 82 5.5 2.4 3.7 1.0 versicolor #&gt; 83 5.8 2.7 3.9 1.2 versicolor #&gt; 84 6.0 2.7 5.1 1.6 versicolor #&gt; 85 5.4 3.0 4.5 1.5 versicolor #&gt; 86 6.0 3.4 4.5 1.6 versicolor #&gt; 87 6.7 3.1 4.7 1.5 versicolor #&gt; 88 6.3 2.3 4.4 1.3 versicolor #&gt; 89 5.6 3.0 4.1 1.3 versicolor #&gt; 90 5.5 2.5 4.0 1.3 versicolor #&gt; 91 5.5 2.6 4.4 1.2 versicolor #&gt; 92 6.1 3.0 4.6 1.4 versicolor #&gt; 93 5.8 2.6 4.0 1.2 versicolor #&gt; 94 5.0 2.3 3.3 1.0 versicolor #&gt; 95 5.6 2.7 4.2 1.3 versicolor #&gt; 96 5.7 3.0 4.2 1.2 versicolor #&gt; 97 5.7 2.9 4.2 1.3 versicolor #&gt; 98 6.2 2.9 4.3 1.3 versicolor #&gt; 99 5.1 2.5 3.0 1.1 versicolor #&gt; 100 5.7 2.8 4.1 1.3 versicolor #&gt; 101 6.3 3.3 6.0 2.5 virginica #&gt; 102 5.8 2.7 5.1 1.9 virginica #&gt; 103 7.1 3.0 5.9 2.1 virginica #&gt; 104 6.3 2.9 5.6 1.8 virginica #&gt; 105 6.5 3.0 5.8 2.2 virginica #&gt; 106 7.6 3.0 6.6 2.1 virginica #&gt; 107 4.9 2.5 4.5 1.7 virginica #&gt; 108 7.3 2.9 6.3 1.8 virginica #&gt; 109 6.7 2.5 5.8 1.8 virginica #&gt; 110 7.2 3.6 6.1 2.5 virginica #&gt; 111 6.5 3.2 5.1 2.0 virginica #&gt; 112 6.4 2.7 5.3 1.9 virginica #&gt; 113 6.8 3.0 5.5 2.1 virginica #&gt; 114 5.7 2.5 5.0 2.0 virginica #&gt; 115 5.8 2.8 5.1 2.4 virginica #&gt; 116 6.4 3.2 5.3 2.3 virginica #&gt; 117 6.5 3.0 5.5 1.8 virginica #&gt; 118 7.7 3.8 6.7 2.2 virginica #&gt; 119 7.7 2.6 6.9 2.3 virginica #&gt; 120 6.0 2.2 5.0 1.5 virginica #&gt; 121 6.9 3.2 5.7 2.3 virginica #&gt; 122 5.6 2.8 4.9 2.0 virginica #&gt; 123 7.7 2.8 6.7 2.0 virginica #&gt; 124 6.3 2.7 4.9 1.8 virginica #&gt; 125 6.7 3.3 5.7 2.1 virginica #&gt; 126 7.2 3.2 6.0 1.8 virginica #&gt; 127 6.2 2.8 4.8 1.8 virginica #&gt; 128 6.1 3.0 4.9 1.8 virginica #&gt; 129 6.4 2.8 5.6 2.1 virginica #&gt; 130 7.2 3.0 5.8 1.6 virginica #&gt; 131 7.4 2.8 6.1 1.9 virginica #&gt; 132 7.9 3.8 6.4 2.0 virginica #&gt; 133 6.4 2.8 5.6 2.2 virginica #&gt; 134 6.3 2.8 5.1 1.5 virginica #&gt; 135 6.1 2.6 5.6 1.4 virginica #&gt; 136 7.7 3.0 6.1 2.3 virginica #&gt; 137 6.3 3.4 5.6 2.4 virginica #&gt; 138 6.4 3.1 5.5 1.8 virginica #&gt; 139 6.0 3.0 4.8 1.8 virginica #&gt; 140 6.9 3.1 5.4 2.1 virginica #&gt; 141 6.7 3.1 5.6 2.4 virginica #&gt; 142 6.9 3.1 5.1 2.3 virginica #&gt; 143 5.8 2.7 5.1 1.9 virginica #&gt; 144 6.8 3.2 5.9 2.3 virginica #&gt; 145 6.7 3.3 5.7 2.5 virginica #&gt; 146 6.7 3.0 5.2 2.3 virginica #&gt; 147 6.3 2.5 5.0 1.9 virginica #&gt; 148 6.5 3.0 5.2 2.0 virginica #&gt; 149 6.2 3.4 5.4 2.3 virginica #&gt; 150 5.9 3.0 5.1 1.8 virginica "],
["janitor-explore.html", "14.2 Exploring", " 14.2 Exploring 14.2.1 tabyl tabyl() 的设计初衷是替代 Base R 中 table()，后者有几个缺点： 不接受数据框输入 不返回数据框 返回的结果很难进一步修饰 tabyl() 用于构建 1 ~ 3 个变量的（交叉）频数表，它 建立在 dplyr 和 tidyr 之上，所以以数据框基本输入、输出对象（但也可以接受一维向量），janitor 还提供了 adorn_* 函数族对其返回的表格进行修饰。以 starwars的一个子集演示 tabyl() 的用法： humans &lt;- starwars %&gt;% filter(species == &quot;Human&quot;) One-way tabyl 一维频数表 t1 &lt;- humans %&gt;% tabyl(eye_color) t1 #&gt; eye_color n percent #&gt; blue 12 0.3429 #&gt; blue-gray 1 0.0286 #&gt; brown 17 0.4857 #&gt; dark 1 0.0286 #&gt; hazel 2 0.0571 #&gt; yellow 2 0.0571 tably() 可以聪明地处理数据中包含缺失值的情况： x &lt;- c(&quot;big&quot;, &quot;big&quot;, &quot;small&quot;, &quot;small&quot;, &quot;small&quot;, NA) tabyl(x) #&gt; x n percent valid_percent #&gt; big 2 0.333 0.4 #&gt; small 3 0.500 0.6 #&gt; &lt;NA&gt; 1 0.167 NA tabyl(x, show_na = F) #&gt; x n percent #&gt; big 2 0.4 #&gt; small 3 0.6 大部分 adorn_* 函数主要用于二维列联表，但也可以适用一维频数表： t1 %&gt;% adorn_pct_formatting() #&gt; eye_color n percent #&gt; blue 12 34.3% #&gt; blue-gray 1 2.9% #&gt; brown 17 48.6% #&gt; dark 1 2.9% #&gt; hazel 2 5.7% #&gt; yellow 2 5.7% Two-way tabyl df %&gt;% tabyl(var_1, var_2) 等同于 df %&gt;% count(var_1, var_2) 后 pivot_wider() 展开其中的某一列，生成列联表： t2 &lt;- humans %&gt;% tabyl(gender, eye_color) t2 #&gt; gender blue blue-gray brown dark hazel yellow #&gt; female 3 0 5 0 1 0 #&gt; male 9 1 12 1 1 2 # count() + pivot_wider() humans %&gt;% count(gender, eye_color) %&gt;% pivot_wider(names_from = eye_color, values_from = n) #&gt; # A tibble: 2 x 7 #&gt; gender blue brown hazel `blue-gray` dark yellow #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 female 3 5 1 NA NA NA #&gt; 2 male 9 12 1 1 1 2 用于修饰的 adorn_* 函数有： adorn_totals(c(\"row\", \"col\")): 添加行列汇总 adorn_percentages(c(\"row\", \"col\"))： 将交叉表的指替换为行或列百分比 adorn_pct_formatting(digits, rounding): 决定百分比的格式 adorn_rounding(): Round a data.frame of numbers (usually the result of adorn_percentages), either using the base R round() function or using janitor’s round_half_up() to round all ties up (thanks, StackOverflow). e.g., round 10.5 up to 11, consistent with Excel’s tie-breaking behavior. This contrasts with rounding 10.5 down to 10 as in base R’s round(10.5). adorn_rounding() returns columns of class numeric, allowing for graphing, sorting, etc. It’s a less-aggressive substitute for adorn_pct_formatting(); these two functions should not be called together. adorn_ns(): add Ns to a tabyl. These can be drawn from the tabyl’s underlying counts, which are attached to the tabyl as metadata, or they can be supplied by the user. adorn_title(placement, row_name, col_name): “combined” 或者 “top”，调整行变量名称的位置 注意在应用这些帮助函数时要遵从一定的逻辑顺序。例如，adorn_ns() 和 adorn_percent_fomatting() 应该在调用 adorn_percentages() 之后。 对 t2 应用 adorn_* 函数： t2 %&gt;% adorn_totals(&quot;col&quot;) %&gt;% adorn_percentages(&quot;row&quot;) %&gt;% adorn_pct_formatting(digits = 2) %&gt;% adorn_ns() %&gt;% adorn_title(&quot;combined&quot;) #&gt; gender/eye_color blue blue-gray brown dark hazel #&gt; female 33.33% (3) 0.00% (0) 55.56% (5) 0.00% (0) 11.11% (1) #&gt; male 34.62% (9) 3.85% (1) 46.15% (12) 3.85% (1) 3.85% (1) #&gt; yellow Total #&gt; 0.00% (0) 100.00% (9) #&gt; 7.69% (2) 100.00% (26) tabyl 对象最终可以传入 knitr::kabel() 中呈现 t2 %&gt;% adorn_totals(&quot;row&quot;) %&gt;% adorn_percentages(&quot;col&quot;) %&gt;% adorn_pct_formatting(digits = 1) %&gt;% adorn_ns() %&gt;% adorn_title(&quot;top&quot;, row_name = &quot;gender&quot;, col_name = &quot;color&quot;) %&gt;% knitr::kable() col or gender blue blue-gray brown dark hazel yellow female 25.0% (3) 0.0% (0) 29.4% (5) 0.0% (0) 50.0% (1) 0.0% (0) male 75.0% (9) 100.0% (1) 70.6% (12) 100.0% (1) 50.0% (1) 100.0% (2) Total 100.0% (12) 100.0% (1) 100.0% (17) 100.0% (1) 100.0% (2) 100.0% (2) Three-way tabyl 在 tabyl() 中传入三个变量时，返回一个二维 tabyl 的列表： t3 &lt;- humans %&gt;% tabyl(eye_color, skin_color, gender) t3 #&gt; $female #&gt; eye_color dark fair light pale tan white #&gt; blue 0 2 1 0 0 0 #&gt; blue-gray 0 0 0 0 0 0 #&gt; brown 0 1 4 0 0 0 #&gt; dark 0 0 0 0 0 0 #&gt; hazel 0 0 1 0 0 0 #&gt; yellow 0 0 0 0 0 0 #&gt; #&gt; $male #&gt; eye_color dark fair light pale tan white #&gt; blue 0 7 2 0 0 0 #&gt; blue-gray 0 1 0 0 0 0 #&gt; brown 3 4 3 0 2 0 #&gt; dark 1 0 0 0 0 0 #&gt; hazel 0 1 0 0 0 0 #&gt; yellow 0 0 0 1 0 1 这时的 adorn_* 函数将会应用于列表中的每个 tabyl 元素： t3 %&gt;% adorn_percentages(&quot;row&quot;) %&gt;% adorn_pct_formatting(digits = 0) %&gt;% adorn_ns() #&gt; $female #&gt; eye_color dark fair light pale tan white #&gt; blue 0% (0) 67% (2) 33% (1) 0% (0) 0% (0) 0% (0) #&gt; blue-gray - (0) - (0) - (0) - (0) - (0) - (0) #&gt; brown 0% (0) 20% (1) 80% (4) 0% (0) 0% (0) 0% (0) #&gt; dark - (0) - (0) - (0) - (0) - (0) - (0) #&gt; hazel 0% (0) 0% (0) 100% (1) 0% (0) 0% (0) 0% (0) #&gt; yellow - (0) - (0) - (0) - (0) - (0) - (0) #&gt; #&gt; $male #&gt; eye_color dark fair light pale tan white #&gt; blue 0% (0) 78% (7) 22% (2) 0% (0) 0% (0) 0% (0) #&gt; blue-gray 0% (0) 100% (1) 0% (0) 0% (0) 0% (0) 0% (0) #&gt; brown 25% (3) 33% (4) 25% (3) 0% (0) 17% (2) 0% (0) #&gt; dark 100% (1) 0% (0) 0% (0) 0% (0) 0% (0) 0% (0) #&gt; hazel 0% (0) 100% (1) 0% (0) 0% (0) 0% (0) 0% (0) #&gt; yellow 0% (0) 0% (0) 0% (0) 50% (1) 0% (0) 50% (1) 14.2.2 get_dupes get_dupes(dat, ...) 返回数据框dat中在变量...上重复的观测，以及重复的次数： mtcars %&gt;% get_dupes(wt, cyl) #&gt; # A tibble: 4 x 12 #&gt; wt cyl dupe_count mpg disp hp drat qsec vs am gear carb #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 3.44 6 2 19.2 168. 123 3.92 18.3 1 0 4 4 #&gt; 2 3.44 6 2 17.8 168. 123 3.92 18.9 1 0 4 4 #&gt; 3 3.57 8 2 14.3 360 245 3.21 15.8 0 0 3 4 #&gt; 4 3.57 8 2 15 301 335 3.54 14.6 0 1 5 8 14.2.3 remove_ 14.2.3.1 remove_empty remove_empty(c(\"rows\", \"cols\")) 移除行或列（或行和列）上全为 NA 值的观测： q &lt;- data.frame(v1 = c(1, NA, 3), v2 = c(NA, NA, NA), v3 = c(&quot;a&quot;, NA, &quot;b&quot;)) q %&gt;% remove_empty(c(&quot;rows&quot;, &quot;cols&quot;)) #&gt; v1 v3 #&gt; 1 1 a #&gt; 3 3 b q %&gt;% remove_empty(&quot;rows&quot;) #&gt; v1 v2 v3 #&gt; 1 1 NA a #&gt; 3 3 NA b q %&gt;% remove_empty(&quot;cols&quot;) #&gt; v1 v3 #&gt; 1 1 a #&gt; 2 NA &lt;NA&gt; #&gt; 3 3 b remove_empty 的实现原理很简单，以移除空的行观测为例：如果某行全为 NA，则该行对应的 rowSums(is.na(dat)) = ncol(dat): function (dat, which = c(&quot;rows&quot;, &quot;cols&quot;)) { if (missing(which) &amp;&amp; !missing(dat)) { message(&quot;value for \\&quot;which\\&quot; not specified, defaulting to c(\\&quot;rows\\&quot;, \\&quot;cols\\&quot;)&quot;) which &lt;- c(&quot;rows&quot;, &quot;cols&quot;) } if ((sum(which %in% c(&quot;rows&quot;, &quot;cols&quot;)) != length(which)) &amp;&amp; !missing(dat)) { stop(&quot;\\&quot;which\\&quot; must be one of \\&quot;rows\\&quot;, \\&quot;cols\\&quot;, or c(\\&quot;rows\\&quot;, \\&quot;cols\\&quot;)&quot;) } if (&quot;rows&quot; %in% which) { dat &lt;- dat[rowSums(is.na(dat)) != ncol(dat), , drop = FALSE] } if (&quot;cols&quot; %in% which) { dat &lt;- dat[, colSums(!is.na(dat)) &gt; 0, drop = FALSE] } dat } 14.2.3.2 remove_constant remove_constant() 移除数据框中的常数列： a &lt;- data.frame(good = 1:3, boring = &quot;the same&quot;) a %&gt;% remove_constant() #&gt; good #&gt; 1 1 #&gt; 2 2 #&gt; 3 3 14.2.4 round_half_up Base R 中的取整函数 round() 采取的规则是 “四舍六入五留双”（Banker’s Rounding，当小数位是 .5 时，若前一位是奇数，则进 1 ； 若前一位数偶数，则退一）： nums &lt;- c(2.5, 3.5) round(nums) #&gt; [1] 2 4 round_half_up 遵循最简单的四舍五入规则: round_half_up(nums) #&gt; [1] 3 4 若希望取整到特定的小数位，例如 0, 0.25, 0.5, 0.75, 1。可以用 round_half_fraction() 并指定除数 14.2.5 excel_numeric_to_date excel_numeric_to_date() 按照 Excel 编码日期的规则(1989/12/31 = 1) 将整数转换为数字： excel_numeric_to_date(41103) #&gt; [1] &quot;2012-07-13&quot; excel_numeric_to_date(41103.01) # ignores decimal places, returns Date object #&gt; [1] &quot;2012-07-13&quot; 14.2.6 top_levels 在李克特量表数据的分析中，常需要知道某个态度变量中占比最高的几个水平，这样的变量在 R 中以有序因子的方式储存，top_levels() 将有序因子的所有水平分为三组（左，中间，右），并分别呈现各组的频数： f &lt;- factor(c(&quot;strongly agree&quot;, &quot;agree&quot;, &quot;neutral&quot;, &quot;neutral&quot;, &quot;disagree&quot;, &quot;strongly agree&quot;), levels = c(&quot;strongly agree&quot;, &quot;agree&quot;, &quot;neutral&quot;, &quot;disagree&quot;, &quot;strongly disagree&quot;)) top_levels(f) #&gt; f n percent #&gt; strongly agree, agree 3 0.500 #&gt; neutral 2 0.333 #&gt; disagree, strongly disagree 1 0.167 top_levels(as.factor(mtcars$hp)) #&gt; as.factor(mtcars$hp) n percent #&gt; 52, 62 2 0.0625 #&gt; &lt;&lt;&lt; Middle Group (18 categories) &gt;&gt;&gt; 28 0.8750 #&gt; 264, 335 2 0.0625 改变两侧分组包含水平的个数： top_levels(as.factor(mtcars$hp), n = 4) #&gt; as.factor(mtcars$hp) n percent #&gt; 52, 62, 65, 66 5 0.156 #&gt; &lt;&lt;&lt; Middle Group (14 categories) &gt;&gt;&gt; 22 0.688 #&gt; 230, 245, 264, 335 5 0.156 14.2.7 row_to_names row_to_names() 将某个观测行提升至列名： dirt &lt;- data.frame(X_1 = c(NA, &quot;ID&quot;, 1:3), X_2 = c(NA, &quot;Value&quot;, 4:6)) dirt #&gt; X_1 X_2 #&gt; 1 &lt;NA&gt; &lt;NA&gt; #&gt; 2 ID Value #&gt; 3 1 4 #&gt; 4 2 5 #&gt; 5 3 6 dirt %&gt;% row_to_names(row_number = 2, remove_rows_above = F) #&gt; ID Value #&gt; 1 &lt;NA&gt; &lt;NA&gt; #&gt; 3 1 4 #&gt; 4 2 5 #&gt; 5 3 6 dirt %&gt;% row_to_names(row_number = 2, remove_rows_above = T) #&gt; ID Value #&gt; 3 1 4 #&gt; 4 2 5 #&gt; 5 3 6 "],
["advanced-relational-data.html", "15 Advanced relational data ", " 15 Advanced relational data "],
["fuzzyjoin.html", "15.1 fuzzyjoin", " 15.1 fuzzyjoin http://varianceexplained.org/fuzzyjoin/reference/index.html 15.1.1 inexact matching library(fuzzyjoin) library(janeaustenr) # combine 50 rows into a passage passages &lt;- tibble(text = prideprejudice) %&gt;% group_by(passage = 1 + row_number() %/% 50) %&gt;% summarize(text = str_c(text, collapse = &quot; &quot;)) passages #&gt; # A tibble: 261 x 2 #&gt; passage text #&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 1 &quot;PRIDE AND PREJUDICE By Jane Austen Chapter 1 It is a truth uni~ #&gt; 2 2 &quot;\\&quot;How so? How can it affect them?\\&quot; \\&quot;My dear Mr. Bennet,\\&quot; replied~ #&gt; 3 3 &quot;are my old friends. I have heard you mention them with consideration~ #&gt; 4 4 &quot;herself, began scolding one of her daughters. \\&quot;Don&#39;t keep coughing~ #&gt; 5 5 &quot; The astonishment of the ladies was just what he wished; that of Mrs~ #&gt; 6 6 &quot;married, I shall have nothing to wish for.\\&quot; In a few days Mr. Bing~ #&gt; # ... with 255 more rows characters &lt;- readr::read_csv( &quot;character,character_regex Elizabeth,Elizabeth Darcy,Darcy Mr. Bennet,Mr. Bennet Mrs. Bennet,Mrs. Bennet Jane,Jane Mary,Mary Lydia,Lydia Kitty,Kitty Wickham,Wickham Mr. Collins,Collins Lady Catherine de Bourgh,de Bourgh Mr. Gardiner,Mr. Gardiner Mrs. Gardiner,Mrs. Gardiner Charlotte Lucas,(Charlotte|Lucas) &quot;) characters #&gt; # A tibble: 14 x 2 #&gt; character character_regex #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Elizabeth Elizabeth #&gt; 2 Darcy Darcy #&gt; 3 Mr. Bennet Mr. Bennet #&gt; 4 Mrs. Bennet Mrs. Bennet #&gt; 5 Jane Jane #&gt; 6 Mary Mary #&gt; # ... with 8 more rows Which character appears in most passages(the dataset with the text column must always come first): character_passages &lt;- passages %&gt;% regex_inner_join(characters, by = c(&quot;text&quot; = &quot;character_regex&quot;)) character_passages %&gt;% count(character, sort = TRUE) #&gt; # A tibble: 14 x 2 #&gt; character n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Elizabeth 227 #&gt; 2 Darcy 159 #&gt; 3 Jane 134 #&gt; 4 Mrs. Bennet 89 #&gt; 5 Wickham 89 #&gt; 6 Lydia 79 #&gt; # ... with 8 more rows # character_passages %&gt;% # select(-character_regex) %&gt;% # pivot_wider(names_from = &quot;character&quot;, values_from = &quot;text&quot;) %&gt;% # mutate_all(str_length) %&gt;% # mutate_all(~ replace_na(.x, 0)) 15.1.2 stringdist library(fuzzyjoin) misspellings #&gt; # A tibble: 4,505 x 2 #&gt; misspelling correct #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 abandonned abandoned #&gt; 2 aberation aberration #&gt; 3 abilties abilities #&gt; 4 abilty ability #&gt; 5 abondon abandon #&gt; 6 abbout about #&gt; # ... with 4,499 more rows library(qdapDictionaries) words &lt;- as_tibble(DICTIONARY) words #&gt; # A tibble: 20,137 x 2 #&gt; word syllables #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 hm 1 #&gt; 2 hmm 1 #&gt; 3 hmmm 1 #&gt; 4 hmph 1 #&gt; 5 mmhmm 2 #&gt; 6 mmhm 2 #&gt; # ... with 20,131 more rows misspellings %&gt;% sample_n(1000) %&gt;% stringdist_inner_join(words, by = c(&quot;misspelling&quot; = &quot;word&quot;), max_dist = 1) #&gt; # A tibble: 764 x 4 #&gt; misspelling correct word syllables #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 exibition exhibition exhibition 4 #&gt; 2 eminate emanate emanate 3 #&gt; 3 eminate emanate geminate 3 #&gt; 4 seperatist separatist separatist 4 #&gt; 5 thyat that that 1 #&gt; 6 visable visible disable 3 #&gt; # ... with 758 more rows "],
["funneljoin.html", "15.2 funneljoin", " 15.2 funneljoin blog: https://hookedondata.org/introducing-the-funneljoin-package/ video: https://www.youtube.com/watch?v=-n4XaYHDlG8 15.2.1 after_join() library(funneljoin) answers &lt;- vroom::vroom(&quot;../data/rquestions/Answers.csv&quot;) questions &lt;- vroom::vroom(&quot;../data/rquestions/Questions.csv&quot;) Look at the data: (answers &lt;- answers %&gt;% janitor::clean_names() %&gt;% filter(!is.na(owner_user_id))) #&gt; # A tibble: 636,662 x 7 #&gt; id owner_user_id creation_date parent_id score is_accepted_ans~ body #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; #&gt; 1 79741 3259 2008-09-17T03~ 79709 -1 FALSE &quot;&lt;p&gt;It&#39;s ~ #&gt; 2 79768 6043 2008-09-17T03~ 79709 9 FALSE &quot;&lt;p&gt;use v~ #&gt; 3 79779 8002 2008-09-17T03~ 79709 0 FALSE &quot;&lt;p&gt;Third~ #&gt; 4 79827 14257 2008-09-17T03~ 79709 1 FALSE &quot;&lt;p&gt;I&#39;m n~ #&gt; 5 79893 14928 2008-09-17T04~ 79709 6 FALSE &quot;&lt;p&gt;Remem~ #&gt; 6 83162 15842 2008-09-17T13~ 77434 70 FALSE &quot;&lt;p&gt;If yo~ #&gt; # ... with 636,656 more rows (questions &lt;- questions %&gt;% janitor::clean_names() %&gt;% filter(!is.na(owner_user_id))) #&gt; # A tibble: 857,160 x 6 #&gt; id owner_user_id creation_date score title body #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 77434 14008 2008-09-16T21:~ 171 How to access t~ &quot;&lt;p&gt;Suppose I hav~ #&gt; 2 95007 15842 2008-09-18T17:~ 56 Explain the qua~ &quot;&lt;p&gt;I&#39;ve been mys~ #&gt; 3 255697 1941213 2008-11-01T15:~ 4 Is there an R p~ &quot;&lt;p&gt;I&#39;m looking f~ #&gt; 4 359438 2173 2008-12-11T14:~ 4 Optimization pa~ &quot;&lt;p&gt;Does anyone k~ #&gt; 5 439526 37751 2009-01-13T15:~ 23 Thinking in Vec~ &quot;&lt;p&gt;I know that R~ #&gt; 6 445059 37751 2009-01-14T23:~ 12 Vectorize my th~ &quot;&lt;p&gt;So earlier I ~ #&gt; # ... with 857,154 more rows first_answer_after_first_question &lt;- questions %&gt;% after_left_join(answers, by_time = &quot;creation_date&quot;, by_user = &quot;owner_user_id&quot;, type = &quot;first-firstafter&quot;, suffix = c(&quot;_question&quot;, &quot;_answer&quot;)) first_answer_after_first_question #&gt; # A tibble: 429,900 x 12 #&gt; id_question owner_user_id creation_date_q~ score_question title body_question #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 &quot;strsplit(~ &quot;&#39;&quot; &#39; fixed=TRUE))) &lt;NA&gt; &quot;colnames(df~ #&gt; 2 &quot;(gsub(&#39;[\\~ &quot;&#39;\\&quot;\\&quot;&quot; &#39; input$txt)) &quot;&#39;\\&quot;~ &lt;NA&gt; #&gt; 3 &quot;str_repla~ &quot;&#39;\\\\\\\\|&#39;&quot; &#39; &#39;)) &lt;NA&gt; &quot;&lt;/code&gt;&lt;/pr~ #&gt; 4 &quot;&#39;-&#39;&quot; &quot;&#39;]&#39;&quot; &#39; &#39; &quot;&#39;\\&quot;~ &quot;f2=f1.lower~ #&gt; 5 &quot;&#39;B&#39;&quot; &quot;&#39;C&quot; &#39; &#39;D&#39;) &lt;NA&gt; &quot;from &amp;lt;- ~ #&gt; 6 &quot;&#39;&quot; &quot;&#39;DE&quot; &#39; &#39;CA &quot;&#39;, ~ &quot;print(Websi~ #&gt; # ... with 429,894 more rows, and 6 more variables: id_answer &lt;chr&gt;, #&gt; # creation_date_answer &lt;chr&gt;, parent_id &lt;chr&gt;, score_answer &lt;dbl&gt;, #&gt; # is_accepted_answer &lt;lgl&gt;, body_answer &lt;chr&gt; type determines funnel types: first_answer_after_first_question %&gt;% summarize_conversions(converted = id_answer) #&gt; # A tibble: 1 x 3 #&gt; nb_users nb_conversions pct_converted #&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 429900 30836 0.0717 15.2.2 funnel in one table Sometimes you have all the data you need in one table. For example, let’s look at this table of user activity on a website. activity &lt;- tibble::tribble( ~ &quot;user_id&quot;, ~ &quot;event&quot;, ~ &quot;timestamp&quot;, 1, &quot;landing&quot;, &quot;2019-07-01&quot;, 1, &quot;registration&quot;, &quot;2019-07-02&quot;, 1, &quot;purchase&quot;, &quot;2019-07-07&quot;, 1, &quot;purchase&quot;, &quot;2019-07-10&quot;, 2, &quot;landing&quot;, &quot;2019-08-01&quot;, 2, &quot;registration&quot;, &quot;2019-08-15&quot;, 3, &quot;landing&quot;, &quot;2019-05-01&quot;, 3, &quot;registration&quot;, &quot;2019-06-01&quot;, 3, &quot;purchase&quot;, &quot;2019-06-04&quot;, 4, &quot;landing&quot;, &quot;2019-06-13&quot; ) activity %&gt;% funnel_start(moment_type = &quot;landing&quot;, moment = &quot;event&quot;, tstamp = &quot;timestamp&quot;, user = &quot;user_id&quot;) #&gt; # A tibble: 4 x 2 #&gt; user_id timestamp_landing #&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 1 2019-07-01 #&gt; 2 2 2019-08-01 #&gt; 3 3 2019-05-01 #&gt; 4 4 2019-06-13 activity %&gt;% funnel_start(moment_type = &quot;landing&quot;, moment = &quot;event&quot;, tstamp = &quot;timestamp&quot;, user = &quot;user_id&quot;) %&gt;% funnel_step(moment_type = &quot;registration&quot;, type = &quot;first-firstafter&quot;) #&gt; # A tibble: 4 x 3 #&gt; user_id timestamp_landing timestamp_registration #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 3 2019-05-01 2019-06-01 #&gt; 2 4 2019-06-13 &lt;NA&gt; #&gt; 3 1 2019-07-01 2019-07-02 #&gt; 4 2 2019-08-01 2019-08-15 activity %&gt;% funnel_start(moment_type = &quot;landing&quot;, moment = &quot;event&quot;, tstamp = &quot;timestamp&quot;, user = &quot;user_id&quot;) %&gt;% funnel_step(moment_type = &quot;registration&quot;, type = &quot;first-firstafter&quot;) %&gt;% funnel_step(moment_type = &quot;purchase&quot;, type = &quot;first-firstafter&quot;) #&gt; # A tibble: 4 x 4 #&gt; user_id timestamp_landing timestamp_registration timestamp_purchase #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 3 2019-05-01 2019-06-01 2019-06-04 #&gt; 2 1 2019-07-01 2019-07-02 2019-07-07 #&gt; 3 2 2019-08-01 2019-08-15 &lt;NA&gt; #&gt; 4 4 2019-06-13 &lt;NA&gt; &lt;NA&gt; "],
["dm.html", "15.3 dm", " 15.3 dm https://krlmlr.github.io/dm # devtools::install_github(&quot;krlmlr/dm&quot;) # library(dm) # dm_nycflights13() "],
["categorical-data-facotr.html", "16 Categorical data (facotr) ", " 16 Categorical data (facotr) "],
["frequency-and-contingency-table.html", "16.1 Frequency and contingency table", " 16.1 Frequency and contingency table 16.1.1 frq() and flat_table() sjmisc https://strengejacke.github.io/sjmisc/articles/exploringdatasets.html library(sjmisc) data(efc) (efc &lt;- efc %&gt;% as_tibble()) #&gt; # A tibble: 908 x 26 #&gt; c12hour e15relat e16sex e17age e42dep c82cop1 c83cop2 c84cop3 c85cop4 c86cop5 #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 16 2 2 83 3 3 2 2 2 1 #&gt; 2 148 2 2 88 3 3 3 3 3 4 #&gt; 3 70 1 2 82 3 2 2 1 4 1 #&gt; 4 168 1 2 67 4 4 1 3 1 1 #&gt; 5 168 2 2 84 4 3 2 1 2 2 #&gt; 6 16 2 2 85 4 2 2 3 3 3 #&gt; # ... with 902 more rows, and 16 more variables: c87cop6 &lt;dbl&gt;, c88cop7 &lt;dbl&gt;, #&gt; # c89cop8 &lt;dbl&gt;, c90cop9 &lt;dbl&gt;, c160age &lt;dbl&gt;, c161sex &lt;dbl&gt;, c172code &lt;dbl&gt;, #&gt; # c175empl &lt;dbl&gt;, barthtot &lt;dbl&gt;, neg_c_7 &lt;dbl&gt;, pos_v_4 &lt;dbl&gt;, quol_5 &lt;dbl&gt;, #&gt; # resttotn &lt;dbl&gt;, tot_sc_e &lt;dbl&gt;, n4pstu &lt;dbl&gt;, nur_pst &lt;dbl&gt; efc %&gt;% frq(c161sex) #&gt; #&gt; carer&#39;s gender (c161sex) &lt;numeric&gt; #&gt; # total N=908 valid N=901 mean=1.76 sd=0.43 #&gt; #&gt; Value | Label | N | Raw % | Valid % | Cum. % #&gt; ----------------------------------------------- #&gt; 1 | Male | 215 | 23.68 | 23.86 | 23.86 #&gt; 2 | Female | 686 | 75.55 | 76.14 | 100.00 #&gt; &lt;NA&gt; | &lt;NA&gt; | 7 | 0.77 | &lt;NA&gt; | &lt;NA&gt; efc %&gt;% group_by(e42dep) %&gt;% frq(c161sex) #&gt; #&gt; carer&#39;s gender (c161sex) &lt;numeric&gt; #&gt; # grouped by: independent #&gt; # total N=66 valid N=66 mean=1.73 sd=0.45 #&gt; #&gt; Value | Label | N | Raw % | Valid % | Cum. % #&gt; ---------------------------------------------- #&gt; 1 | Male | 18 | 27.27 | 27.27 | 27.27 #&gt; 2 | Female | 48 | 72.73 | 72.73 | 100.00 #&gt; &lt;NA&gt; | &lt;NA&gt; | 0 | 0.00 | &lt;NA&gt; | &lt;NA&gt; #&gt; #&gt; #&gt; carer&#39;s gender (c161sex) &lt;numeric&gt; #&gt; # grouped by: slightly dependent #&gt; # total N=225 valid N=224 mean=1.76 sd=0.43 #&gt; #&gt; Value | Label | N | Raw % | Valid % | Cum. % #&gt; ----------------------------------------------- #&gt; 1 | Male | 54 | 24.00 | 24.11 | 24.11 #&gt; 2 | Female | 170 | 75.56 | 75.89 | 100.00 #&gt; &lt;NA&gt; | &lt;NA&gt; | 1 | 0.44 | &lt;NA&gt; | &lt;NA&gt; #&gt; #&gt; #&gt; carer&#39;s gender (c161sex) &lt;numeric&gt; #&gt; # grouped by: moderately dependent #&gt; # total N=306 valid N=306 mean=1.74 sd=0.44 #&gt; #&gt; Value | Label | N | Raw % | Valid % | Cum. % #&gt; ----------------------------------------------- #&gt; 1 | Male | 80 | 26.14 | 26.14 | 26.14 #&gt; 2 | Female | 226 | 73.86 | 73.86 | 100.00 #&gt; &lt;NA&gt; | &lt;NA&gt; | 0 | 0.00 | &lt;NA&gt; | &lt;NA&gt; #&gt; #&gt; #&gt; carer&#39;s gender (c161sex) &lt;numeric&gt; #&gt; # grouped by: severely dependent #&gt; # total N=304 valid N=304 mean=1.79 sd=0.41 #&gt; #&gt; Value | Label | N | Raw % | Valid % | Cum. % #&gt; ----------------------------------------------- #&gt; 1 | Male | 63 | 20.72 | 20.72 | 20.72 #&gt; 2 | Female | 241 | 79.28 | 79.28 | 100.00 #&gt; &lt;NA&gt; | &lt;NA&gt; | 0 | 0.00 | &lt;NA&gt; | &lt;NA&gt; flat_table(efc, e42dep, c161sex) #&gt; c161sex Male Female #&gt; e42dep #&gt; independent 18 48 #&gt; slightly dependent 54 170 #&gt; moderately dependent 80 226 #&gt; severely dependent 63 241 flat_table(efc, e42dep, c161sex, margin = &quot;col&quot;) #&gt; c161sex Male Female #&gt; e42dep #&gt; independent 8.37 7.01 #&gt; slightly dependent 25.12 24.82 #&gt; moderately dependent 37.21 32.99 #&gt; severely dependent 29.30 35.18 library(janitor) efc %&gt;% tabyl(e42dep, c161sex, show_na = FALSE) #&gt; e42dep 1 2 #&gt; 1 18 48 #&gt; 2 54 170 #&gt; 3 80 226 #&gt; 4 63 241 "],
["coding.html", "16.2 Coding", " 16.2 Coding 16.2.1 rec() sjmisc https://strengejacke.github.io/sjmisc/articles/recodingvariables.html efc$burden &lt;- rec( efc$neg_c_7, rec = c(&quot;min:9=1 [low]; 10:12=2 [moderate]; 13:max=3 [high]; else=NA&quot;), var.label = &quot;Subjective burden&quot;, as.num = FALSE # we want a factor ) # print frequencies frq(efc$burden) #&gt; #&gt; Subjective burden (x) &lt;categorical&gt; #&gt; # total N=908 valid N=892 mean=2.03 sd=0.81 #&gt; #&gt; Value | Label | N | Raw % | Valid % | Cum. % #&gt; ------------------------------------------------- #&gt; 1 | low | 280 | 30.84 | 31.39 | 31.39 #&gt; 2 | moderate | 301 | 33.15 | 33.74 | 65.13 #&gt; 3 | high | 311 | 34.25 | 34.87 | 100.00 #&gt; &lt;NA&gt; | &lt;NA&gt; | 16 | 1.76 | &lt;NA&gt; | &lt;NA&gt; "],
["cutting.html", "16.3 Cutting", " 16.3 Cutting 16.3.1 chop() santoku https://hughjonesd.github.io/santoku/tutorials/00-visualintroduction.html # devtools::install_github(&quot;hughjonesd/santoku&quot;) # library(tidyverse) (load tidyverse before santoku to avoid conflicts) library(santoku) cut() in base R x &lt;- rnorm(100) cut(x, 5) %&gt;% table() # 5 equal intervals #&gt; . #&gt; (-2.88,-1.62] (-1.62,-0.368] (-0.368,0.884] (0.884,2.14] (2.14,3.4] #&gt; 5 25 52 17 1 cut(x, -3:3) %&gt;% table() #&gt; . #&gt; (-3,-2] (-2,-1] (-1,0] (0,1] (1,2] (2,3] #&gt; 3 15 27 38 16 0 ntile() in dplyr: ntile(x, 5) %&gt;% table() #&gt; . #&gt; 1 2 3 4 5 #&gt; 20 20 20 20 20 chop() chopped &lt;- chop(x, breaks = -5:5) chopped %&gt;% table() #&gt; . #&gt; [-3, -2) [-2, -1) [-1, 0) [0, 1) [1, 2) [3, 4) #&gt; 3 15 27 38 16 1 # chop() returns a factor tibble(x, chopped) #&gt; # A tibble: 100 x 2 #&gt; x chopped #&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 -0.422 [-1, 0) #&gt; 2 0.0569 [0, 1) #&gt; 3 0.711 [0, 1) #&gt; 4 -1.59 [-2, -1) #&gt; 5 0.597 [0, 1) #&gt; 6 1.22 [1, 2) #&gt; # ... with 94 more rows If data is beyond the limits of breaks, they will be extended automatically, unless extend = FALSE, and values beyond the bounds will be converted to NA: chopped &lt;- chop(x, breaks = -1:1, extend = FALSE) tibble(x, chopped) #&gt; # A tibble: 100 x 2 #&gt; x chopped #&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 -0.422 [-1, 0) #&gt; 2 0.0569 [0, 1] #&gt; 3 0.711 [0, 1] #&gt; 4 -1.59 &lt;NA&gt; #&gt; 5 0.597 [0, 1] #&gt; 6 1.22 &lt;NA&gt; #&gt; # ... with 94 more rows To chop a single number into a separate category, put the number twice in breaks: x_zeros &lt;- x x_zeros[1:5] &lt;- 0 chopped &lt;- chop(x_zeros, c(-1, 0, 0, 1)) tibble(x, chopped) #&gt; # A tibble: 100 x 2 #&gt; x chopped #&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 -0.422 {0} #&gt; 2 0.0569 {0} #&gt; 3 0.711 {0} #&gt; 4 -1.59 {0} #&gt; 5 0.597 {0} #&gt; 6 1.22 (1, 3.39] #&gt; # ... with 94 more rows To quickly produce a table of chopped data, use tab(): tab(x, breaks = -3:3) #&gt; x #&gt; [-3, -2) [-2, -1) [-1, 0) [0, 1) [1, 2) (3, 3.39] #&gt; 3 15 27 38 16 1 "],
["dealing-with-missing-values.html", "17 Dealing with missing values ", " 17 Dealing with missing values "],
["explore-missing.html", "17.1 Exploring", " 17.1 Exploring 17.1.1 naniar http://naniar.njtierney.com/ library(naniar) 17.1.1.1 Shadown matrices 17.1.1.2 Summaries 17.1.1.3 Visualizations http://naniar.njtierney.com/articles/naniar-visualisation.html ggplot(airquality) + geom_miss_point(aes(Ozone, Solar.R)) ggplot(data = airquality, aes(x = Ozone, y = Solar.R)) + geom_miss_point() + facet_wrap(~Month, ncol = 2) + theme(legend.position = &quot;bottom&quot;) gg_miss_upset(airquality) gg_miss_upset(riskfactors) 17.1.2 Replace a value with NA It’s worth to mention one dplyr functions that does the same thing: dplyr::na_if() y &lt;- c(&quot;abc&quot;, &quot;def&quot;, &quot;&quot;, &quot;ghi&quot;) na_if(y, &quot;&quot;) #&gt; [1] &quot;abc&quot; &quot;def&quot; NA &quot;ghi&quot; # na_if is particularly useful inside mutate, # and is meant for use with vectors rather than entire data frames starwars %&gt;% select(name, eye_color) %&gt;% mutate(eye_color = na_if(eye_color, &quot;unknown&quot;)) #&gt; # A tibble: 87 x 2 #&gt; name eye_color #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Luke Skywalker blue #&gt; 2 C-3PO yellow #&gt; 3 R2-D2 red #&gt; 4 Darth Vader yellow #&gt; 5 Leia Organa brown #&gt; 6 Owen Lars blue #&gt; # ... with 81 more rows article: http://naniar.njtierney.com/articles/replace-with-na.html df &lt;- tibble::tribble( ~name, ~x, ~y, ~z, &quot;N/A&quot;, 1, &quot;N/A&quot;, -100, &quot;N A&quot;, 3, &quot;NOt available&quot;, -99, &quot;N / A&quot;, NA, &quot;29&quot;, -98, &quot;Not Available&quot;, -99, &quot;25&quot;, -101, &quot;John Smith&quot;, -98, &quot;28&quot;, -1) df %&gt;% replace_with_na(replace = list(x = -99)) #&gt; # A tibble: 5 x 4 #&gt; name x y z #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 N/A 1 N/A -100 #&gt; 2 N A 3 NOt available -99 #&gt; 3 N / A NA 29 -98 #&gt; 4 Not Available NA 25 -101 #&gt; 5 John Smith -98 28 -1 df %&gt;% replace_with_na(replace = list(x = c(-99, -98))) #&gt; # A tibble: 5 x 4 #&gt; name x y z #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 N/A 1 N/A -100 #&gt; 2 N A 3 NOt available -99 #&gt; 3 N / A NA 29 -98 #&gt; 4 Not Available NA 25 -101 #&gt; 5 John Smith NA 28 -1 df %&gt;% replace_with_na(replace = list(x = c(-99,-98), z = c(-99, -98))) #&gt; # A tibble: 5 x 4 #&gt; name x y z #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 N/A 1 N/A -100 #&gt; 2 N A 3 NOt available NA #&gt; 3 N / A NA 29 NA #&gt; 4 Not Available NA 25 -101 #&gt; 5 John Smith NA 28 -1 df %&gt;% replace_with_na_all(condition = ~.x == -99) #&gt; # A tibble: 5 x 4 #&gt; name x y z #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 N/A 1 N/A -100 #&gt; 2 N A 3 NOt available NA #&gt; 3 N / A NA 29 -98 #&gt; 4 Not Available NA 25 -101 #&gt; 5 John Smith -98 28 -1 common_na_numbers #&gt; [1] -9 -99 -999 -9999 9999 66 77 88 common_na_strings #&gt; [1] &quot;NA&quot; &quot;N A&quot; &quot;N/A&quot; &quot;NA &quot; &quot; NA&quot; &quot;N /A&quot; &quot;N / A&quot; &quot; N / A&quot; #&gt; [9] &quot;N / A &quot; &quot;na&quot; &quot;n a&quot; &quot;n/a&quot; &quot;na &quot; &quot; na&quot; &quot;n /a&quot; &quot;n / a&quot; #&gt; [17] &quot; a / a&quot; &quot;n / a &quot; &quot;NULL&quot; &quot;null&quot; &quot;&quot; &quot;\\\\?&quot; &quot;\\\\*&quot; &quot;\\\\.&quot; df %&gt;% replace_with_na_all(~.x %in% common_na_strings) #&gt; # A tibble: 5 x 4 #&gt; name x y z #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 &lt;NA&gt; 1 &lt;NA&gt; -100 #&gt; 2 &lt;NA&gt; 3 NOt available -99 #&gt; 3 &lt;NA&gt; NA 29 -98 #&gt; 4 Not Available -99 25 -101 #&gt; 5 John Smith -98 28 -1 17.1.3 janitor janitor::tabyl() generates a frequency table and exposing missing values at the same time janitor::tabyl(df, y) #&gt; y n percent #&gt; 25 1 0.2 #&gt; 28 1 0.2 #&gt; 29 1 0.2 #&gt; N/A 1 0.2 #&gt; NOt available 1 0.2 17.1.4 sjmisc when missing variable is factor sjmisc::frq "],
["wrangling.html", "17.2 Wrangling", " 17.2 Wrangling 17.2.1 tidyr tidyr provides a handful of tools for converting between implicit (absent rows) and explicit (NA) missing values, and for handling explicit NAs. 17.2.1.1 drop_na() drop rows containing missing values # df %&gt;% drop_na(Ozone, Solar.R, Wind, Temp, Month, Day) airquality &lt;- airquality %&gt;% as_tibble() airquality %&gt;% drop_na() #&gt; # A tibble: 111 x 6 #&gt; Ozone Solar.R Wind Temp Month Day #&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 41 190 7.4 67 5 1 #&gt; 2 36 118 8 72 5 2 #&gt; 3 12 149 12.6 74 5 3 #&gt; 4 18 313 11.5 62 5 4 #&gt; 5 23 299 8.6 65 5 7 #&gt; 6 19 99 13.8 59 5 8 #&gt; # ... with 105 more rows airquality %&gt;% drop_na(Solar.R) #&gt; # A tibble: 146 x 6 #&gt; Ozone Solar.R Wind Temp Month Day #&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 41 190 7.4 67 5 1 #&gt; 2 36 118 8 72 5 2 #&gt; 3 12 149 12.6 74 5 3 #&gt; 4 18 313 11.5 62 5 4 #&gt; 5 23 299 8.6 65 5 7 #&gt; 6 19 99 13.8 59 5 8 #&gt; # ... with 140 more rows 17.2.1.2 replace_na() replace missing values # NULL are the list-col equivalent of NAs df &lt;- tibble(x = c(1, 2, NA), y = c(&quot;a&quot;, NA, &quot;b&quot;), z = list(1:5, NULL, 10:20)) df %&gt;% mutate(x = replace_na(x, 0)) #&gt; # A tibble: 3 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;list&gt; #&gt; 1 1 a &lt;int [5]&gt; #&gt; 2 2 &lt;NA&gt; &lt;NULL&gt; #&gt; 3 0 b &lt;int [11]&gt; df %&gt;% replace_na(list(x = 0, y = &quot;unknown&quot;)) #&gt; # A tibble: 3 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;list&gt; #&gt; 1 1 a &lt;int [5]&gt; #&gt; 2 2 unknown &lt;NULL&gt; #&gt; 3 0 b &lt;int [11]&gt; df %&gt;% replace_na(list(z = 5)) #&gt; # A tibble: 3 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;list&gt; #&gt; 1 1 a &lt;int [5]&gt; #&gt; 2 2 &lt;NA&gt; &lt;dbl [1]&gt; #&gt; 3 NA b &lt;int [11]&gt; 17.2.1.3 fill() fill in missing values with previous or next value df &lt;- tibble(Month = rep(1:12, 2), Year = c(2000, rep(NA, 11), 2001, rep(NA, 11))) df %&gt;% fill(Year) # .direction = &quot;down&quot; #&gt; # A tibble: 24 x 2 #&gt; Month Year #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 2000 #&gt; 2 2 2000 #&gt; 3 3 2000 #&gt; 4 4 2000 #&gt; 5 5 2000 #&gt; 6 6 2000 #&gt; # ... with 18 more rows df %&gt;% fill(Year, .direction = &quot;up&quot;) #&gt; # A tibble: 24 x 2 #&gt; Month Year #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 2000 #&gt; 2 2 2001 #&gt; 3 3 2001 #&gt; 4 4 2001 #&gt; 5 5 2001 #&gt; 6 6 2001 #&gt; # ... with 18 more rows df %&gt;% fill(Year, .direction = &quot;updown&quot;) #&gt; # A tibble: 24 x 2 #&gt; Month Year #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 2000 #&gt; 2 2 2001 #&gt; 3 3 2001 #&gt; 4 4 2001 #&gt; 5 5 2001 #&gt; 6 6 2001 #&gt; # ... with 18 more rows df %&gt;% fill(Year, .direction = &quot;downup&quot;) #&gt; # A tibble: 24 x 2 #&gt; Month Year #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 1 2000 #&gt; 2 2 2000 #&gt; 3 3 2000 #&gt; 4 4 2000 #&gt; 5 5 2000 #&gt; 6 6 2000 #&gt; # ... with 18 more rows 17.2.1.4 full_seq() create the full sequence of values in a vector This is useful if you want to fill in missing values that should have been observed but weren’t. For example, full_seq(c(1, 2, 4, 6), 1) will return 1:6. # `period` specify step length full_seq(c(1, 3, 5, 11, 13), period = 2) #&gt; [1] 1 3 5 7 9 11 13 # Works with dates, too! y &lt;- lubridate::ymd(c(&quot;2020-01-01&quot;, &quot;2020-01-03&quot;)) full_seq(y, period = 1) #&gt; [1] &quot;2020-01-01&quot; &quot;2020-01-02&quot; &quot;2020-01-03&quot; 17.2.1.5 expand() expand data frame to include all combinations of values expand() creates a data frame containing all conbinations of specified columns, often used in conjunction with left_join() to convert implicit missing values to explicit missing values, with anti_join() to figure out which combinations are missing. To find all unique combinations of x, y and z, including those not found in the data, supply each variable as a separate argument. To find only the combinations that occur in the data, use nest: expand(df, nesting(x, y, z)). You can combine the two forms. For example, expand(df, nesting(school_id, student_id), date) would produce a row for every student for each date. For factors, the full set of levels (not just those that appear in the data) are used. For continuous variables, you may need to fill in values that don’t appear in the data: to do so use expressions like year = 2010:2020or year = full_seq(year, 1). # All possible combinations of vs &amp; cyl, even those that aren&#39;t # present in the data expand(mtcars, vs, cyl) #&gt; # A tibble: 6 x 2 #&gt; vs cyl #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 4 #&gt; 2 0 6 #&gt; 3 0 8 #&gt; 4 1 4 #&gt; 5 1 6 #&gt; 6 1 8 # Only combinations of vs and cyl that appear in the data expand(mtcars, nesting(vs, cyl)) #&gt; # A tibble: 5 x 2 #&gt; vs cyl #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 4 #&gt; 2 0 6 #&gt; 3 0 8 #&gt; 4 1 4 #&gt; 5 1 6 # Implicit missings --------------------------------------------------------- df &lt;- tibble( year = c(2010, 2010, 2010, 2010, 2012, 2012, 2012), qtr = c( 1, 2, 3, 4, 1, 2, 3), return = rnorm(7) ) df %&gt;% expand(year, qtr) #&gt; # A tibble: 8 x 2 #&gt; year qtr #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2010 1 #&gt; 2 2010 2 #&gt; 3 2010 3 #&gt; 4 2010 4 #&gt; 5 2012 1 #&gt; 6 2012 2 #&gt; # ... with 2 more rows df %&gt;% expand(year = 2010:2012, qtr) #&gt; # A tibble: 12 x 2 #&gt; year qtr #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2010 1 #&gt; 2 2010 2 #&gt; 3 2010 3 #&gt; 4 2010 4 #&gt; 5 2011 1 #&gt; 6 2011 2 #&gt; # ... with 6 more rows df %&gt;% expand(year = full_seq(year, 1), qtr) #&gt; # A tibble: 12 x 2 #&gt; year qtr #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2010 1 #&gt; 2 2010 2 #&gt; 3 2010 3 #&gt; 4 2010 4 #&gt; 5 2011 1 #&gt; 6 2011 2 #&gt; # ... with 6 more rows # Each person was given one of two treatments, repeated three times # But some of the replications haven&#39;t happened yet, so we have # incomplete data: experiment &lt;- tibble( name = rep(c(&quot;Alex&quot;, &quot;Robert&quot;, &quot;Sam&quot;), c(3, 2, 1)), trt = rep(c(&quot;a&quot;, &quot;b&quot;, &quot;a&quot;), c(3, 2, 1)), rep = c(1, 2, 3, 1, 2, 1), measurement_1 = runif(6), measurement_2 = runif(6) ) # We can figure out the complete set of data with expand() # Each person only gets one treatment, so we nest name and trt together: all &lt;- experiment %&gt;% expand(nesting(name, trt), rep) all #&gt; # A tibble: 9 x 3 #&gt; name trt rep #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Alex a 1 #&gt; 2 Alex a 2 #&gt; 3 Alex a 3 #&gt; 4 Robert b 1 #&gt; 5 Robert b 2 #&gt; 6 Robert b 3 #&gt; # ... with 3 more rows # use left_join to convert implicit missing values to explicit missing values all %&gt;% left_join(experiment) #&gt; # A tibble: 9 x 5 #&gt; name trt rep measurement_1 measurement_2 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Alex a 1 0.614 0.379 #&gt; 2 Alex a 2 0.915 0.811 #&gt; 3 Alex a 3 0.838 0.475 #&gt; 4 Robert b 1 0.855 0.838 #&gt; 5 Robert b 2 0.501 0.739 #&gt; 6 Robert b 3 NA NA #&gt; # ... with 3 more rows # can use anti_join to figure out which observations are missing all %&gt;% anti_join(experiment) #&gt; # A tibble: 3 x 3 #&gt; name trt rep #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Robert b 3 #&gt; 2 Sam a 2 #&gt; 3 Sam a 3 # And use right_join to add in the appropriate missing values to the # original data experiment %&gt;% right_join(all) #&gt; # A tibble: 9 x 5 #&gt; name trt rep measurement_1 measurement_2 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Alex a 1 0.614 0.379 #&gt; 2 Alex a 2 0.915 0.811 #&gt; 3 Alex a 3 0.838 0.475 #&gt; 4 Robert b 1 0.855 0.838 #&gt; 5 Robert b 2 0.501 0.739 #&gt; 6 Robert b 3 NA NA #&gt; # ... with 3 more rows complete() is a short hand function around expand() + left_join(): firt create specified combinations and then left join original data to convert implicit missing values to explicit missing values: experiment %&gt;% complete(rep, nesting(name, trt)) #&gt; # A tibble: 9 x 5 #&gt; rep name trt measurement_1 measurement_2 #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 Alex a 0.614 0.379 #&gt; 2 1 Robert b 0.855 0.838 #&gt; 3 1 Sam a 0.158 0.278 #&gt; 4 2 Alex a 0.915 0.811 #&gt; 5 2 Robert b 0.501 0.739 #&gt; 6 2 Sam a NA NA #&gt; # ... with 3 more rows 17.2.1.6 expand_grid: create a tibble from all combinations of inputs expand_grid() is analogus to a (atomic) vector version if expand(). Instead of taking in a data frame, expand_grid() use multiple name-value pairs to generate all combinations : expand_grid(x = c(3, 2, 1), y = c(10, 5), z = letters[1:3]) #&gt; # A tibble: 18 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 3 10 a #&gt; 2 3 10 b #&gt; 3 3 10 c #&gt; 4 3 5 a #&gt; 5 3 5 b #&gt; 6 3 5 c #&gt; # ... with 12 more rows crossing() is a wrapper around expand_grid() that deduplicates and sorts each input. crossing(x = c(3, 2, 1), y = c(10, 5), z = letters[1:3]) #&gt; # A tibble: 18 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 1 5 a #&gt; 2 1 5 b #&gt; 3 1 5 c #&gt; 4 1 10 a #&gt; 5 1 10 b #&gt; 6 1 10 c #&gt; # ... with 12 more rows 17.2.2 janitor library(janitor) remove_empty(dat, which = c(\"rows\", \"cols\")) (df &lt;- tibble(x = c(1, NA, 3), y = rep(NA, 3), z = c(4, NA, 5))) #&gt; # A tibble: 3 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; #&gt; 1 1 NA 4 #&gt; 2 NA NA NA #&gt; 3 3 NA 5 df %&gt;% remove_empty() #&gt; # A tibble: 2 x 2 #&gt; x z #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 4 #&gt; 2 3 5 df %&gt;% remove_empty(&quot;rows&quot;) #&gt; # A tibble: 2 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; #&gt; 1 1 NA 4 #&gt; 2 3 NA 5 df %&gt;% remove_empty(&quot;cols&quot;) #&gt; # A tibble: 3 x 2 #&gt; x z #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 4 #&gt; 2 NA NA #&gt; 3 3 5 17.2.3 visdat visdat::vis_missing() library(visdat) vis_miss(df) Advanced: mice, Amelia "],
["imputation.html", "17.3 Imputation", " 17.3 Imputation https://jiangjun.link/post/2018/12/r-missing-data 用中位数或均值填补缺失值: df &lt;- tibble(x = c(1, 2, NA, 5, 9, NA), y = c(NA, 20, 1, NA, 5, NA), z = 5:10) df %&gt;% mutate_all( ~ ifelse(is.na(.x), median(.x, na.rm = T), .)) #&gt; # A tibble: 6 x 3 #&gt; x y z #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1 5 5 #&gt; 2 2 20 6 #&gt; 3 3.5 1 7 #&gt; 4 5 5 8 #&gt; 5 9 5 9 #&gt; 6 3.5 5 10 rlang::%|% simputation # install.packages(&quot;simputation&quot;, dependencies = TRUE) https://edwinth.github.io/padr/ "],
["references.html", "References", " References Arel-Bundock, Vincent. 2019. WDI: World Development Indicators (World Bank). https://CRAN.R-project.org/package=WDI. Firke, Sam. 2019. Janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor. Hester, Jim, and Hadley Wickham. 2019. Vroom: Read and Write Rectangular Text Data Quickly. https://CRAN.R-project.org/package=vroom. Waring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, and Shannon Ellis. 2019. Skimr: Compact and Flexible Summaries of Data. https://CRAN.R-project.org/package=skimr. Wickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. \" O’Reilly Media, Inc.\". "]
]
